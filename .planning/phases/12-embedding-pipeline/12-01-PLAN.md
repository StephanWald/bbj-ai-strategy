---
phase: 12-embedding-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - rag-ingestion/pyproject.toml
  - rag-ingestion/sql/schema.sql
  - rag-ingestion/src/bbj_rag/config.py
  - rag-ingestion/src/bbj_rag/db.py
  - rag-ingestion/src/bbj_rag/models.py
  - rag-ingestion/src/bbj_rag/chunker.py
  - rag-ingestion/src/bbj_rag/embedder.py
  - rag-ingestion/src/bbj_rag/pipeline.py
  - rag-ingestion/src/bbj_rag/cli.py
  - rag-ingestion/tests/test_chunker.py
  - rag-ingestion/tests/test_embedder.py
autonomous: true

must_haves:
  truths:
    - "Documents are split into chunks at heading boundaries, preserving code blocks intact"
    - "Embedding client generates vectors via Ollama (primary) with configurable API fallback"
    - "Chunks are bulk-inserted into pgvector using binary COPY protocol with idempotent dedup"
    - "CLI command `bbj-rag ingest --source flare` runs the full pipeline end-to-end"
    - "Individual stage commands (parse, embed) exist for debugging and re-runs"
    - "Schema uses vector(1024) matching Qwen3-Embedding-0.6B output dimensions"
  artifacts:
    - path: "rag-ingestion/src/bbj_rag/chunker.py"
      provides: "Heading-aware markdown chunker with code block preservation"
      exports: ["chunk_document"]
    - path: "rag-ingestion/src/bbj_rag/embedder.py"
      provides: "Embedding client abstraction with Ollama and OpenAI implementations"
      exports: ["Embedder", "OllamaEmbedder", "OpenAIEmbedder"]
    - path: "rag-ingestion/src/bbj_rag/pipeline.py"
      provides: "Pipeline orchestrator wiring parse -> intelligence -> chunk -> embed -> store"
      exports: ["run_pipeline"]
    - path: "rag-ingestion/src/bbj_rag/cli.py"
      provides: "Click CLI with ingest, parse, and validate commands"
      exports: ["cli"]
    - path: "rag-ingestion/tests/test_chunker.py"
      provides: "Chunker unit tests"
    - path: "rag-ingestion/tests/test_embedder.py"
      provides: "Embedder unit tests with mocked Ollama/OpenAI"
  key_links:
    - from: "rag-ingestion/src/bbj_rag/cli.py"
      to: "rag-ingestion/src/bbj_rag/pipeline.py"
      via: "cli ingest command calls run_pipeline()"
      pattern: "run_pipeline"
    - from: "rag-ingestion/src/bbj_rag/pipeline.py"
      to: "rag-ingestion/src/bbj_rag/chunker.py"
      via: "pipeline calls chunk_document() for each parsed Document"
      pattern: "chunk_document"
    - from: "rag-ingestion/src/bbj_rag/pipeline.py"
      to: "rag-ingestion/src/bbj_rag/db.py"
      via: "pipeline calls bulk_insert_chunks() for each embedded batch"
      pattern: "bulk_insert_chunks"
    - from: "rag-ingestion/src/bbj_rag/pipeline.py"
      to: "rag-ingestion/src/bbj_rag/embedder.py"
      via: "pipeline calls embedder.embed_batch() for each chunk batch"
      pattern: "embed_batch"
---

<objective>
Implement the full embedding pipeline: heading-aware chunker, embedding client abstraction (Ollama + OpenAI fallback), binary COPY bulk storage, pipeline orchestrator, and Click CLI with both full-pipeline and individual-stage commands.

Purpose: This is the core machinery that turns parsed Flare documents into searchable vectors in pgvector -- the bridge between Phase 11's intelligence layer and Phase 12-02's search validation.

Output: Six new source files (chunker.py, embedder.py, pipeline.py, cli.py, test_chunker.py, test_embedder.py) plus updates to config.py, db.py, schema.sql, and pyproject.toml. Running `bbj-rag ingest --source flare` will parse, classify, chunk, embed, and store all Flare documentation.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-embedding-pipeline/12-RESEARCH.md
@.planning/phases/11-bbj-intelligence/11-02-SUMMARY.md

# Existing source files to modify
@rag-ingestion/pyproject.toml
@rag-ingestion/sql/schema.sql
@rag-ingestion/src/bbj_rag/config.py
@rag-ingestion/src/bbj_rag/db.py
@rag-ingestion/src/bbj_rag/models.py
@rag-ingestion/src/bbj_rag/intelligence/__init__.py
@rag-ingestion/src/bbj_rag/parsers/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Foundation updates, chunker, and embedder with unit tests</name>
  <files>
    rag-ingestion/pyproject.toml
    rag-ingestion/sql/schema.sql
    rag-ingestion/src/bbj_rag/config.py
    rag-ingestion/src/bbj_rag/db.py
    rag-ingestion/src/bbj_rag/chunker.py
    rag-ingestion/src/bbj_rag/embedder.py
    rag-ingestion/tests/test_chunker.py
    rag-ingestion/tests/test_embedder.py
  </files>
  <action>
**1a. Update pyproject.toml** -- Add new dependencies to the `[project] dependencies` list:
- `"ollama>=0.6,<1"` -- Ollama Python client for local embedding inference
- `"click>=8.1,<9"` -- CLI framework
- `"pyyaml>=6.0,<7"` -- YAML parsing for validation cases

Add to `[dependency-groups] dev`:
- `"openai>=1.0,<2"` -- API fallback embedding provider (dev-only unless configured)

Add entry point section:
```toml
[project.scripts]
bbj-rag = "bbj_rag.cli:cli"
```

Run `cd rag-ingestion && uv sync` to install new dependencies.

**1b. Update sql/schema.sql** -- Change `vector(1536)` to `vector(1024)` on the embedding column. This matches Qwen3-Embedding-0.6B default output dimensions (1024). The HNSW index definition stays the same (it indexes whatever dimension the column uses). Add a comment noting the dimension change.

**1c. Update config.py** -- Update Settings defaults to match the new embedding model:
- `embedding_model` default: `"qwen3-embedding:0.6b"` (was `"text-embedding-3-small"`)
- `embedding_dimensions` default: `1024` (was `1536`)
- `chunk_size` default: `400` (was `512`) -- target tokens for heading-aware chunking
- `chunk_overlap` default: `50` (was `64`) -- ~12% overlap
- Add new field: `embedding_provider: str = Field(default="ollama")` -- "ollama" or "openai"
- Add new field: `embedding_batch_size: int = Field(default=64)` -- batch size for embedding inference

**1d. Update db.py** -- Add `bulk_insert_chunks()` function using psycopg3 binary COPY protocol via staging table pattern. Keep existing `insert_chunk()` and `insert_chunks_batch()` intact for backward compatibility.

The function signature:
```python
def bulk_insert_chunks(conn: psycopg.Connection[object], chunks: list[Chunk]) -> int:
```

Implementation pattern (from 12-RESEARCH.md Pattern 3):
1. Create temp table: `CREATE TEMP TABLE _chunks_staging (LIKE chunks INCLUDING DEFAULTS) ON COMMIT DROP`
2. Binary COPY into staging table with `set_types()` for all columns including `vector` and `jsonb`
3. `INSERT INTO chunks (...) SELECT ... FROM _chunks_staging ON CONFLICT (content_hash) DO NOTHING`
4. Return `cur.rowcount` (number of newly inserted rows)
5. Commit the transaction

Column list for COPY (must include context_header and deprecated which were added in Phase 11):
`source_url, title, doc_type, content, content_hash, context_header, generations, deprecated, embedding, metadata`

Also update `_INSERT_CHUNK_SQL` to include `context_header` and `deprecated` columns which were added in Phase 11 but missing from the INSERT statement. The current INSERT only has: source_url, title, doc_type, content, content_hash, generations, embedding, metadata. Add context_header and deprecated.

Update `_chunk_to_params()` to include `chunk.context_header` and `chunk.deprecated` in the tuple, matching the new column order.

**1e. Create chunker.py** -- Heading-aware markdown chunker. Module: `rag-ingestion/src/bbj_rag/chunker.py`

Public function:
```python
def chunk_document(doc: Document, max_tokens: int = 400, overlap_tokens: int = 50) -> list[Chunk]:
```

Implementation:
1. `_split_at_headings(content)` -- Split markdown at `##` and `###` heading boundaries. Return list of `(heading_path, section_text)` tuples. The heading_path is the accumulated heading hierarchy (e.g., `["Description", "Parameters"]`).
2. `_split_oversized(text, max_tokens, overlap_tokens)` -- If a section exceeds max_tokens, sub-split at paragraph (`\n\n`) boundaries, then sentence boundaries if paragraphs are still too large. Return list of text strings.
3. Code block preservation: Before splitting, identify fenced code blocks (``` ... ```). Treat them as atomic units -- never split inside a code block. If a code block alone exceeds max_tokens, keep it as a single chunk (BBj code examples should not be truncated).
4. Token approximation: Use `len(text.split()) / 0.75` as token count estimate (word count divided by 0.75). This is sufficient for chunking boundaries with 32K context models.
5. Context header prepending: For each chunk, build a context header using `build_context_header()` from the intelligence package with the document's section_path metadata, title, and the heading path for this specific chunk. Prepend the header to chunk content with `\n\n` separator.
6. Create chunks via `Chunk.from_content()` factory (preserves content_hash consistency).
7. Each chunk inherits `source_url`, `title`, `doc_type`, `generations`, `deprecated`, and `metadata` from the parent Document. The `context_header` field stores the header text separately.

Edge cases:
- Empty sections (heading with no body): skip, don't create empty chunks
- Document with no headings: treat entire content as one section
- Very small documents (below max_tokens): produce a single chunk

**1f. Create embedder.py** -- Embedding client abstraction. Module: `rag-ingestion/src/bbj_rag/embedder.py`

Define a `Protocol`:
```python
class Embedder(Protocol):
    def embed_batch(self, texts: list[str]) -> list[list[float]]: ...
    @property
    def dimensions(self) -> int: ...
```

Two implementations:

`OllamaEmbedder`:
- Constructor: `__init__(self, model: str = "qwen3-embedding:0.6b", dimensions: int = 1024)`
- `embed_batch()`: Call `ollama.embed(model=self._model, input=texts)`, return `response.embeddings`
- `dimensions` property returns `self._dimensions`

`OpenAIEmbedder`:
- Constructor: `__init__(self, model: str = "text-embedding-3-small", dimensions: int = 1024)`
- Lazy-import `from openai import OpenAI` in constructor (so openai is only needed when this provider is used)
- `embed_batch()`: Call `self._client.embeddings.create(model=self._model, input=texts, dimensions=self._dimensions)`, return `[e.embedding for e in response.data]`
- `dimensions` property returns `self._dimensions`

Factory function:
```python
def create_embedder(settings: Settings) -> Embedder:
    if settings.embedding_provider == "openai":
        return OpenAIEmbedder(model=settings.embedding_model, dimensions=settings.embedding_dimensions)
    return OllamaEmbedder(model=settings.embedding_model, dimensions=settings.embedding_dimensions)
```

**1g. Create test_chunker.py** -- Unit tests for the chunker:
- Test: single section document produces one chunk
- Test: multi-heading document splits at headings
- Test: oversized section sub-splits at paragraph boundaries
- Test: code blocks are never split (fenced ``` blocks stay intact)
- Test: empty sections are skipped
- Test: context header is prepended to chunk content
- Test: chunk inherits Document metadata (source_url, generations, doc_type, etc.)
- Test: document with no headings produces chunk(s) from full content
- Test: very small document produces single chunk
- Test: chunk content_hash is deterministic (same content = same hash)

All tests use in-memory Document objects (no DB, no network). Build test Documents using the Document model directly.

**1h. Create test_embedder.py** -- Unit tests with mocked backends:
- Test: OllamaEmbedder.embed_batch calls ollama.embed with correct model and input (mock ollama module)
- Test: OpenAIEmbedder.embed_batch calls openai client with correct params (mock openai module)
- Test: create_embedder returns OllamaEmbedder when provider is "ollama"
- Test: create_embedder returns OpenAIEmbedder when provider is "openai"
- Test: dimensions property returns configured value

Use `unittest.mock.patch` to mock the ollama and openai libraries. Do NOT call actual embedding APIs.
  </action>
  <verify>
Run from rag-ingestion/:
```bash
uv sync
uv run pytest tests/test_chunker.py tests/test_embedder.py -v
uv run ruff check src/bbj_rag/chunker.py src/bbj_rag/embedder.py
uv run ruff format --check src/bbj_rag/chunker.py src/bbj_rag/embedder.py
```
All tests pass. No ruff lint or format errors. `uv sync` installs ollama, click, pyyaml without errors.

Also verify full existing test suite still passes:
```bash
uv run pytest -x
```
No regressions (all prior tests still pass).
  </verify>
  <done>
- schema.sql uses vector(1024)
- config.py defaults: qwen3-embedding:0.6b, 1024 dims, 400 chunk size, 50 overlap, ollama provider, 64 batch size
- db.py has bulk_insert_chunks() using binary COPY protocol and existing insert functions include context_header/deprecated
- chunker.py splits at headings, preserves code blocks, prepends context headers
- embedder.py has Embedder protocol, OllamaEmbedder, OpenAIEmbedder, create_embedder factory
- pyproject.toml has ollama, click, pyyaml deps and bbj-rag entry point
- test_chunker.py and test_embedder.py pass with 10+ tests each
- Full test suite passes with no regressions
  </done>
</task>

<task type="auto">
  <name>Task 2: Pipeline orchestrator and Click CLI</name>
  <files>
    rag-ingestion/src/bbj_rag/pipeline.py
    rag-ingestion/src/bbj_rag/cli.py
  </files>
  <action>
**2a. Create pipeline.py** -- Pipeline orchestrator. Module: `rag-ingestion/src/bbj_rag/pipeline.py`

Public function:
```python
def run_pipeline(
    parser,           # DocumentParser protocol instance
    embedder,         # Embedder protocol instance
    conn,             # psycopg.Connection
    batch_size: int = 64,
    resume: bool = False,
    max_tokens: int = 400,
    overlap_tokens: int = 50,
) -> dict[str, int]:
```

Implementation:
1. Initialize stats dict: `{"docs_parsed": 0, "chunks_created": 0, "chunks_embedded": 0, "chunks_stored": 0}`
2. Import intelligence functions: `tag_generation`, `classify_doc_type`, `build_context_header`, `extract_heading_hierarchy`
3. For each `doc` from `parser.parse()`:
   a. Apply intelligence: `doc.generations = tag_generation(doc)`, `doc.doc_type = classify_doc_type(doc)` (using heading hierarchy and path)
   b. Apply context header: Compute with `build_context_header()` using doc metadata, title, and headings
   c. Chunk with `chunk_document(doc, max_tokens, overlap_tokens)`
   d. Accumulate chunks into batch
   e. When batch reaches `batch_size`:
      - Call `embedder.embed_batch([c.content for c in batch])` to get vectors
      - Assign `chunk.embedding = vector` for each chunk/vector pair
      - Call `bulk_insert_chunks(conn, batch)` to store
      - Update stats, log progress with `logger.info()`
      - Clear batch
4. Process final partial batch
5. Log final stats summary
6. Return stats dict

For the intelligence application step, the pipeline needs to:
- Extract headings from doc.content using `extract_heading_hierarchy(doc.content)`
- Call `classify_doc_type(headings, doc.source_url, doc.content)` to get the doc type string
- Call `tag_generation(doc.source_url, doc.metadata)` with the document's source URL and metadata (which contains condition tags from the Flare parser)
- Call `build_context_header(section_path, title, heading_path)` where section_path comes from `doc.metadata.get("section_path", "")`

Read the intelligence module's actual function signatures from the source files before implementing to ensure correct argument passing. The intelligence functions take different arguments than the simplified overview above -- check `generations.py`, `doc_types.py`, and `context_headers.py` for exact signatures.

Resume mode: If `resume=True`, before inserting each batch, check which content_hashes already exist in the database and skip those chunks. A simple approach: query `SELECT content_hash FROM chunks WHERE content_hash = ANY(%s)` with the batch's hashes, then filter out existing ones before embedding. This avoids re-embedding chunks that are already stored.

**2b. Create cli.py** -- Click CLI entry point. Module: `rag-ingestion/src/bbj_rag/cli.py`

Structure:
```python
@click.group()
@click.option('--verbose', '-v', is_flag=True, help='Enable debug logging')
def cli(verbose: bool) -> None:
    """BBj RAG ingestion pipeline."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s %(levelname)s %(name)s: %(message)s',
    )
```

Commands:

**`bbj-rag ingest`** -- Full pipeline:
```python
@cli.command()
@click.option('--source', type=click.Choice(['flare']), required=True, help='Documentation source')
@click.option('--resume', is_flag=True, help='Skip already-stored chunks (dev mode)')
@click.option('--batch-size', default=64, type=int, help='Embedding batch size')
def ingest(source: str, resume: bool, batch_size: int) -> None:
```
- Load Settings from config.toml
- Create parser based on source (FlareParser for 'flare')
- Create embedder via `create_embedder(settings)`
- Open DB connection via `get_connection(settings.database_url)`
- Call `run_pipeline(parser, embedder, conn, batch_size, resume)`
- Print summary stats
- Close connection

**`bbj-rag parse`** -- Parse only (no embedding):
```python
@cli.command()
@click.option('--source', type=click.Choice(['flare']), required=True)
def parse(source: str) -> None:
```
- Load Settings, create parser
- Iterate parser.parse(), count documents
- Print count and sample titles (useful for debugging parser output)

**`bbj-rag validate`** -- Run search validation (placeholder for Plan 12-02):
```python
@cli.command()
def validate() -> None:
    """Run search validation assertions."""
    click.echo("Search validation not yet implemented. See Plan 12-02.")
```

The Flare parser instantiation: Read the FlareParser constructor signature from `rag-ingestion/src/bbj_rag/parsers/flare.py` to understand what arguments it needs (likely a path to the Content directory from `settings.flare_source_path`). Wire it correctly.

Error handling in CLI:
- If Ollama is not running or model not found, catch the error and print a helpful message: "Ollama not available. Run: ollama pull qwen3-embedding:0.6b"
- If database connection fails, print connection error with the URL (mask password if present)
- Wrap pipeline execution in try/except, log errors, and exit with non-zero code on failure
  </action>
  <verify>
Run from rag-ingestion/:
```bash
uv run ruff check src/bbj_rag/pipeline.py src/bbj_rag/cli.py
uv run ruff format --check src/bbj_rag/pipeline.py src/bbj_rag/cli.py
uv run bbj-rag --help
uv run bbj-rag ingest --help
uv run bbj-rag parse --help
uv run bbj-rag validate
uv run pytest -x
```
- Ruff passes with no errors
- `bbj-rag --help` shows the three commands (ingest, parse, validate)
- `bbj-rag ingest --help` shows --source, --resume, --batch-size options
- `bbj-rag validate` prints the placeholder message
- Full test suite still passes
  </verify>
  <done>
- pipeline.py orchestrates parse -> intelligence -> chunk -> embed -> store with batch processing and progress logging
- pipeline.py supports --resume mode (skip already-stored chunks)
- cli.py provides `bbj-rag ingest --source flare` for full pipeline
- cli.py provides `bbj-rag parse --source flare` for parse-only debugging
- cli.py provides `bbj-rag validate` as placeholder for Plan 12-02
- Error handling for missing Ollama model and database connection failures
- bbj-rag entry point registered in pyproject.toml and functional
- Full test suite passes with no regressions
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. `uv sync` in rag-ingestion/ installs all dependencies without errors
2. `uv run pytest -x` passes all tests (existing + new chunker/embedder tests)
3. `uv run ruff check src/` passes with no errors
4. `uv run ruff format --check src/` passes with no formatting issues
5. `uv run bbj-rag --help` shows available commands
6. `uv run bbj-rag ingest --help` shows --source, --resume, --batch-size options
7. sql/schema.sql shows vector(1024) not vector(1536)
8. config.py defaults show qwen3-embedding:0.6b, 1024 dimensions, ollama provider
</verification>

<success_criteria>
- Heading-aware chunker splits documents at heading boundaries, preserves code blocks, prepends context headers
- Embedding abstraction supports Ollama (primary) and OpenAI (fallback) with batch processing
- Binary COPY bulk insert stores chunks with all metadata columns including context_header and deprecated
- Pipeline orchestrates parse -> intelligence -> chunk -> embed -> store with batch processing and stage-by-stage logging
- CLI provides both full-pipeline (`ingest`) and individual-stage (`parse`) commands
- Schema uses vector(1024) matching Qwen3-Embedding-0.6B
- All tests pass, no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/12-embedding-pipeline/12-01-SUMMARY.md`
</output>
