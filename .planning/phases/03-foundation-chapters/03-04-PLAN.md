---
phase: 03-foundation-chapters
plan: 04
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - docs/03-fine-tuning/index.md
autonomous: true

must_haves:
  truths:
    - "A developer reader understands the full fine-tuning pipeline: training data structure, base model selection rationale, QLoRA approach, and Ollama hosting"
    - "Base model recommendation is current (Qwen2.5-Coder, not CodeLlama/StarCoder2) with rationale and alternatives"
    - "The LoRA/QLoRA approach is explained with enough detail that a developer could begin implementation"
    - "The Unsloth -> GGUF -> Ollama pipeline is described as a concrete, actionable workflow"
    - "The chapter uses TL;DR block, decision callouts, Mermaid diagrams, and BBj syntax highlighting"
    - "The chapter ends with a Current Status section"
  artifacts:
    - path: "docs/03-fine-tuning/index.md"
      provides: "Complete Chapter 3 content"
      min_lines: 200
  key_links:
    - from: "docs/03-fine-tuning/index.md"
      to: "prism-bbj.js"
      via: "bbj fenced code blocks"
      pattern: "```bbj"
    - from: "docs/03-fine-tuning/index.md"
      to: "theme-mermaid"
      via: "mermaid code blocks"
      pattern: "```mermaid"
---

<objective>
Research and write Chapter 3: "Fine-Tuning the Model" -- a complete, standalone chapter covering the technical approach to creating a BBj-aware language model.

Purpose: This is the most technically dense chapter in the foundation set. It must give a developer enough detail to understand (and eventually begin) the fine-tuning process, while also making the case to leadership that this is a practical, affordable approach. The chapter must reflect the current (2025/2026) state of the art, not the January 2025 paper's recommendations.

Output: A fully written chapter at `docs/03-fine-tuning/index.md` that replaces the current placeholder content.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-foundation-chapters/03-CONTEXT.md
@.planning/phases/03-foundation-chapters/03-RESEARCH.md

## Source Material

Read `/Users/beff/_workspace/bbj-ai-strategy/bbj-llm-strategy.md` for domain knowledge. The relevant sections are:
- "Component 1: Fine-Tuned BBj Language Model" (line ~204-310) -- training objectives, data structure, model selection
- "Appendix B: Sample Training Data Format" (line ~1009-1125) -- JSON training data schema and examples
- "Resource Requirements" (line ~820-860) -- infrastructure and costs
- "Implementation Roadmap" (line ~751-820) -- phases and timeline

**CRITICAL:** The source paper is from January 2025. The AI/ML landscape has shifted significantly. Use the RESEARCH.md findings (below) to update all model and tool recommendations. Do NOT recommend CodeLlama or StarCoder2 as primary options -- they have been surpassed by Qwen2.5-Coder.

## Key Research Findings (from 03-RESEARCH.md) -- MUST be incorporated:

### Base Model Selection
- **Primary recommendation:** Qwen2.5-Coder-7B-Base (Apache 2.0, 5.5T tokens, 92+ languages, FIM support)
- **Why not CodeLlama:** Surpassed on all benchmarks by Qwen2.5-Coder. Mention historically but not as primary.
- **Why not StarCoder2:** Same -- benchmark-surpassed, not competitive for new projects.
- **Qwen2.5-Coder 7B-Instruct:** 88.4% HumanEval, surpasses models 3x its size
- **Qwen2.5-Coder 32B-Instruct:** Matches GPT-4o on code generation
- **Size recommendation:** 7B for single-GPU fine-tuning and customer self-hosting; 14B/32B if hardware allows
- **Qwen3-Coder (July 2025):** Mention as newer option but only ships in large MoE sizes. Qwen2.5-Coder base models are more practical for fine-tuning.

### Fine-Tuning Approach
- **QLoRA via Unsloth:** 2x speed, 70% less VRAM, 0% accuracy loss vs vanilla QLoRA. Dominant tool in 2025.
- **Single RTX 4090 ($1,500) can fine-tune a 7B model** -- headline for leadership audience
- **Apply LoRA to all layers** (attention + MLP), not just attention
- **1-3 epochs sufficient** for instruction tuning
- **Data quality > quantity:** 1,000 excellent examples can outperform 10,000 mediocre ones
- **Beware catastrophic forgetting:** Evaluate general code ability alongside BBj-specific ability
- **Sequential fine-tuning:** Continued pretraining on BBj corpus, then instruction fine-tuning
- **Synthetic data generation:** Use larger models to generate BBj training examples from docs

### Pipeline
1. Fine-tune Qwen2.5-Coder-7B-Base using QLoRA via Unsloth
2. Export to GGUF format using llama.cpp's `convert_hf_to_gguf.py`
3. Create Ollama Modelfile pointing to GGUF
4. Serve via `ollama serve` with OpenAI-compatible API
5. Customers self-host with `ollama run bbj-coder`

### Ollama Hosting
- Version 0.9.x+, native GGUF import, LoRA adapter support
- Desktop apps (macOS/Windows), OpenAI + Anthropic API compatible
- Direct Hugging Face integration
- Q4_0 default quantization; NVFP4/FP8 on RTX GPUs

### Training Data
- JSON format with: code, description, generation label, context, tags
- Generation labels: "all", "character", "vpro5", "bbj-gui", "dwc"
- Need both universal patterns and generation-specific examples
- 10,000-50,000 examples target, but quality over quantity

## Content Patterns (from Phase 2)

- TL;DR: `:::tip[TL;DR]` admonition at the top
- Decision callouts: `:::info[Decision: ...]` with Choice, Rationale, Alternatives, Status fields
- Mermaid diagrams: ````mermaid` code blocks for flow/pipeline diagrams
- BBj code: ````bbj title="..."` fenced blocks with syntax highlighting
- This chapter stays as `.md` (no Tabs needed)

## Writing Voice (from CONTEXT.md)

- Tone: authoritative practitioner -- confident, direct
- Body text is developer-first and technical
- TL;DR blocks carry non-technical audiences
- State decisions confidently, note alternatives briefly ("We selected X because Y. Z was considered but...")
- Inline citations with links for key claims (benchmarks, papers, tools)
- Frame model/tool recommendations as "as of [date]" -- the field moves fast
</context>

<tasks>

<task type="auto">
  <name>Task 1: Research and write Chapter 3 â€” Fine-Tuning the Model</name>
  <files>docs/03-fine-tuning/index.md</files>
  <action>
Read the source material at `/Users/beff/_workspace/bbj-ai-strategy/bbj-llm-strategy.md` (especially lines 204-310, 820-860, and appendix B at lines 1009-1125) to understand the training data structure, model selection rationale, and pipeline. Then write Chapter 3 from scratch, updating all model and tool recommendations to reflect the current state of the art per the research findings in the context section above.

**This file stays as `.md`** -- no JSX components needed.

**Chapter structure -- follow this outline but use Claude's discretion for exact headings and depth:**

1. **Frontmatter** -- Update: sidebar_position: 1 (keep as-is, it's first in the fine-tuning folder), title: "Fine-Tuning the Model", improve description to be more descriptive

2. **TL;DR block** -- `:::tip[TL;DR]` with 3-4 sentences: Fine-tuned Qwen2.5-Coder-7B on generation-labeled BBj training data using QLoRA, served locally via Ollama. A $1,500 GPU can train what would cost $50K+ for full fine-tuning. Self-hosting gives customers data privacy.

3. **Opening** -- The foundation of the entire BBj AI strategy is a fine-tuned language model. Without it, no IDE extension, no documentation chat, no AI assistance of any kind works for BBj. This chapter covers how to build it.

4. **Training Data Structure** -- The most important section for implementation:
   - JSON format with required fields: code, description, generation, context, tags
   - Generation labels and their meaning: "all", "character", "vpro5", "bbj-gui", "dwc"
   - Show 2-3 JSON training example snippets (one "all" universal, one generation-specific)
   - Explain the universal vs generation-specific split
   - Data quality emphasis: curated, correct, well-labeled examples
   - Volume target: 10,000-50,000 examples, but quality > quantity
   - Synthetic data strategy: using larger models to generate BBj examples from documentation
   - Include a BBj code block showing what kind of code gets included in training data

5. **Base Model Selection** -- Key decision section with citations:
   - `:::info[Decision: Qwen2.5-Coder-7B-Base as Starting Point]`
   - Why Qwen2.5-Coder: benchmarks (cite HumanEval scores), Apache 2.0, FIM support, size variety
   - Size rationale: 7B for practical single-GPU training and customer self-hosting; 14B/32B noted as upgrades
   - Comparison table: Qwen2.5-Coder vs CodeLlama vs StarCoder2 vs DeepSeek-V3 (with why each alternative is less suitable)
   - Mention Qwen3-Coder as newer option but explain why Qwen2.5-Coder base models are more practical (dense sizes, documented workflows)
   - Frame as "as of January 2026" -- recommend re-evaluating when starting implementation
   - Include links to Qwen2.5-Coder blog and arxiv paper

6. **The LoRA/QLoRA Approach** -- Technical explanation accessible to developers:
   - What LoRA is: train small adapter matrices instead of modifying all model weights
   - What QLoRA adds: quantize base model to 4-bit, train LoRA adapters at full precision
   - Why this matters: $1,500 GPU vs $50K+ full fine-tuning (headline for leadership)
   - Key hyperparameters: rank, alpha, target modules (all layers, not just attention)
   - Training details: 1-3 epochs, quality over hyperparameter tuning
   - Catastrophic forgetting warning: evaluate general code ability alongside BBj ability
   - Sequential fine-tuning approach: continued pretraining then instruction fine-tuning
   - Pipeline diagram (Mermaid): Training Data -> QLoRA Fine-Tuning (Unsloth) -> Merged Model -> GGUF Export -> Ollama Deployment

7. **Toolchain: Unsloth + llama.cpp + Ollama** -- The practical pipeline:
   - Unsloth for training: 2x speed, 70% less VRAM, zero accuracy loss
   - llama.cpp for GGUF conversion: `convert_hf_to_gguf.py`
   - Ollama for serving: Modelfile creation, `ollama serve`, OpenAI-compatible API
   - Include a Mermaid flow diagram showing the pipeline steps
   - Brief mention of alternatives: LLaMA-Factory, Axolotl (for completeness, not primary recommendation)

8. **Hosting via Ollama** -- Self-hosting as a differentiator:
   - Why self-hosting matters: customer data privacy, no cloud dependency, predictable costs
   - Ollama capabilities: native GGUF import, desktop apps, API compatibility, quantization options
   - Deployment model: customers run `ollama run bbj-coder` on their own hardware
   - Hardware requirements: basic (8GB VRAM for 7B Q4), recommended (16GB+ VRAM for better quality)
   - `:::info[Decision: Ollama for Local Model Serving]` callout

9. **Current Status** -- Where things stand honestly:
   - Training data structure defined, curation in progress
   - Base model selected (Qwen2.5-Coder-7B-Base), pending first training run
   - Unsloth pipeline tested with sample data
   - Ollama deployment verified with stock Qwen models
   - What's next: scale training data, run first fine-tuning, evaluate on BBj benchmarks

**Mermaid diagrams:** At least 2 -- the fine-tuning pipeline flow and the deployment/hosting architecture. These help developers visualize the workflow.

**BBj code examples:** Show BBj code that would appear in training data. Show examples of what the trained model would be able to complete.

**JSON examples:** Show 2-3 training data JSON snippets illustrating the format.

**Citations:** Include inline links for key claims:
- Qwen2.5-Coder benchmarks: link to blog and/or arxiv
- Unsloth performance claims: link to Unsloth docs
- Ollama capabilities: link to Ollama docs

**Length guidance:** This is the most technically dense chapter. It needs to be substantial -- likely 250-400 lines. But stay focused on actionable content. Every section should serve either the developer reader (implementation detail) or the leadership reader (cost/feasibility argument).
  </action>
  <verify>
- `npm run build` completes with zero errors
- `grep -c "mermaid" docs/03-fine-tuning/index.md` returns 2+ (pipeline + hosting diagrams)
- `grep -q ":::tip\[TL;DR\]" docs/03-fine-tuning/index.md` confirms TL;DR block
- `grep -q ":::info\[Decision:" docs/03-fine-tuning/index.md` confirms decision callout(s)
- `grep -q "## Current Status" docs/03-fine-tuning/index.md` confirms status section
- `grep -q "Qwen2.5-Coder" docs/03-fine-tuning/index.md` confirms updated model recommendation
- `grep -q "Unsloth" docs/03-fine-tuning/index.md` confirms current toolchain
- `grep -q "Ollama" docs/03-fine-tuning/index.md` confirms hosting coverage
- File is at least 200 lines (substantive content)
- No "Coming Soon" placeholder text remains
  </verify>
  <done>
- Chapter 3 is a complete, technically dense chapter covering the full fine-tuning approach
- Model recommendation is current (Qwen2.5-Coder, not CodeLlama/StarCoder2) with rationale and citations
- QLoRA/LoRA approach is explained with enough detail for a developer to begin implementation
- Unsloth -> GGUF -> Ollama pipeline is described as a concrete workflow
- Training data structure is documented with JSON examples
- Ollama self-hosting is positioned as a key differentiator
- TL;DR, decision callouts, Mermaid diagrams, and Current Status section are all present
- A developer reader finishes the chapter knowing how to approach the fine-tuning work
- A leadership reader finishes understanding that this is practical and affordable
- The site builds cleanly with `npm run build`
  </done>
</task>

</tasks>

<verification>
- `npm run build` passes
- Chapter content is substantial (200+ lines), not placeholder
- All content patterns used: TL;DR, decision callout(s), Mermaid diagrams, BBj code blocks
- Model recommendations are current (Qwen2.5-Coder, not CodeLlama)
- Fine-tuning pipeline is concrete and actionable
- Training data structure documented with examples
- Ollama hosting covered with hardware guidance
- Current Status section exists
- No "Coming Soon" placeholder text remains
- Inline citations with links for key claims
</verification>

<success_criteria>
Chapter 3 is the technical blueprint for the fine-tuning initiative. A developer finishes it understanding the training data format, why Qwen2.5-Coder was selected, how QLoRA works with Unsloth, and how the resulting model gets deployed via Ollama. A leadership reader finishes it understanding that fine-tuning is practical (single $1,500 GPU), the toolchain is mature, and self-hosting gives customers data privacy.
</success_criteria>

<output>
After completion, create `.planning/phases/03-foundation-chapters/03-04-SUMMARY.md`
</output>
