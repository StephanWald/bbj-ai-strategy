---
phase: 25-result-quality-foundation
plan: 03
type: execute
wave: 2
depends_on: ["25-01"]
files_modified:
  - rag-ingestion/src/bbj_rag/pipeline.py
  - rag-ingestion/src/bbj_rag/chunker.py
  - rag-ingestion/scripts/validate_e2e.py
autonomous: true

must_haves:
  truths:
    - "Ingestion pipeline computes source_type and display_url for every chunk during ingestion"
    - "Chunks created by chunk_document() carry source_type and display_url from the parent Document"
    - "E2E validation script checks for display_url and source_type in search results"
    - "E2E validation script checks for source_type_counts in search response metadata"
  artifacts:
    - path: "rag-ingestion/src/bbj_rag/pipeline.py"
      provides: "url_mapping integration: calls classify_source_type and map_display_url before chunking"
      contains: "classify_source_type"
    - path: "rag-ingestion/src/bbj_rag/chunker.py"
      provides: "chunk_document passes source_type and display_url from Document to Chunk"
      contains: "source_type"
    - path: "rag-ingestion/scripts/validate_e2e.py"
      provides: "Assertions for display_url, source_type, and source_type_counts in search results"
      contains: "display_url"
  key_links:
    - from: "rag-ingestion/src/bbj_rag/pipeline.py"
      to: "rag-ingestion/src/bbj_rag/url_mapping.py"
      via: "import classify_source_type and map_display_url"
      pattern: "from bbj_rag.url_mapping import"
    - from: "rag-ingestion/src/bbj_rag/chunker.py"
      to: "rag-ingestion/src/bbj_rag/models.py"
      via: "Chunk.from_content receives source_type and display_url params"
      pattern: "source_type=doc\\.source_type"
---

<objective>
Wire URL mapping into the ingestion pipeline so new ingestion runs populate source_type and display_url, update the chunker to pass these fields through, and extend the E2E validation script to verify the new response fields.

Purpose: Completes the ingestion-side integration so future corpus rebuilds automatically populate the new columns. The validation script update ensures Phase 25 success criteria can be verified end-to-end.

Output: Pipeline and chunker with url_mapping integration, plus an enhanced E2E validation script.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/25-result-quality-foundation/25-CONTEXT.md
@.planning/phases/25-result-quality-foundation/25-RESEARCH.md
@.planning/phases/25-result-quality-foundation/25-01-SUMMARY.md

@rag-ingestion/src/bbj_rag/pipeline.py
@rag-ingestion/src/bbj_rag/chunker.py
@rag-ingestion/scripts/validate_e2e.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire URL mapping into pipeline and chunker</name>
  <files>
    rag-ingestion/src/bbj_rag/pipeline.py
    rag-ingestion/src/bbj_rag/chunker.py
  </files>
  <action>
**pipeline.py changes:**
1. Add import: `from bbj_rag.url_mapping import classify_source_type, map_display_url`
2. In `run_pipeline()`, after the intelligence enrichment block (the `if doc.doc_type and doc.doc_type != "web_crawl":` / `else:` block), add URL mapping for ALL documents (not conditional -- every doc gets source_type and display_url):
   ```python
   # Compute source_type and display_url for URL mapping.
   source_type = classify_source_type(doc.source_url)
   display_url = map_display_url(doc.source_url)
   doc = doc.model_copy(
       update={
           "source_type": source_type,
           "display_url": display_url,
       }
   )
   ```
   Place this after the intelligence block but before `chunk_document()` call. This ensures every document (Flare, non-Flare, web_crawl) gets the mapping applied.

**chunker.py changes:**
In `chunk_document()`, update the `Chunk.from_content()` call to pass through the new fields from the Document:
- Add `source_type=doc.source_type` parameter
- Add `display_url=doc.display_url` parameter

These get passed alongside the existing `source_url=doc.source_url`, `title=doc.title`, etc. parameters.
  </action>
  <verify>
1. `cd /Users/beff/_workspace/bbj-ai-strategy/rag-ingestion && uv run python -c "from bbj_rag.pipeline import run_pipeline; print('pipeline OK')"` -- imports without error.
2. `uv run python -c "from bbj_rag.chunker import chunk_document; from bbj_rag.models import Document; d = Document(source_url='flare://Content/test.htm', title='T', doc_type='flare', content='Test content here', generations=['bbj'], source_type='flare', display_url='https://documentation.basis.cloud/BASISHelp/WebHelp/test.htm'); chunks = chunk_document(d); print(chunks[0].source_type, chunks[0].display_url)"` -- prints `flare https://documentation.basis.cloud/BASISHelp/WebHelp/test.htm`.
3. `uv run pytest tests/ -v -k "not test_mcp and not test_e2e"` -- existing tests pass.
  </verify>
  <done>
Pipeline computes source_type and display_url via url_mapping for every document before chunking. Chunker passes both fields through to Chunk.from_content(). New ingestion runs will populate the columns automatically.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend E2E validation script with display_url and source_type assertions</name>
  <files>
    rag-ingestion/scripts/validate_e2e.py
  </files>
  <action>
Read the full `validate_e2e.py` to understand its structure, then add:

1. **New assertion in REST API result checks:**
   For each search result returned by the REST API, assert:
   - `display_url` key exists and is a non-empty string
   - `source_type` key exists and is a non-empty string
   - If source_type is `"flare"`, verify display_url starts with `https://documentation.basis.cloud/`
   - If source_type is `"wordpress"` or `"web_crawl"`, verify display_url starts with `https://`
   - If source_type is `"pdf"`, `"bbj_source"`, or `"mdx"`, verify display_url starts with `[`

2. **New assertion for source_type_counts:**
   After a search response is received, check that `source_type_counts` exists in the response JSON and is a dict with string keys and integer values.

3. **Source diversity check:**
   Add a specific test query designed to exercise diversity reranking. Use query `"BBj GUI programming window button example"` with limit=10. After receiving results, count distinct source_types. Log whether multiple source types appeared. This is informational (not a hard pass/fail) since it depends on corpus content, but log a warning if all 10 results are the same source_type.

4. **Update report generation:**
   If the validation script generates a VALIDATION.md report, add a section for "URL Mapping & Source Diversity" that includes:
   - Whether display_url was present on all results
   - Whether source_type_counts was present
   - The diversity test query results (source types found)

Keep changes additive -- don't break existing assertions or report sections.
  </action>
  <verify>
`cd /Users/beff/_workspace/bbj-ai-strategy/rag-ingestion && uv run python -c "import scripts.validate_e2e; print('validate script OK')"` or `uv run python scripts/validate_e2e.py --help` -- script is syntactically valid. (Full run requires Docker services, so syntax check is sufficient for plan verification.)
  </verify>
  <done>
E2E validation script checks display_url and source_type on every search result, verifies source_type_counts in response metadata, and includes a diversity test query with informational source type breakdown.
  </done>
</task>

</tasks>

<verification>
1. `uv run python -c "from bbj_rag.pipeline import run_pipeline; print('OK')"` -- pipeline imports with url_mapping
2. `uv run python -c "from bbj_rag.chunker import chunk_document; from bbj_rag.models import Document; d = Document(source_url='flare://Content/test.htm', title='T', doc_type='flare', content='Test content', generations=['bbj'], source_type='flare', display_url='https://example.com'); c = chunk_document(d); assert c[0].source_type == 'flare'; print('chunker OK')"` -- fields pass through
3. `uv run pytest tests/ -v -k "not test_mcp and not test_e2e"` -- existing tests pass
4. Validation script is syntactically valid
</verification>

<success_criteria>
- pipeline.py imports and calls classify_source_type + map_display_url for every document
- chunker.py passes source_type and display_url through to Chunk.from_content()
- validate_e2e.py asserts display_url and source_type on search results
- validate_e2e.py checks source_type_counts in response metadata
- validate_e2e.py includes diversity test query
- All existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/25-result-quality-foundation/25-03-SUMMARY.md`
</output>
