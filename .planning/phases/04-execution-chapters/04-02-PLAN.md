---
phase: 04-execution-chapters
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/05-documentation-chat/index.md
autonomous: true

must_haves:
  truths:
    - "A reader understands why generic chat services (kapa.ai, Algolia) fail for BBj specifically"
    - "The chapter presents architectural requirements without committing to a specific deployment model"
    - "Generation-aware response design is explained with concrete before/after examples"
    - "The shared infrastructure concept (same model + RAG as IDE) is clear"
    - "A Current Status section honestly states this is vision-defined but not yet built"
  artifacts:
    - path: "docs/05-documentation-chat/index.md"
      provides: "Chapter 5: Documentation Chat"
      min_lines: 250
      contains: "TL;DR"
  key_links:
    - from: "docs/05-documentation-chat/index.md"
      to: "/docs/fine-tuning"
      via: "cross-reference link"
      pattern: "/docs/fine-tuning"
    - from: "docs/05-documentation-chat/index.md"
      to: "/docs/rag-database"
      via: "cross-reference link"
      pattern: "/docs/rag-database"
---

<objective>
Write Chapter 5: Documentation Chat -- covering why generic chat services fail for BBj, the architectural requirements for a generation-aware documentation chat system, deployment model options, streaming response design, and the shared infrastructure advantage.

Purpose: Chapter 5 presents the second consumer application of the shared AI infrastructure. It explains why BBj needs its own documentation chat and how to architect one. The vision is still forming, so the chapter frames requirements and options rather than a locked-in deployment.

Output: Complete Chapter 5 replacing the current placeholder (~14 lines) with a researched chapter (~250-350 lines) following established content patterns.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-execution-chapters/04-CONTEXT.md
@.planning/phases/04-execution-chapters/04-RESEARCH.md

# Source material from the strategy paper (lines 501-607)
@bbj-llm-strategy.md

# Pattern reference
@docs/03-fine-tuning/index.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write Chapter 5 -- Documentation Chat</name>
  <files>docs/05-documentation-chat/index.md</files>
  <action>
Read the source material and research, then replace the placeholder content in docs/05-documentation-chat/index.md with a complete chapter.

**Source material to absorb before writing:**
1. bbj-llm-strategy.md lines 501-607 (Component 3: Documentation Chat System)
2. .planning/phases/04-execution-chapters/04-RESEARCH.md -- Chapter 5 sections (Why Generic Services Fail, Self-Hosted Chat Architecture Options, Streaming Response Pattern)
3. .planning/phases/04-execution-chapters/04-CONTEXT.md -- Chapter 5 framing: vision still forming, document architectural requirements and design principles without committing to deployment model

**Required structure (follow this order):**

1. **Frontmatter** -- keep existing sidebar_position: 1, update title and description
2. **TL;DR block** (:::tip[TL;DR]) -- 2-3 sentences: why generic services fail, generation-aware chat leveraging shared model + RAG, same infrastructure as IDE
3. **Opening paragraph** -- Frame the problem: developers asking BBj questions deserve accurate, generation-appropriate answers with source citations. Connect to Chapter 1 (generic LLMs fail) and Chapter 2 (shared infrastructure).
4. **Why Generic Services Fail** -- Concrete comparison table: kapa.ai, Algolia Ask AI, GitHub Copilot Chat -- how each works and why each fails for BBj. kapa.ai has 200+ enterprise customers (Docker, OpenAI, Nokia) proving the concept works for mainstream tech but NOT for BBj. The core problem: generic LLMs have zero BBj training data.
5. **Architectural Requirements** -- List the 5-6 non-negotiable requirements the chat system must meet: (1) uses BBj-aware model, (2) retrieves generation-tagged documentation, (3) streams responses in real-time, (4) provides source citations, (5) adapts answers to user's target generation, (6) leverages shared infrastructure. Decision callout (:::info[Decision: ...]) on shared infrastructure: same Ollama + RAG as IDE.
6. **Generation-Aware Response Design** -- The core value proposition. Show the strategy paper's two examples: "How do I create a window?" answered without context (defaults to modern DWC) and with legacy context (Visual PRO/5 WINDOW CREATE). Include the BBj code blocks with syntax highlighting. Explain how generation hints flow from query analysis to RAG filtering to prompt assembly.
7. **Chat Architecture** -- Mermaid sequence diagram from RESEARCH.md (User -> Chat Widget -> Backend -> RAG + LLM -> streaming response). Explain each step: query receipt, generation hint detection, RAG retrieval with generation filter, enriched prompt assembly, LLM streaming, response with citations.
8. **Deployment Options** -- Three options as a table with trade-offs: embedded widget, standalone service, hybrid. Per CONTEXT.md this is Claude's Discretion. Frame the hybrid approach as architecturally recommended but present all three. Note that the backend architecture (RAG + Ollama + streaming) is identical regardless of frontend choice.
9. **Streaming and Citations** -- SSE (Server-Sent Events) as the standard pattern. Brief description of the streaming flow: POST query -> process through RAG -> SSE stream -> tokens + citations. Note compatibility with Ollama's streaming API.
10. **Conversation Context** -- Brief section on session memory: conversation history enables follow-up questions ("What about the Visual PRO/5 version?") without restating context.
11. **Current Status** -- :::note[Where Things Stand -- January 2026] admonition. Shipped: nothing (this is planned). Defined: architectural requirements and design principles (this chapter). Planned: chat backend, widget integration, RAG pipeline dependency (see Chapter 6). Be honest that the deployment model is not yet decided.
12. **Cross-references** -- Link to /docs/fine-tuning (shared model), /docs/rag-database (shared retrieval), /docs/ide-integration (parallel consumer), /docs/strategic-architecture (shared infrastructure concept), /docs/implementation-roadmap (timeline).

**Content patterns to follow:**
- :::tip[TL;DR] at top
- At least 1 :::info[Decision: ...] callout
- At least 1 Mermaid diagram (the sequence diagram)
- BBj code examples in the generation-aware response section
- Architecture-first framing throughout
- Inline citations with links
- No "Coming Soon" text

**What to AVOID:**
- Do NOT over-commit on deployment model -- per CONTEXT.md the vision is still forming
- Do NOT present specific deployment URLs or production infrastructure details
- Do NOT claim this is built -- it is planned/designed
- Do NOT copy the strategy paper's ASCII art -- use Mermaid
- Do NOT make the chapter shorter than 250 lines
- Do NOT use .mdx extension
  </action>
  <verify>
Run `npm run build` from the project root. Build must pass with zero errors. Then verify:
1. File exists and is >250 lines: `wc -l docs/05-documentation-chat/index.md`
2. Contains TL;DR: `grep -c "TL;DR" docs/05-documentation-chat/index.md`
3. Contains Decision callout: `grep -c ":::info\[Decision:" docs/05-documentation-chat/index.md`
4. Contains Mermaid diagram: `grep -c "mermaid" docs/05-documentation-chat/index.md`
5. Contains Current Status: `grep -c "Current Status" docs/05-documentation-chat/index.md`
6. No "Coming Soon": `grep -c "Coming Soon" docs/05-documentation-chat/index.md` returns 0
7. Cross-references: `grep -c "/docs/" docs/05-documentation-chat/index.md`
  </verify>
  <done>
Chapter 5 is a complete, build-passing chapter of 250+ lines that explains why generic chat services fail for BBj, presents architectural requirements for generation-aware documentation chat, shows deployment options without over-committing, and honestly states the current status as vision-defined but not yet built.
  </done>
</task>

</tasks>

<verification>
- `npm run build` passes with zero errors
- Chapter 5 replaces placeholder with substantive content (250+ lines)
- All content patterns present: TL;DR, Decision callout, Mermaid diagram, Current Status
- Cross-references to at least 3 other chapters
- Does NOT over-commit on deployment model (presents options with trade-offs)
- Shared infrastructure concept is clear (same model + RAG as IDE)
- Generation-aware response examples included with BBj code
</verification>

<success_criteria>
A reader understands: (1) why kapa.ai and similar services fail for BBj, (2) what the chat system must do architecturally, (3) how generation-aware responses work with concrete examples, (4) the deployment model options and trade-offs, and (5) that this is designed but not yet built. The chapter builds with zero errors and follows all established patterns.
</success_criteria>

<output>
After completion, create `.planning/phases/04-execution-chapters/04-02-SUMMARY.md`
</output>
