---
phase: 23.1-wordpress-parser-fix
plan: 03
type: execute
wave: 2
depends_on: ["23.1-01"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "All 6 sources are ingested from scratch with zero corrupted binary garbage chunks"
    - "WordPress Advantage source includes PDF documents extracted as markdown text"
    - "Ingestion summary table shows OK status for all sources"
    - "/stats endpoint returns non-zero chunk counts for all expected source types"
  artifacts: []
  key_links:
    - from: "docker compose exec app bbj-ingest-all"
      to: "pgvector chunks table"
      via: "Clean re-ingest populates all sources"
      pattern: "bbj-ingest-all.*--clean"
---

<objective>
Run a full clean re-ingest of all 6 BBj documentation sources via Docker to produce a clean baseline with no corrupted chunks from the previous binary misparse.

Purpose: The previous ingestion run stored binary garbage from PDFs parsed as HTML. A clean re-ingest ensures the corpus is correct, with PDF content properly extracted as markdown. This also validates that the parser fix from Plan 01 works against the real corpus.

Output: A fully populated pgvector database with clean chunks from all 6 sources, verified via the /stats endpoint.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@rag-ingestion/docker-compose.yml
@rag-ingestion/sources.toml
@rag-ingestion/src/bbj_rag/ingest_all.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rebuild Docker image and run full clean re-ingest</name>
  <files></files>
  <action>
The parser fix from Plan 01 must be committed before this step. The Docker image needs to be rebuilt to include the fixed WordPress parser.

Steps:
1. Ensure Ollama is running on the host: `curl -s http://localhost:11434/api/tags` should return a JSON list of models.
2. Rebuild the Docker image to pick up parser changes:
   ```bash
   cd /Users/beff/_workspace/bbj-ai-strategy/rag-ingestion
   docker compose build app
   ```
3. Start services (if not already running):
   ```bash
   docker compose up -d
   ```
4. Wait for health check to pass:
   ```bash
   docker compose exec app curl -sf http://localhost:8000/health
   ```
5. Run the full clean re-ingest:
   ```bash
   docker compose exec app bbj-ingest-all --config sources.toml --clean -v
   ```
   This will:
   - Delete all existing chunks for each source before re-ingesting
   - Parse, chunk, embed, and store documents for all 9 source entries (6 parser types)
   - Print a summary table showing docs parsed, chunks created, and duration per source

6. After ingestion completes, verify via the /stats endpoint:
   ```bash
   curl -s http://localhost:10800/stats | python3 -m json.tool
   ```
   Expect non-zero counts for multiple doc types and generation tags.

7. Run a test search to verify quality:
   ```bash
   curl -s http://localhost:10800/search \
     -H "Content-Type: application/json" \
     -d '{"query": "BBjGrid", "limit": 3}' | python3 -m json.tool
   ```
   Expect results with readable content (no binary garbage).

NOTE: The full ingestion will take significant time (embedding all chunks). The Advantage source specifically should now yield PDF documents alongside HTML articles.
  </action>
  <verify>
1. `docker compose exec app bbj-ingest-all` exits with code 0 (all sources OK)
2. `curl -s http://localhost:10800/stats` returns non-zero total_chunks
3. `curl -s http://localhost:10800/search -d '{"query": "BBjGrid"}'` returns readable content
  </verify>
  <done>
All 9 source entries ingested successfully. Summary table shows OK for all sources. No binary garbage in search results.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Full clean re-ingest of all 6 BBj documentation sources with fixed WordPress parser. PDF content from Advantage articles is now extracted as markdown text instead of binary garbage.</what-built>
  <how-to-verify>
1. Check the ingestion summary table output -- all sources should show "OK" status
2. Run: `curl -s http://localhost:10800/stats | python3 -m json.tool`
   - Verify non-zero chunk counts across multiple doc types
   - Verify generation tags are present (bbj, dwc, bbj_gui, etc.)
3. Run: `curl -s http://localhost:10800/search -H "Content-Type: application/json" -d '{"query": "BBjGrid columns", "limit": 3}' | python3 -m json.tool`
   - Verify results contain readable documentation text
   - Verify no binary garbage or HTML parsing artifacts in content fields
4. Run: `curl -s http://localhost:10800/search -H "Content-Type: application/json" -d '{"query": "BBJSP command", "limit": 3}' | python3 -m json.tool`
   - This query should find content from the PDF articles (previously corrupted)
   - Verify the content is clean markdown text extracted from the PDF
  </how-to-verify>
  <resume-signal>Type "approved" if results look clean, or describe any issues with corrupted content</resume-signal>
</task>

</tasks>

<verification>
1. `bbj-ingest-all --clean` completed with exit code 0
2. /stats returns non-zero counts for all expected doc types
3. /search returns readable content with no binary garbage
4. PDF-sourced content from Advantage articles is clean markdown
5. User has verified search quality
</verification>

<success_criteria>
All 6 BBj documentation sources fully re-ingested with clean data. Search results verified to contain readable content. No corrupted binary chunks remain in the database.
</success_criteria>

<output>
After completion, create `.planning/phases/23.1-wordpress-parser-fix/23.1-03-SUMMARY.md`
</output>
