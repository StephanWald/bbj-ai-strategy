# Phase 23.1: WordPress Parser Fix - Research

**Researched:** 2026-02-02
**Domain:** Python ingestion pipeline (PDF handling, HTTP Content-Type, Ollama embeddings, Docker documentation)
**Confidence:** HIGH

## Summary

This phase fixes a binary garbage bug in the WordPress parser (PDFs parsed as HTML), adds Content-Type-based format detection for redirected URLs, batches Ollama embedding requests, runs a full re-ingest, and updates the README with Docker/API/MCP documentation.

The partial fix is already in the working tree: `_is_pdf_url()`, `_fetch_pdf_bytes()`, `_parse_pdf_bytes()`, `_title_from_pdf_url()`, relative URL resolution with `urljoin`, and module-level import cleanup. What remains is Content-Type fallback detection (for URLs that redirect and lose the `.pdf` extension), handling two specific redirecting URLs that throw skip exceptions, the batch embedding optimization, the full re-ingest, and the README documentation.

The existing `OllamaEmbedder.embed_batch()` already passes a list to `ollama.embed(input=texts)`, which means Ollama already receives batch input in a single API call. The "1-at-a-time" slowness mentioned in the phase context refers to the pipeline's `batch_size` parameter controlling how many chunks accumulate before calling `embed_batch()`. The current default is already 64. The optimization opportunity is confirming the batch path works correctly and tuning the batch size.

**Primary recommendation:** Commit the working tree changes (PDF handling + relative URL fix), add Content-Type detection to `_fetch_page`/`_fetch_pdf_bytes`, keep the default batch_size=64 (already batched), run `bbj-ingest-all --clean`, and add Docker/API/MCP sections to the README.

## Standard Stack

### Core (already in project)

| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| pymupdf | >=1.25 (via pymupdf4llm) | Low-level PDF parsing (open from bytes) | Industry standard PDF library for Python, already used by pdf.py parser |
| pymupdf4llm | >=0.2.9 | PDF-to-Markdown conversion | Optimized for LLM consumption, handles tables/headings, already a dependency |
| httpx | >=0.28,<1 | HTTP client with redirect support | Already used, `follow_redirects=True` already set on Client |
| ollama | >=0.6,<1 | Ollama Python client | Already used, `embed()` accepts list input natively for batch |
| beautifulsoup4 | >=4.13,<5 | HTML parsing | Already used for WordPress HTML content extraction |

### Supporting (no new dependencies needed)

This phase requires zero new dependencies. All libraries are already in `pyproject.toml`.

**Installation:** No changes to pyproject.toml required.

## Architecture Patterns

### Existing Project Structure (no changes needed)

```
src/bbj_rag/
    parsers/wordpress.py    # WordPress parsers (Advantage + KB) -- modify this
    embedder.py             # OllamaEmbedder -- already batches, verify behavior
    pipeline.py             # run_pipeline -- batch_size already configurable
    config.py               # Settings -- embedding_batch_size=64 already defined
    ingest_all.py           # bbj-ingest-all CLI -- used for re-ingest
    api/routes.py           # REST API (/search, /stats) -- document in README
    health.py               # /health endpoint -- document in README
    mcp_server.py           # MCP stdio server -- document in README
```

### Pattern 1: Content-Type-Based Format Detection

**What:** When a URL does not have `.pdf` in the path (e.g., after redirect), check the response `Content-Type` header to detect PDFs.

**When to use:** After fetching any URL that could be a PDF behind a redirect.

**Example:**
```python
# Source: httpx official docs (python-httpx.org/quickstart)
resp = client.get(url)
if resp.status_code == 200:
    content_type = resp.headers.get("content-type", "")
    if "application/pdf" in content_type:
        # This is a PDF despite the URL not ending in .pdf
        return resp.content  # bytes
    else:
        return resp.text  # HTML string
```

**Key detail:** After `follow_redirects=True`, `resp.url` contains the final URL. But the Content-Type header is the authoritative signal -- a redirect to `?download=true&id=123` still returns `application/pdf` in the header even though the URL has no `.pdf` extension.

### Pattern 2: Unified Fetch with Format Detection

**What:** Replace the split between `_fetch_page` (returns str) and `_fetch_pdf_bytes` (returns bytes) with a unified fetch that detects format from Content-Type.

**When to use:** In the AdvantageParser main loop where URLs are iterated.

**Example approach (two viable options):**

**Option A -- Modify `_fetch_page` to return a tagged union:**
```python
from dataclasses import dataclass

@dataclass
class FetchResult:
    url: str
    content_type: str
    data: bytes  # raw bytes from response

    @property
    def is_pdf(self) -> bool:
        return "application/pdf" in self.content_type

    @property
    def text(self) -> str:
        return self.data.decode("utf-8", errors="replace")
```

**Option B -- Add Content-Type check at the call site:**
```python
# In the parse() loop, after pre-checking _is_pdf_url():
if _is_pdf_url(url):
    pdf_bytes = _fetch_pdf_bytes(client, url, self.max_retries)
    ...
else:
    # Fetch as HTML first, but check Content-Type
    resp = _fetch_response(client, url, self.max_retries)
    if resp is None:
        continue
    content_type = resp.headers.get("content-type", "")
    if "application/pdf" in content_type:
        # Redirected to a PDF -- handle as PDF
        doc = self._parse_pdf_bytes(url, resp.content)
        ...
    else:
        html = resp.text
        # proceed with HTML parsing
```

**Recommendation:** Option B is simpler and preserves the existing helper functions. Add a new `_fetch_response()` helper that returns the raw `httpx.Response` object (or None on failure), and use it where Content-Type detection is needed.

### Pattern 3: Graceful Redirect Handling

**What:** Some URLs on the Advantage index page redirect to destinations that previously caused skip exceptions (likely because `_fetch_page` returned None or because the redirected content was binary).

**When to use:** The fix is already partially in place (the `_is_pdf_url` check). With Content-Type detection, any URL that redirects to a PDF will be caught by the `"application/pdf" in content_type` check.

**Key detail from httpx docs:**
```python
resp = client.get(url)
str(resp.url)  # final URL after all redirects
resp.history   # list of redirect Response objects
```

### Anti-Patterns to Avoid

- **Checking only URL extension for format:** URLs can redirect. A link to `/advantage/BBjGuide` might redirect to a CDN URL like `/downloads/?file=BBjGuide.pdf`. Always check Content-Type as fallback.
- **Passing large lists to ollama.embed() in a single call:** While the API accepts lists, the known GitHub issue #6262 shows quality degradation at very high parallelism. The current default batch_size=64 is reasonable; do not increase to hundreds.
- **Using separate `_fetch_page` and `_fetch_pdf_bytes` for all cases:** This forces format detection before fetching. A unified approach that fetches first, then checks Content-Type, handles all redirect scenarios cleanly.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| PDF text extraction | Custom PDF parser | pymupdf + pymupdf4llm `to_markdown()` | Already in project, handles tables, headings, multi-column layouts |
| PDF detection | URL-only check | URL extension + Content-Type header | Redirected URLs lose extension info |
| Batch embedding | Custom HTTP calls to Ollama API | `ollama.embed(model=..., input=[...])` | Python client already handles batch natively |
| URL resolution | String concatenation | `urllib.parse.urljoin()` | Already fixed in working tree; handles relative, absolute, protocol-relative |
| Markdown from HTML | Custom tag stripping | `_html_to_markdown()` (existing) | Already in web_crawl.py, reused by WordPress parser |

**Key insight:** The existing codebase already has all the building blocks. This phase is about wiring them together correctly (Content-Type check) and documenting what exists (README).

## Common Pitfalls

### Pitfall 1: Content-Type Header Parsing

**What goes wrong:** Content-Type headers often include parameters like `application/pdf; charset=utf-8` or `text/html; charset=UTF-8`. A naive equality check `== "application/pdf"` fails.

**Why it happens:** HTTP servers append charset and other parameters after a semicolon.

**How to avoid:** Use substring check: `"application/pdf" in content_type.lower()`.

**Warning signs:** PDFs still being parsed as HTML despite Content-Type detection code.

### Pitfall 2: Ollama Batch Embedding Quality at High Parallelism

**What goes wrong:** Embedding quality degrades with batch sizes >= 16 when Ollama parallelism env vars are set (OLLAMA_MAX_QUEUE, OLLAMA_NUM_PARALLEL).

**Why it happens:** GitHub issue #6262 -- high parallelism settings in Ollama cause embedding accuracy to drop. Cosine similarity drops from 0.9999 to ~0.96.

**How to avoid:** Do NOT set `OLLAMA_MAX_QUEUE` or `OLLAMA_NUM_PARALLEL` to high values. The default embedding_batch_size=64 is fine for the `input` list in a single `ollama.embed()` call -- the issue is about server-side parallelism, not client-side batch size.

**Warning signs:** Search quality drops after re-ingest with new settings.

### Pitfall 3: Binary Content Parsed as HTML

**What goes wrong:** PDF bytes fed to BeautifulSoup produce garbage text that gets chunked and embedded, polluting the vector database.

**Why it happens:** The original code had no format detection -- every fetched URL was assumed to be HTML.

**How to avoid:** Check Content-Type BEFORE parsing. The working tree fix already handles `.pdf` extension URLs. The Content-Type fallback catches redirected PDFs.

**Warning signs:** Chunks containing sequences like `%PDF-1.4`, binary escape sequences, or long strings of non-printable characters.

### Pitfall 4: Missing Rate Limit Sleep for PDF URLs

**What goes wrong:** The existing code sleeps between HTML requests but the PDF branch (added in the working tree) does not include the rate limit sleep in the same place.

**Why it happens:** The rate limit sleep was inside the `else` (HTML) branch. The `if _is_pdf_url(url)` branch has the sleep at the end of the outer loop (`if i < len(urls): time.sleep(...)`), which is correct.

**How to avoid:** Verify the rate limit sleep applies to ALL URLs, not just HTML ones. Check the working tree code carefully -- the sleep at line 318 is outside both branches, so it applies universally. This is correct.

### Pitfall 5: Re-ingest Without Clean Flag

**What goes wrong:** Running `bbj-ingest-all` without `--clean` leaves old corrupted chunks in the database alongside new clean ones.

**Why it happens:** Without `--clean`, the pipeline adds new chunks but does not remove old ones. Corrupted binary-garbage chunks from the previous ingest remain.

**How to avoid:** Use `bbj-ingest-all --clean` for the re-ingest step. This deletes existing chunks per source before re-ingesting.

## Code Examples

### Content-Type Detection Helper

```python
# New helper to replace both _fetch_page and _fetch_pdf_bytes
# for the AdvantageParser where redirect detection is needed
def _fetch_response(
    client: httpx.Client,
    url: str,
    max_retries: int,
) -> httpx.Response | None:
    """Fetch a URL with retries, returning the raw Response.

    Returns None on failure after exhausting retries.
    The caller checks response.headers["content-type"] to determine
    whether the content is HTML or PDF.
    """
    for attempt in range(1 + max_retries):
        try:
            resp = client.get(url)
            if resp.status_code == 200:
                return resp
            logger.warning(
                "HTTP %d for %s (attempt %d)",
                resp.status_code, url, attempt + 1,
            )
        except httpx.HTTPError as exc:
            logger.warning(
                "HTTP error for %s: %s (attempt %d)",
                url, exc, attempt + 1,
            )
        if attempt < max_retries:
            time.sleep(1.0 * (attempt + 1))
    return None
```

### Updated Parse Loop (AdvantageParser)

```python
# In AdvantageParser.parse() -- conceptual approach
for i, url in enumerate(urls, 1):
    # Fast path: URL extension check
    if _is_pdf_url(url):
        pdf_bytes = _fetch_pdf_bytes(client, url, self.max_retries)
        if pdf_bytes is not None:
            doc = self._parse_pdf_bytes(url, pdf_bytes)
            if doc is not None:
                yield doc
    else:
        # Fetch and check Content-Type for redirect-to-PDF case
        resp = _fetch_response(client, url, self.max_retries)
        if resp is None:
            if i < len(urls):
                time.sleep(self.rate_limit)
            continue

        content_type = resp.headers.get("content-type", "")
        if "application/pdf" in content_type.lower():
            # Redirected to PDF
            doc = self._parse_pdf_bytes(url, resp.content)
            if doc is not None:
                yield doc
        else:
            # HTML content -- parse as before
            soup = BeautifulSoup(resp.text, "lxml")
            # ... existing HTML parsing logic ...

    if i < len(urls):
        time.sleep(self.rate_limit)
```

### pymupdf PDF-from-Bytes (already in working tree)

```python
# Source: working tree -- wordpress.py _parse_pdf_bytes
doc = pymupdf.open(stream=pdf_bytes, filetype="pdf")
data = pymupdf4llm.to_markdown(doc, page_chunks=True, write_images=False)
# data is list[dict] -- each dict has "text" key with markdown string
page_texts = []
for page_data in data:
    text = page_data.get("text", "")
    if isinstance(text, str) and text.strip():
        page_texts.append(text)
content = "\n\n".join(page_texts)
```

### Ollama Batch Embedding (already working)

```python
# Source: embedder.py OllamaEmbedder.embed_batch
# Already sends list of texts in a single API call
response = ollama_client.embed(model=self._model, input=texts)
return response.embeddings  # list[list[float]]
```

### Full Re-ingest Command

```bash
# From rag-ingestion/ directory, with Docker Compose running:
docker compose exec app bbj-ingest-all --config sources.toml --clean
# OR locally:
bbj-ingest-all --config sources.toml --clean --data-dir /path/to/data
```

## Discretionary Recommendations

### Optimal Batch Size for Ollama Embeddings

**Recommendation:** Keep `embedding_batch_size = 64` (the current default).

**Rationale:**
- The `ollama.embed()` API accepts list input natively -- this already IS batch embedding.
- The pipeline accumulates `batch_size` chunks before calling `embed_batch()`, which sends them all in one API call.
- GitHub issue #6262 shows quality degradation is tied to server-side parallelism settings, not the `input` list size. Default Ollama settings do not trigger this.
- The default 64 balances memory usage (64 chunks x ~400 tokens each = ~25K tokens per batch) with throughput.
- No change needed -- the current code already batches correctly.

### PDF doc_type and Metadata

**Recommendation:** Keep PDFs as `doc_type='article'` with `metadata={'format': 'pdf'}`.

**Rationale:**
- The existing PdfParser (pdf.py) uses `doc_type='concept'` for standalone PDF sections. But Advantage PDFs are magazine articles, not reference docs.
- `doc_type='article'` is consistent with all other Advantage documents (HTML articles also use `doc_type='article'`).
- Adding `metadata['format'] = 'pdf'` (already in the working tree) allows filtering/reporting on format without polluting the doc_type taxonomy.
- The pipeline skips intelligence enrichment for pre-populated doc_types, so `doc_type='article'` passes through unchanged. This is correct.

### README Structure and Ordering

**Recommendation:** Add four new sections to the existing README, inserted after the "Usage" section and before "Project Structure":

```
## Prerequisites           (existing)
## Installation            (existing)
## Configuration           (existing)
## Usage                   (existing - local CLI)
## Docker Usage            (NEW - parallel track)
  ### Quick Start
  ### Single Source Ingestion
  ### Full Ingestion (All Sources)
  ### Viewing Logs
## REST API                (NEW)
  ### Endpoints
    - POST /search
    - GET /health
    - GET /stats
  ### Example Requests (curl)
## MCP Server              (NEW)
  ### Setup (claude_desktop_config.json)
  ### How It Works
## Project Structure       (existing)
## Development             (existing)
## Further Reading         (existing)
```

**Rationale:**
- Docker Usage as a full parallel track (local vs Docker side-by-side) per user decision
- REST API section covers all three endpoints with curl examples
- MCP server section shows Claude Desktop integration config
- New sections slot in logically between "how to run" (Usage) and "how it works" (Project Structure)

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| URL extension only for PDF detection | URL extension + Content-Type header | This phase | Catches PDFs behind redirects |
| Single-text embedding calls | Batch `ollama.embed(input=[...])` | Already done (embedder.py) | Already batching correctly |
| `_fetch_page` returns str only | `_fetch_response` returns raw Response | This phase | Enables Content-Type inspection |
| `from urllib.parse import ...` inside methods | Module-level imports | Already in working tree | Cleaner code, no behavioral change |

## Open Questions

1. **Which specific URLs redirect and cause skip exceptions?**
   - What we know: Two specific URLs from the Advantage index page redirect and were causing issues
   - What's unclear: The exact URLs are not identified in the context; they will be discovered during testing
   - Recommendation: Run `bbj-rag parse --source advantage` with verbose logging to identify which URLs produce warnings. The Content-Type detection fix should handle them automatically.

2. **Does pymupdf4llm handle all Advantage PDF formats correctly?**
   - What we know: The working tree code uses `pymupdf4llm.to_markdown(doc, page_chunks=True, write_images=False)` which works for the existing pdf.py parser
   - What's unclear: Advantage PDFs may have different formatting (multi-column, scanned images) than the GUI programming guide PDF
   - Recommendation: Test with `bbj-rag parse --source advantage` after the fix and spot-check a few PDF documents for content quality.

3. **Exact batch embedding behavior under Docker**
   - What we know: Docker Compose sets `OLLAMA_HOST=http://host.docker.internal:11434` and the app container calls Ollama on the host
   - What's unclear: Whether the Docker network hop adds meaningful latency per batch call vs local
   - Recommendation: Not a concern for correctness. The re-ingest will be slower than local but functionally identical.

## Sources

### Primary (HIGH confidence)
- Codebase inspection: `wordpress.py`, `embedder.py`, `pipeline.py`, `config.py`, `ingest_all.py`, `cli.py`, `app.py`, `api/routes.py`, `health.py`, `mcp_server.py` -- all read directly
- `git diff` of working tree changes -- verified exact changes pending commit
- httpx official docs (python-httpx.org/quickstart) -- response.headers, response.url, follow_redirects verified
- Ollama REST API docs (github.com/ollama/ollama/blob/main/docs/api.md) -- /api/embed input accepts list, response has embeddings array
- Ollama Python client README (github.com/ollama/ollama-python) -- embed() accepts list input

### Secondary (MEDIUM confidence)
- pymupdf4llm PyPI and GitHub README -- to_markdown page_chunks returns list of dicts with "text" key
- GitHub issue ollama/ollama#6262 -- batch quality degradation tied to parallelism settings, not input list size

### Tertiary (LOW confidence)
- None -- all findings verified against primary or secondary sources

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- all libraries already in project, APIs verified against official docs
- Architecture: HIGH -- patterns derived from reading existing codebase + httpx response API docs
- Pitfalls: HIGH -- binary garbage bug is the primary motivation; batch quality issue verified against GitHub issue
- Discretionary items: MEDIUM -- recommendations based on codebase patterns and reasonable defaults

**Research date:** 2026-02-02
**Valid until:** 2026-03-04 (stable domain, no fast-moving dependencies)
