# Phase 23.1: WordPress Parser Fix - Context

**Gathered:** 2026-02-02
**Status:** Ready for planning

<domain>
## Phase Boundary

Fix the WordPress parser to correctly handle PDF links (which were being processed as HTML, producing binary garbage in the database), add Content-Type-based format detection for redirected URLs, batch Ollama embedding requests for faster ingestion, run a full re-ingest of all 6 sources, and update the rag-ingestion README with Docker usage documentation including API and MCP sections.

</domain>

<decisions>
## Implementation Decisions

### PDF handling
- PDFs linked from the Advantage index page were being fetched and parsed as HTML, producing binary garbage chunks in the database
- The fix (already partially in working tree) adds pymupdf/pymupdf4llm for in-memory PDF extraction
- PDF detection must use both URL extension (.pdf) AND response Content-Type header (application/pdf) as fallback — some redirected URLs may not have .pdf in the final URL
- Follow redirects for all URLs (httpx follow_redirects=True is already set)
- Two specific URLs that redirect were causing skipping exceptions — these must be handled gracefully

### Batch embedding
- Current embedding calls are 1-at-a-time to Ollama `/api/embed` — very slow for full ingestion
- Batch multiple chunks per API call to speed up the process
- Include this optimization in this phase since a full re-ingest will be required anyway

### Re-ingestion strategy
- After code fixes land, run a full re-ingest of ALL 6 sources from scratch (not just WordPress)
- This ensures a clean baseline with no corrupted chunks from the previous binary misparse
- The re-ingest step should be part of the phase plan (not done manually)

### README updates
- Add Docker equivalents alongside every CLI example (full parallel track — local vs Docker side-by-side)
- Cover both `bbj-rag` (single source) and `bbj-ingest-all` (all sources) commands in Docker context
- Include REST API endpoints (/search, /health, /stats) and MCP server setup documentation in the same README
- Currently the README only covers local CLI usage with no mention of Docker Compose

### Claude's Discretion
- Optimal batch size for Ollama embedding calls (balance throughput vs memory)
- Whether to keep PDFs as doc_type='article' with metadata format='pdf' or use a distinct doc_type
- Exact README structure and ordering of new sections

</decisions>

<specifics>
## Specific Ideas

- The binary garbage bug was discovered during a real ingestion run — PDFs from the Advantage index were being fetched successfully but then fed to BeautifulSoup as if they were HTML
- The partial fix already in the working tree adds `_is_pdf_url()`, `_fetch_pdf_bytes()`, `_parse_pdf_bytes()`, and `_title_from_pdf_url()` to the WordPress parser
- The import cleanup (moving `urljoin`, `urlparse` to module-level) and relative URL resolution fix are also already in the working tree
- The README should help someone go from zero to running searches against the ingested corpus using Docker

</specifics>

<deferred>
## Deferred Ideas

- Concurrent ingestion workers (parallel embedding calls to keep GPU saturated) — optimization beyond batch requests
- Persistent HTTP connection reuse for Ollama embedding calls — further performance tuning
- Map source_url to clickable HTTP links (e.g., flare:// → https://documentation.basis.cloud/...) — UX improvement for chat users

</deferred>

---

*Phase: 23.1-wordpress-parser-fix*
*Context gathered: 2026-02-02*
