---
phase: 14-documentation-quality
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - rag-ingestion/README.md
  - rag-ingestion/src/bbj_rag/intelligence/report.py
  - rag-ingestion/src/bbj_rag/cli.py
  - rag-ingestion/tests/test_report.py
autonomous: true

must_haves:
  truths:
    - "Running `bbj-rag report` against a populated database prints chunk counts by source, by generation, and by document type"
    - "Quality report includes automated warnings for anomalies (empty sources, unbalanced distributions, unknown doc types)"
    - "Running `bbj-rag ingest --source <X>` auto-prints the quality report after successful ingestion"
    - "README provides step-by-step setup instructions that a new engineer can follow from zero to running the pipeline"
    - "README has a complete configuration reference table covering all TOML settings and environment variables"
    - "README has a CLI command reference covering ingest, parse, report, and validate"
  artifacts:
    - path: "rag-ingestion/README.md"
      provides: "Project README with setup, config, and usage guide"
      contains: "Prerequisites"
    - path: "rag-ingestion/src/bbj_rag/intelligence/report.py"
      provides: "Quality report with DB-based queries and anomaly warnings"
      contains: "print_quality_report"
    - path: "rag-ingestion/src/bbj_rag/cli.py"
      provides: "CLI with report command and post-ingest report"
      contains: "def report"
    - path: "rag-ingestion/tests/test_report.py"
      provides: "Unit tests for quality report functions"
      contains: "test_"
  key_links:
    - from: "rag-ingestion/src/bbj_rag/cli.py"
      to: "rag-ingestion/src/bbj_rag/intelligence/report.py"
      via: "import and call print_quality_report"
      pattern: "print_quality_report"
    - from: "rag-ingestion/README.md"
      to: "https://stephanwald.github.io/bbj-ai-strategy/docs/rag-database/getting-started"
      via: "Markdown hyperlink to docs site"
      pattern: "stephanwald\\.github\\.io"
---

<objective>
Implement the post-ingestion quality report (CLI command + auto-print after ingest) and write the comprehensive rag-ingestion README.

Purpose: The quality report makes ingestion results measurable (requirement QUAL-01) -- engineers can see chunk distributions and catch anomalies. The README gives new engineers a complete onboarding path from prerequisites through running the pipeline (requirement QUAL-02). Together they complete the documentation phase.

Output: Quality report module with DB queries and anomaly warnings, `report` CLI command, auto-print integration in `ingest`, unit tests, and a comprehensive README.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-documentation-quality/14-CONTEXT.md
@.planning/phases/14-documentation-quality/14-RESEARCH.md

Read these source files for the quality report implementation:
@rag-ingestion/src/bbj_rag/intelligence/report.py
@rag-ingestion/src/bbj_rag/cli.py
@rag-ingestion/src/bbj_rag/db.py
@rag-ingestion/src/bbj_rag/config.py
@rag-ingestion/src/bbj_rag/models.py
@rag-ingestion/src/bbj_rag/pipeline.py

Read these for README content accuracy:
@rag-ingestion/pyproject.toml
@rag-ingestion/Makefile
@rag-ingestion/src/bbj_rag/parsers/__init__.py
@rag-ingestion/sql/schema.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement quality report module and CLI command</name>
  <files>
    rag-ingestion/src/bbj_rag/intelligence/report.py
    rag-ingestion/src/bbj_rag/cli.py
    rag-ingestion/tests/test_report.py
  </files>
  <action>
**Extend `intelligence/report.py`** -- Add these new functions alongside the existing `build_report` and `print_report` (keep those intact):

1. `_query_report_data(conn) -> tuple[dict[str, int], dict[str, int], dict[str, int], int]`
   - Takes a psycopg connection
   - Runs 3 SQL queries against the `chunks` table:
     - By source: CASE on source_url prefix (flare://, pdf://, mdx://, file://, and LIKE patterns for basis.cloud/advantage and basis.cloud/knowledge). Use `'%basis.cloud/advantage%'` and `'%basis.cloud/knowledge%'` for generous matching. Anything else maps to 'unknown'.
     - By generation: `SELECT g, COUNT(*) FROM chunks, unnest(generations) AS g GROUP BY g ORDER BY cnt DESC`
     - By doc_type: `SELECT doc_type, COUNT(*) FROM chunks GROUP BY doc_type ORDER BY cnt DESC`
     - Also query total: `SELECT COUNT(*) FROM chunks`
   - Returns (by_source, by_generation, by_doc_type, total)

2. `_check_anomalies(by_source, by_generation, by_doc_type, total) -> list[str]`
   - Check for: empty expected sources (flare, advantage, kb, pdf, mdx, bbj-source), suspiciously low counts (<10 chunks from a source), high untagged percentage (>5%), unknown doc_types (not in known set: api-reference, concept, example, migration, language-reference, best-practice, version-note, article, tutorial), dominant source (>90% of total)
   - Returns list of warning strings

3. `print_quality_report(conn) -> None`
   - Calls `_query_report_data` and `_check_anomalies`
   - Formats output using click.echo (NOT print -- be consistent with CLI patterns):
     ```
     === BBj RAG Quality Report ===

     Chunks by Source:
       flare            2,847  (68.2%)
       ...
       ─────────────────────────────
       Total            4,173

     Chunks by Generation:
       ...

     Chunks by Document Type:
       ...

     Warnings:
       [!] warning text here
     ```
   - If total is 0, print "No chunks in database. Run ingestion first." and return early.
   - Warnings section only printed if warnings exist.
   - Use comma-formatted numbers and right-aligned counts with percentages.

4. Update `__all__` to export `print_quality_report`.

**Modify `cli.py`** -- Two changes:

1. Add `report` command:
   ```python
   @cli.command()
   def report() -> None:
       """Show post-ingestion quality report."""
       from bbj_rag.db import get_connection
       from bbj_rag.intelligence.report import print_quality_report

       settings = Settings()
       try:
           conn = get_connection(settings.database_url)
       except Exception as exc:
           safe_url = _mask_password(settings.database_url)
           click.echo(f"Database connection failed: {exc}\nURL: {safe_url}", err=True)
           sys.exit(1)
       try:
           print_quality_report(conn)
       finally:
           conn.close()
   ```

2. In the `ingest` command, after the success `click.echo` (after line ~102), add:
   ```python
   # Auto-print quality report after successful ingestion
   from bbj_rag.intelligence.report import print_quality_report
   click.echo()  # blank line separator
   print_quality_report(conn)
   ```
   Place this BEFORE the `except` block but inside the `try`, after the existing success echo. Important: the `conn` is still open at this point (the `finally: conn.close()` comes later).

**Create `tests/test_report.py`** -- Unit tests:

1. Test `_check_anomalies` with various inputs:
   - Empty sources: verify warning when expected source has 0 chunks
   - Low counts: verify warning when a source has <10 chunks
   - High untagged: verify warning when >5% are untagged
   - Unknown doc_type: verify warning for unrecognized type
   - Dominant source: verify warning when one source has >90%
   - Clean data: verify empty warnings list when all is healthy

2. Test `_query_report_data` is importable (don't test actual DB queries -- those require a live database)

3. Test `print_quality_report` handles zero-total gracefully (mock the connection and cursor to return 0 for count and empty results for other queries)

Use pytest. Import from `bbj_rag.intelligence.report`. Use `unittest.mock.patch` or `unittest.mock.MagicMock` for DB mocking.
  </action>
  <verify>
1. `uv run pytest tests/test_report.py -v` passes from `rag-ingestion/` directory
2. `uv run ruff check src/bbj_rag/intelligence/report.py src/bbj_rag/cli.py` passes
3. `uv run mypy src/bbj_rag/intelligence/report.py src/bbj_rag/cli.py` passes
4. Existing tests still pass: `uv run pytest` (excluding search_validation)
5. `uv run bbj-rag report --help` shows the command help text
  </verify>
  <done>Quality report module has DB-query-based `print_quality_report(conn)` function with anomaly warnings. CLI has `report` command and auto-prints report after `ingest`. Tests verify anomaly detection logic and graceful handling of empty database.</done>
</task>

<task type="auto">
  <name>Task 2: Write rag-ingestion README</name>
  <files>rag-ingestion/README.md</files>
  <action>
Create `rag-ingestion/README.md` with the following structure. Read the actual source files (config.py, cli.py, pyproject.toml, Makefile) to ensure accuracy -- do NOT guess at settings, commands, or defaults.

**Section 1: Title + Description**
```markdown
# BBj RAG Ingestion Pipeline

A Python pipeline that parses BBj documentation from multiple sources, tags content by BBj generation and document type, chunks it with contextual headers, generates embeddings, and stores everything in PostgreSQL with pgvector for hybrid retrieval.
```

**Section 2: Overview** (2-3 sentences)
- What the pipeline does (parse 6 source types, tag, chunk, embed, store)
- What it produces (a searchable pgvector database for the BBj AI documentation chat)
- Link to the [Getting Started guide](https://stephanwald.github.io/bbj-ai-strategy/docs/rag-database/getting-started) for design rationale

**Section 3: Prerequisites** -- Step-by-step, with version verification commands:

1. **Python 3.12+** -- `python3 --version`
2. **uv** (Python package manager) -- `uv --version`, install link: https://docs.astral.sh/uv/getting-started/installation/
3. **PostgreSQL 16+ with pgvector** -- `psql --version`
   - Install pgvector extension: `CREATE EXTENSION IF NOT EXISTS vector;`
   - Create database: `createdb bbj_rag`
   - Apply schema: `psql bbj_rag < sql/schema.sql`
4. **Ollama** (for embedding model) -- `ollama --version`, install link: https://ollama.com
   - Pull embedding model: `ollama pull qwen3-embedding:0.6b`

**Section 4: Installation**
```bash
git clone https://github.com/StephanWald/bbj-ai-strategy.git
cd bbj-ai-strategy/rag-ingestion
uv sync
```

**Section 5: Configuration**

Sub-section "Config File" -- The pipeline reads `config.toml` in the working directory. Show example config.toml with all settings.

Sub-section "Configuration Reference" -- Table with columns: Setting | Type | Default | Env Override | Description
Build this table from the ACTUAL `Settings` class in `config.py`. Read the file to get exact field names, types, and defaults. Every field in the Settings class gets a row. The env override column shows the `BBJ_RAG_` prefixed name (e.g., `database_url` -> `BBJ_RAG_DATABASE_URL`).

Sub-section "Environment Variables" -- Brief explanation that env vars override TOML values. Priority: constructor args > env vars > TOML file > field defaults. Prefix: `BBJ_RAG_`.

**Section 6: Usage** -- CLI command reference:

For EACH command, show: the command, all flags/options, a usage example, and expected output.

1. `bbj-rag ingest` -- Full pipeline (parse -> tag -> chunk -> embed -> store)
   - `--source` (required): flare, pdf, advantage, kb, mdx, bbj-source
   - `--resume`: Skip already-stored chunks
   - `--batch-size`: Embedding batch size (default: 64)
   - `-v/--verbose`: Debug logging
   - Example: `bbj-rag ingest --source flare`
   - Note: Auto-prints quality report after completion

2. `bbj-rag parse` -- Parse only (no embedding, no storage)
   - `--source` (required): same choices as ingest
   - Example: `bbj-rag parse --source pdf`

3. `bbj-rag report` -- Post-ingestion quality report
   - No required options
   - Example: `bbj-rag report`
   - Shows chunk counts by source, generation, doc type, plus anomaly warnings

4. `bbj-rag validate` -- Search validation
   - `-v/--verbose`: Show detailed test results
   - Example: `bbj-rag validate`
   - Requires populated database

**Section 7: Project Structure** -- Tree of `src/bbj_rag/` with one-line descriptions:
```
src/bbj_rag/
    cli.py              # Click CLI entry point
    config.py           # Settings (TOML + env var loading)
    models.py           # Document and Chunk Pydantic models
    pipeline.py         # Pipeline orchestrator (parse -> tag -> chunk -> embed -> store)
    chunker.py          # Heading-aware text chunking
    embedder.py         # Embedding via Ollama or OpenAI
    db.py               # PostgreSQL connection and bulk insert
    schema.py           # Schema creation helper
    search.py           # Dense, BM25, and hybrid RRF search
    intelligence/
        __init__.py
        generations.py  # BBj generation tagger (all/character/vpro5/bbj_gui/dwc)
        doc_types.py    # Document type classifier
        context_headers.py  # Hierarchical context header builder
        report.py       # Quality report (generation distribution + DB metrics)
    parsers/
        __init__.py     # DocumentParser protocol
        flare.py        # MadCap Flare XHTML parser
        flare_toc.py    # Flare TOC index builder
        flare_cond.py   # Flare condition tag extractor
        web_crawl.py    # Web crawl fallback parser
        wordpress.py    # WordPress parsers (Advantage + KB)
        pdf.py          # PDF parser (pymupdf4llm)
        mdx.py          # Docusaurus MDX parser
        bbj_source.py   # BBj source code parser
```

**Section 8: Development**
- `make check` -- Run all checks (lint, typecheck, test)
- `make test` -- Run tests
- `make lint` -- Run ruff linter
- `make format` -- Format code with ruff
- `make typecheck` -- Run mypy
- `make clean` -- Remove build artifacts

**Section 9: Further Reading**
- [Getting Started with RAG Ingestion](https://stephanwald.github.io/bbj-ai-strategy/docs/rag-database/getting-started) -- Pipeline design and code tour
- [Chapter 6: RAG Database Design](https://stephanwald.github.io/bbj-ai-strategy/docs/rag-database) -- Full design rationale (generation tagging, chunking strategy, hybrid search)

**Writing guidelines:**
- Audience: readable by outsiders, optimized for internal BASIS engineers (per CONTEXT.md decision)
- Full step-by-step for prerequisites -- assumes minimal prior setup
- Every command has a verification step or expected output
- Cross-links to docs site for design rationale (bidirectional linking per CONTEXT.md decision)
- Keep it practical -- this is a reference, not a tutorial
  </action>
  <verify>
1. File exists at `rag-ingestion/README.md`
2. Contains all 9 sections (Title, Overview, Prerequisites, Installation, Configuration, Usage, Project Structure, Development, Further Reading)
3. Configuration reference table has a row for every field in Settings class (14 fields)
4. Usage section covers all 4 CLI commands (ingest, parse, report, validate)
5. Prerequisites section includes verification commands for Python, uv, PostgreSQL, Ollama
6. Contains links to the docs site (stephanwald.github.io)
7. Contains the GitHub clone URL
  </verify>
  <done>README provides complete onboarding: prerequisites with verification commands, installation, full configuration reference, CLI command reference, project structure tree, development commands, and cross-links to the docs site.</done>
</task>

</tasks>

<verification>
1. `uv run pytest` passes from `rag-ingestion/` (all tests including new report tests)
2. `uv run ruff check src/ tests/` passes
3. `uv run mypy src/` passes
4. `uv run bbj-rag report --help` prints help text
5. `rag-ingestion/README.md` exists with all required sections
6. `intelligence/report.py` contains `print_quality_report` function
7. `cli.py` contains `report` command and auto-print in `ingest`
</verification>

<success_criteria>
- Quality report CLI command queries the database and displays chunk distributions in three breakdowns (by source, by generation, by document type)
- Quality report includes automated anomaly warnings (empty sources, low counts, high untagged percentage, unknown doc types, dominant source)
- `ingest` command auto-prints the quality report after successful ingestion
- README has complete setup instructions from zero to running pipeline
- README has full configuration and CLI reference
- All tests pass, linter clean, type checker clean
</success_criteria>

<output>
After completion, create `.planning/phases/14-documentation-quality/14-02-SUMMARY.md`
</output>
