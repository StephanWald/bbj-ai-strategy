# Phase 14: Documentation & Quality - Research

**Researched:** 2026-02-01
**Domain:** Docusaurus sub-page authoring, README technical writing, CLI quality reporting, Mermaid diagrams, GitHub URL linking
**Confidence:** HIGH (all findings verified against actual codebase and official documentation)

## Summary

This phase produces three deliverables: a Getting Started sub-page under Chapter 6 in the Docusaurus docs site, a comprehensive README.md for the rag-ingestion project, and a CLI quality report command. No new pipeline code is needed -- this phase documents and validates what phases 8-13 already built.

The codebase is well-structured and complete. The `rag-ingestion/` directory contains 24 Python source files across `cli.py`, `config.py`, `models.py`, `pipeline.py`, `chunker.py`, `embedder.py`, `search.py`, `schema.py`, `db.py`, an `intelligence/` package (4 modules), and a `parsers/` package (8 modules). All configuration flows through `config.py` (pydantic-settings with TOML + env var overrides). The CLI uses Click with 3 existing commands (`ingest`, `parse`, `validate`). A partial intelligence report module (`intelligence/report.py`) already exists but only covers generation distribution -- it needs extension for the full quality report (by-source, by-doc-type breakdowns, anomaly warnings).

The docs site uses Docusaurus v3 with autogenerated sidebars (`sidebars.ts` uses `{type: 'autogenerated', dirName: '.'}`). Chapter 6 (`docs/06-rag-database/`) currently has only `index.md`. No chapter currently has sub-pages, so the Getting Started page will be the first sub-page. Docusaurus handles sub-pages automatically -- adding a markdown file to the category folder with `sidebar_position` frontmatter is sufficient. Mermaid is already configured (`markdown.mermaid: true` and `@docusaurus/theme-mermaid` in themes). The git remote is `https://github.com/StephanWald/bbj-ai-strategy.git`.

**Primary recommendation:** Implement the Getting Started sub-page as `docs/06-rag-database/getting-started.md` with Mermaid `graph LR` for the pipeline flow diagram. Use branch-based GitHub URLs (`blob/main/...`) since the project has no release tags. Build the quality report as a new `report` CLI command querying the database directly, plus auto-print integration at the end of `ingest`. Write the README from scratch at `rag-ingestion/README.md`.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| Docusaurus | v3 (already installed) | Documentation site with Mermaid support | Already hosts the docs site; autogenerated sidebar handles sub-pages |
| @docusaurus/theme-mermaid | (already installed) | Mermaid diagram rendering | Already configured in docusaurus.config.ts |
| Click | >=8.1 (already installed) | CLI framework for new `report` command | Already powers the CLI; adding a command is trivial |
| psycopg | >=3.3 (already installed) | Database queries for quality report | Already in project for all DB operations |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| Mermaid | >=10.4 (bundled with Docusaurus) | Pipeline flow diagram in Getting Started page | Rendering `graph LR` diagrams in markdown |
| collections.Counter (stdlib) | N/A | Aggregating chunk counts by dimension | Quality report tabulation |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Mermaid diagram | ASCII art diagram | Mermaid already works in the site, renders interactively, supports dark mode; ASCII is static |
| Mermaid diagram | Static PNG image | Would require separate image file and manual updates; Mermaid is source-as-code |
| DB-query report | File-scan report | DB query is authoritative (what's actually stored); file-scan only shows what was parsed |

**Installation:** No new dependencies required. All libraries are already installed.

## Architecture Patterns

### Recommended Project Structure
```
docs/06-rag-database/
    _category_.json           # Existing category config
    index.md                  # Existing Chapter 6 content
    getting-started.md        # NEW: Getting Started sub-page

rag-ingestion/
    README.md                 # NEW: Project README
    src/bbj_rag/
        cli.py                # MODIFY: Add `report` command, add report after ingest
        intelligence/
            report.py         # MODIFY: Extend with source/doc_type breakdowns + anomaly warnings
```

### Pattern 1: Docusaurus Sub-Page Under Category
**What:** Add a markdown file to an existing category folder. Docusaurus autogenerated sidebar picks it up automatically.
**When to use:** Always for adding pages under existing chapters.
**How it works:**

The sidebar is configured as `{type: 'autogenerated', dirName: '.'}` which scans the filesystem. Adding `getting-started.md` to `docs/06-rag-database/` with frontmatter `sidebar_position: 1` makes it appear as a sub-item under "RAG Database Design" in the sidebar navigation.

```markdown
---
sidebar_position: 1
title: "Getting Started with RAG Ingestion"
description: "How the pipeline works, how to set it up, and how to run it"
---

# Getting Started with RAG Ingestion
...
```

The `_category_.json` already links to `index` as the category's landing page. The new file becomes a child item. No changes needed to `sidebars.ts` or `_category_.json`.

### Pattern 2: Mermaid Pipeline Flow Diagram
**What:** Use Mermaid `graph LR` (left-to-right) for the pipeline flow diagram showing parse -> tag -> chunk -> embed -> store.
**When to use:** For the Getting Started page pipeline overview.
**Why this format:** Chapter 6 already uses `graph LR` mermaid diagrams with the same styling conventions (fill colors, stroke colors). The new diagram should follow the same visual pattern for consistency.

```markdown
```mermaid
graph LR
    subgraph "Sources"
        FLARE["Flare XHTML"]
        PDF["PDF"]
        WP["WordPress"]
        MDX["MDX"]
        BBJ["BBj Source"]
    end

    subgraph "Pipeline Stages"
        PARSE["Parse"]
        TAG["Tag Generation<br/>+ Doc Type"]
        CHUNK["Chunk<br/>(heading-aware)"]
        EMBED["Embed<br/>(Ollama/OpenAI)"]
        STORE["Store<br/>(pgvector)"]
    end

    FLARE --> PARSE
    PDF --> PARSE
    WP --> PARSE
    MDX --> PARSE
    BBJ --> PARSE
    PARSE --> TAG
    TAG --> CHUNK
    CHUNK --> EMBED
    EMBED --> STORE

    style FLARE fill:#e8f4e8,stroke:#2d8a2d
    style STORE fill:#e8e8f4,stroke:#2d2d8a
`` `
```

### Pattern 3: GitHub URL Construction (Branch-Based)
**What:** Build GitHub URLs from the git remote for code deep links.
**When to use:** All code references in the Getting Started page and README cross-links.

The git remote is `https://github.com/StephanWald/bbj-ai-strategy.git`. The project has no release tags (only commits on `main`), so use branch-based URLs:

- **File link:** `https://github.com/StephanWald/bbj-ai-strategy/blob/main/rag-ingestion/src/bbj_rag/models.py`
- **Directory link:** `https://github.com/StephanWald/bbj-ai-strategy/tree/main/rag-ingestion/src/bbj_rag/parsers`
- **File with line anchor:** `https://github.com/StephanWald/bbj-ai-strategy/blob/main/rag-ingestion/src/bbj_rag/models.py#L14-L27`

Use `blob/main/` for files, `tree/main/` for directories. Line anchors use `#L{start}-L{end}` format.

**Recommendation on GitHub URL format:** Use branch-based (`blob/main/...`) rather than tag-based. The project has no release tags and no release cadence that would justify tag references. Branch-based URLs always point to current code, which is appropriate for a strategy document that describes an active implementation.

### Pattern 4: Quality Report via Database Query
**What:** Query the `chunks` table directly for quality metrics rather than re-parsing source files.
**When to use:** For the `report` CLI command and post-ingest summary.
**Why:** The database is the source of truth for what was actually stored. The existing `intelligence/report.py` works on in-memory Document lists but the post-ingestion report needs to query what's in the DB.

```sql
-- By source (derived from source_url prefix)
SELECT
    CASE
        WHEN source_url LIKE 'flare://%' THEN 'flare'
        WHEN source_url LIKE 'https://basis.cloud/advantage%' THEN 'advantage'
        WHEN source_url LIKE 'https://basis.cloud/knowledge%' THEN 'kb'
        WHEN source_url LIKE 'pdf://%' THEN 'pdf'
        WHEN source_url LIKE 'file://%' THEN 'bbj-source'
        WHEN source_url LIKE 'mdx://%' THEN 'mdx'
        ELSE 'unknown'
    END AS source,
    COUNT(*) AS chunk_count
FROM chunks
GROUP BY source
ORDER BY chunk_count DESC;

-- By generation (unnest array)
SELECT g AS generation, COUNT(*) AS chunk_count
FROM chunks, unnest(generations) AS g
GROUP BY g
ORDER BY chunk_count DESC;

-- By doc_type
SELECT doc_type, COUNT(*) AS chunk_count
FROM chunks
GROUP BY doc_type
ORDER BY chunk_count DESC;
```

### Pattern 5: Click Command Registration
**What:** Add new `report` command to existing Click CLI group.
**When to use:** For the standalone quality report command.

```python
@cli.command()
def report() -> None:
    """Show post-ingestion quality report."""
    settings = Settings()
    conn = get_connection(settings.database_url)
    try:
        print_quality_report(conn)
    finally:
        conn.close()
```

The existing `ingest` command should call the same `print_quality_report()` function after successful pipeline completion.

### Anti-Patterns to Avoid
- **Duplicating Chapter 6 design content in Getting Started:** The Getting Started page explains HOW (practical setup and code navigation). Chapter 6 explains WHY (design rationale). Don't repeat the "why" -- link to it.
- **Hardcoding GitHub URLs:** Construct them from the remote URL pattern. If the repo moves, only one base URL needs updating.
- **Quality report that re-parses files:** Query the database -- it's what was actually stored, and it's fast. Re-parsing would be slow and might produce different results than what's in the DB.
- **Generating report to a file:** Decision is CLI output only. Keep it simple -- `click.echo()` output that can be piped if needed.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Sidebar navigation for sub-page | Manual sidebar config | Docusaurus autogenerated sidebar + frontmatter `sidebar_position` | Autogenerated sidebar already handles the whole site |
| Pipeline flow diagram | ASCII art or image file | Mermaid `graph LR` in markdown | Already configured, renders in light/dark mode, version-controlled as text |
| Markdown table of contents | Manual anchor links | Docusaurus auto-generated TOC | Built-in feature, stays in sync automatically |
| GitHub URL construction | Manual URL strings | Pattern-based construction from git remote | Consistent, maintainable, single point of change |
| Config reference documentation | Prose descriptions | Table generated from Settings class fields | Settings class has 14 fields with types and defaults -- table is the right format |

**Key insight:** This phase is documentation, not code creation. The temptation is to build elaborate tooling (report generators, doc builders, link validators). Resist it -- manual writing with standard patterns produces better documentation faster.

## Common Pitfalls

### Pitfall 1: Getting Started Page Becomes a Second Chapter 6
**What goes wrong:** The Getting Started page duplicates the design rationale already in Chapter 6 (generation tagging strategy, chunking philosophy, hybrid search design). Readers get confused about which page to read.
**Why it happens:** Natural instinct to provide complete context. The writer re-explains concepts instead of linking to them.
**How to avoid:** Start with "why" in the sense of "why does the pipeline exist and what approach does it take" (brief framing), then transition quickly to "how" (practical code tour, setup steps, running commands). Link to Chapter 6 sections for deep design rationale.
**Warning signs:** Paragraphs that start with "The reason we chose..." or "The design decision was..." -- those belong in Chapter 6.

### Pitfall 2: README Assumes Too Much or Too Little
**What goes wrong:** README either assumes the reader has PostgreSQL and Ollama already running (skipping setup), or spends pages explaining what Python is.
**Why it happens:** Audience mismatch -- writing for yourself vs. writing for a new team member.
**How to avoid:** The decision is explicit: "readable by outsiders, optimized for internal BASIS engineers." This means: full step-by-step for prerequisites (PostgreSQL, Ollama, uv), but don't explain Python basics. Include version requirements and verification commands (`psql --version`, `ollama --version`, `uv --version`).
**Warning signs:** "Install PostgreSQL" with no version, no verification command, no troubleshooting tip.

### Pitfall 3: Quality Report Shows Raw Numbers Without Context
**What goes wrong:** Report shows "flare: 2847 chunks" but doesn't indicate whether that's healthy or problematic. Engineers can't tell if something went wrong.
**Why it happens:** Building the counting query is easy; building the interpretation layer takes more thought.
**How to avoid:** Include automated warnings for anomalies:
- Empty sources (0 chunks from a configured source)
- Unbalanced distributions (one source has >90% of all chunks)
- Unknown doc types (chunks with empty or unexpected doc_type values)
- Suspiciously low counts (fewer than 10 chunks from a source that should have hundreds)
- All-untagged generations (100% of chunks from a source are "untagged")
**Warning signs:** A report that only has numbers and no warnings section.

### Pitfall 4: GitHub Links Break Silently
**What goes wrong:** Code links in the Getting Started page point to files that don't exist or have moved. Links break with no automated check.
**Why it happens:** Documentation is authored separately from code. Renames and moves happen without updating docs.
**How to avoid:** Use a consistent URL pattern and link to files at a stable level (not deep function-level links that break on refactoring). Prefer file-level links for most references; use line-range links only for key data models or protocol definitions that are unlikely to move.
**Warning signs:** Links to specific line numbers for utility functions that change frequently.

### Pitfall 5: Mermaid Diagram Becomes Too Complex
**What goes wrong:** Pipeline diagram tries to show every parser, every intelligence module, every database table. Becomes unreadable on mobile.
**Why it happens:** Desire for completeness.
**How to avoid:** One high-level diagram (5 sources -> 5 pipeline stages -> 1 store). Chapter 6 already has the detailed version. The Getting Started diagram is an orientation map, not a technical specification.
**Warning signs:** More than 15 nodes in a single Mermaid graph.

## Code Examples

### Example 1: Getting Started Page Frontmatter and Structure
```markdown
---
sidebar_position: 1
title: "Getting Started with RAG Ingestion"
description: "Source-by-source ingestion approach, pipeline architecture, and setup guide"
---

# Getting Started with RAG Ingestion

## Why This Approach

Brief framing: source-by-source ingestion, generation tagging, why chunking
matters. Links to [Chapter 6](./index.md) for full design rationale.

## Pipeline Architecture

Mermaid diagram: Sources -> Parse -> Tag -> Chunk -> Embed -> Store

## Sources

Table or sections covering each source type with links to parser code.

## Key Data Models

Inline snippets of Document and Chunk models with GitHub links to full files.

## Running the Pipeline

Brief reference pointing to the README for full setup and CLI usage.
```

### Example 2: README Structure
```markdown
# BBj RAG Ingestion Pipeline

One-sentence description.

## Overview

2-3 sentences on what the pipeline does and what sources it supports.

## Prerequisites

Step-by-step: PostgreSQL + pgvector, Ollama, Python 3.12+, uv

## Installation

git clone, cd, uv sync

## Configuration

### Config File (config.toml)
Table of all settings with types, defaults, descriptions.

### Environment Variables
BBJ_RAG_ prefix pattern.

## Usage

### Full Ingestion
bbj-rag ingest --source flare

### Parse Only (Debug)
bbj-rag parse --source pdf

### Quality Report
bbj-rag report

### Search Validation
bbj-rag validate

## Project Structure

Tree of src/bbj_rag/ with one-line descriptions.

## Development

make check, make test, make lint, make format

## Further Reading

Links to docs site Getting Started page and Chapter 6.
```

### Example 3: Quality Report CLI Output Format
```
=== BBj RAG Quality Report ===

Chunks by Source:
  flare            2,847  (68.2%)
  advantage          412  (9.9%)
  kb                 389  (9.3%)
  pdf                287  (6.9%)
  mdx                156  (3.7%)
  bbj-source          82  (2.0%)
  ─────────────────────────────
  Total            4,173

Chunks by Generation:
  all              1,823  (43.7%)
  bbj_gui          1,412  (33.8%)
  dwc                634  (15.2%)
  vpro5              189  (4.5%)
  character           73  (1.7%)
  untagged            42  (1.0%)

Chunks by Document Type:
  api-reference    1,956  (46.9%)
  concept            892  (21.4%)
  language-ref       534  (12.8%)
  tutorial           312  (7.5%)
  article            287  (6.9%)
  example            112  (2.7%)
  migration           54  (1.3%)
  best-practice       26  (0.6%)

Warnings:
  [!] 42 chunks (1.0%) have generation "untagged" -- review source tagging
  [!] No chunks from source "web-crawl" -- verify crawl configuration
```

### Example 4: Quality Report Anomaly Warning Logic
```python
def _check_anomalies(
    by_source: dict[str, int],
    by_generation: dict[str, int],
    by_doc_type: dict[str, int],
    total: int,
) -> list[str]:
    """Generate warning messages for quality anomalies."""
    warnings: list[str] = []

    # Empty sources
    expected_sources = {"flare", "advantage", "kb", "pdf", "mdx", "bbj-source"}
    for src in expected_sources:
        if by_source.get(src, 0) == 0:
            warnings.append(f'No chunks from source "{src}" -- verify configuration')

    # Suspiciously low counts
    for src, count in by_source.items():
        if 0 < count < 10:
            warnings.append(
                f'Only {count} chunks from "{src}" -- expected more'
            )

    # Untagged generation percentage
    untagged = by_generation.get("untagged", 0)
    if total > 0 and untagged / total > 0.05:
        pct = (untagged / total) * 100
        warnings.append(
            f'{untagged} chunks ({pct:.1f}%) have generation "untagged"'
        )

    # Unknown doc types
    known_types = {
        "api-reference", "concept", "example", "migration",
        "language-reference", "best-practice", "version-note",
        "article", "tutorial",
    }
    for dt in by_doc_type:
        if dt not in known_types:
            warnings.append(f'Unknown doc_type "{dt}" ({by_doc_type[dt]} chunks)')

    # Dominant source (>90%)
    if total > 0:
        for src, count in by_source.items():
            if count / total > 0.90:
                warnings.append(
                    f'Source "{src}" has {count/total*100:.0f}% of all chunks '
                    f"-- verify other sources ingested"
                )

    return warnings
```

### Example 5: Database Query for Report Data
```python
def _query_report_data(
    conn: psycopg.Connection[object],
) -> tuple[dict[str, int], dict[str, int], dict[str, int], int]:
    """Query chunk distribution from the database.

    Returns (by_source, by_generation, by_doc_type, total).
    """
    with conn.cursor() as cur:
        # Total count
        cur.execute("SELECT COUNT(*) FROM chunks")
        total = int(cur.fetchone()[0])  # type: ignore[index]

        # By source (derived from source_url prefix)
        cur.execute("""
            SELECT
                CASE
                    WHEN source_url LIKE 'flare://%' THEN 'flare'
                    WHEN source_url LIKE 'https://basis.cloud/advantage%' THEN 'advantage'
                    WHEN source_url LIKE 'https://basis.cloud/knowledge%' THEN 'kb'
                    WHEN source_url LIKE 'pdf://%' THEN 'pdf'
                    WHEN source_url LIKE 'file://%' THEN 'bbj-source'
                    WHEN source_url LIKE 'mdx://%' THEN 'mdx'
                    ELSE 'unknown'
                END AS source,
                COUNT(*) AS cnt
            FROM chunks
            GROUP BY source
            ORDER BY cnt DESC
        """)
        by_source = {str(row[0]): int(row[1]) for row in cur.fetchall()}

        # By generation (unnest array)
        cur.execute("""
            SELECT g, COUNT(*) AS cnt
            FROM chunks, unnest(generations) AS g
            GROUP BY g
            ORDER BY cnt DESC
        """)
        by_generation = {str(row[0]): int(row[1]) for row in cur.fetchall()}

        # By doc_type
        cur.execute("""
            SELECT doc_type, COUNT(*) AS cnt
            FROM chunks
            GROUP BY doc_type
            ORDER BY cnt DESC
        """)
        by_doc_type = {str(row[0]): int(row[1]) for row in cur.fetchall()}

    return by_source, by_generation, by_doc_type, total
```

## Discretionary Recommendations

The following items were left to Claude's discretion in the CONTEXT.md. Here are research-backed recommendations:

### Report Granularity: Three-Way Breakdown (Not Cross-Tabs)
**Recommendation:** Use three independent breakdowns (by-source, by-generation, by-doc-type) rather than cross-tabulations (e.g., source x generation matrix).

**Rationale:** Cross-tabs produce sparse matrices that are hard to read in CLI output. A 6-source x 5-generation matrix has 30 cells, most of which would be zero (e.g., MDX source will only have "dwc" generation). Three separate breakdowns give clearer signal with fewer numbers. If an engineer needs cross-tab analysis, they can query the database directly with SQL.

### GitHub URL Format: Branch-Based (Not Tag-Based)
**Recommendation:** Use `blob/main/` for all GitHub URLs.

**Rationale:** The project has zero release tags. The git log shows only feature commits on `main`. There is no release cadence (no semver tags, no GitHub releases). Branch-based URLs always point to current code, which is the correct behavior for documentation describing an active implementation. If the project later adopts release tagging, URLs can be updated to `blob/v{version}/`.

### Deep Link Level: File-Level Default, Line-Level for Key Models Only
**Recommendation:** Default to file-level links (e.g., `blob/main/.../models.py`). Use line-range links only for:
- `Document` and `Chunk` model definitions in `models.py`
- `DocumentParser` protocol in `parsers/__init__.py`
- `Settings` class in `config.py`
- `Embedder` protocol in `embedder.py`

**Rationale:** Line numbers change on every commit that modifies the file. File-level links are stable. Line-level links are justified only for key definitions that readers need to see immediately without scrolling -- the four cases above are stable, high-value anchors that define the pipeline's contracts.

### Pipeline Diagram Format: Mermaid `graph LR`
**Recommendation:** Use Mermaid `graph LR` (left-to-right flow graph).

**Rationale:** Mermaid is already configured in the Docusaurus site (`markdown.mermaid: true`, `@docusaurus/theme-mermaid`). Chapter 6's `index.md` already uses `graph LR` with subgraph grouping and color styling. Using the same format maintains visual consistency. Mermaid renders natively in both light and dark modes via the configured `mermaid.theme` settings. ASCII art would not support dark mode or responsive layout. A static image would require separate file management.

### README Section Ordering
**Recommendation:** Use this section order for the README:

1. Title + one-line description
2. Overview (what the pipeline does, 2-3 sentences)
3. Prerequisites (PostgreSQL + pgvector, Ollama, Python 3.12+, uv)
4. Installation (clone, cd, uv sync)
5. Configuration (TOML reference table, env vars)
6. Usage (CLI commands: ingest, parse, report, validate)
7. Project Structure (tree with descriptions)
8. Development (make targets)
9. Further Reading (links to docs site)

**Rationale:** This follows the "onboarding funnel" pattern: what is this -> what do I need -> how do I install it -> how do I configure it -> how do I use it -> how is it organized -> how do I develop on it. Each section answers the next natural question a new engineer would ask.

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Manual sidebar config for Docusaurus sub-pages | Autogenerated sidebar with frontmatter `sidebar_position` | Docusaurus v2.x+ | No sidebar.ts changes needed for sub-pages |
| Separate Mermaid plugin (`mdx-mermaid`) | Built-in `@docusaurus/theme-mermaid` with `markdown.mermaid: true` | Docusaurus v2.4+ | Simpler config, Mermaid 11+ support |
| GitHub `blob/master/` links | GitHub `blob/main/` links | GitHub default branch rename ~2020 | Must use `main` not `master` |

**Deprecated/outdated:**
- `mdx-mermaid` npm package: Replaced by official `@docusaurus/theme-mermaid`. Do not install.
- Mermaid `graph TD` (top-down) for pipeline diagrams: `graph LR` (left-to-right) is standard for pipeline/flow representations and matches Chapter 6's existing diagrams.

## Open Questions

1. **source_url prefix completeness for report queries**
   - What we know: Current parsers use these prefixes: `flare://`, `https://basis.cloud/advantage`, `https://basis.cloud/knowledge`, `pdf://`, `file://`, `mdx://`
   - What's unclear: Whether the WordPress URLs might have variations (e.g., trailing paths differ from the `LIKE` patterns). The `advantage_index_url` defaults to `https://basis.cloud/advantage-index/` and `kb_index_url` to `https://basis.cloud/knowledge-base/`.
   - Recommendation: Verify the actual `source_url` patterns stored by WordPress parsers before finalizing the CASE statement. A simple `SELECT DISTINCT LEFT(source_url, 30) FROM chunks` after ingestion will confirm patterns. Use a generous `LIKE` pattern (e.g., `'%basis.cloud/advantage%'`) to capture variations.

2. **Existing intelligence/report.py reuse vs. new module**
   - What we know: `intelligence/report.py` has `build_report()` and `print_report()` that operate on in-memory `list[Document]`. The new quality report needs DB-query-based data.
   - What's unclear: Whether to extend the existing module or create a new one.
   - Recommendation: Create the new DB-based report logic as a separate function (e.g., `print_quality_report(conn)`) in the same `intelligence/report.py` module. This keeps all reporting logic in one place while keeping the existing `build_report` / `print_report` functions intact for in-memory use (they're used in tests and could be useful for parse-only debugging).

## Sources

### Primary (HIGH confidence)
- **Codebase inspection** -- Direct reading of all 24 Python source files in `rag-ingestion/src/bbj_rag/`, `docusaurus.config.ts`, `sidebars.ts`, all `_category_.json` files, and `docs/06-rag-database/index.md`
- **Git remote** -- `git remote get-url origin` confirms `https://github.com/StephanWald/bbj-ai-strategy.git`
- **Docusaurus official docs** -- [Autogenerated sidebars](https://docusaurus.io/docs/sidebar/autogenerated), [Diagrams](https://docusaurus.io/docs/next/markdown-features/diagrams)
- **GitHub official docs** -- [Getting permanent links to files](https://docs.github.com/en/repositories/working-with-files/using-files/getting-permanent-links-to-files)

### Secondary (MEDIUM confidence)
- **WebSearch: Docusaurus Mermaid support** -- Confirmed Mermaid 11+ support in Docusaurus v3.6+, consistent with project config
- **WebSearch: GitHub URL patterns** -- Confirmed `blob/` for files, `tree/` for directories, `#L{n}-L{m}` for line ranges

### Tertiary (LOW confidence)
- None. All findings verified against codebase or official documentation.

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- No new libraries needed; all tools already installed and configured
- Architecture: HIGH -- All patterns verified against existing codebase structure and Docusaurus docs
- Pitfalls: HIGH -- Based on direct codebase analysis and documentation writing experience
- Discretionary items: HIGH -- Each recommendation supported by codebase evidence

**Research date:** 2026-02-01
**Valid until:** 2026-03-01 (stable -- no fast-moving dependencies)
