---
phase: 09-schema-data-models
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - rag-ingestion/sql/schema.sql
  - rag-ingestion/src/bbj_rag/db.py
  - rag-ingestion/src/bbj_rag/schema.py
  - rag-ingestion/tests/test_db.py
autonomous: true

must_haves:
  truths:
    - "Schema DDL creates the chunks table with BIGSERIAL id, vector(1536) column, tsvector GENERATED ALWAYS AS column, TEXT[] generations column, and UNIQUE content_hash"
    - "HNSW index (cosine), GIN index on search_vector, and GIN index on generations are all defined in the DDL"
    - "insert_chunk function uses ON CONFLICT (content_hash) DO NOTHING for idempotent re-ingestion"
    - "pgvector types are registered on every connection via register_vector()"
    - "JSONB metadata is properly serialized using psycopg Json() adapter"
  artifacts:
    - path: "rag-ingestion/sql/schema.sql"
      provides: "Complete pgvector DDL with table, indexes, and extension"
      contains: "CREATE TABLE IF NOT EXISTS chunks"
    - path: "rag-ingestion/src/bbj_rag/db.py"
      provides: "Connection management, chunk insert with dedup, batch insert"
      exports: ["get_connection", "insert_chunk", "insert_chunks_batch"]
    - path: "rag-ingestion/src/bbj_rag/schema.py"
      provides: "DDL execution helper to apply schema.sql"
      exports: ["apply_schema"]
    - path: "rag-ingestion/tests/test_db.py"
      provides: "Unit tests for connection helper and insert SQL generation"
  key_links:
    - from: "rag-ingestion/src/bbj_rag/db.py"
      to: "rag-ingestion/src/bbj_rag/models.py"
      via: "import Chunk for type annotations in insert functions"
      pattern: "from bbj_rag\\.models import Chunk"
    - from: "rag-ingestion/src/bbj_rag/db.py"
      to: "pgvector.psycopg"
      via: "register_vector() on connection open"
      pattern: "register_vector"
    - from: "rag-ingestion/src/bbj_rag/schema.py"
      to: "rag-ingestion/sql/schema.sql"
      via: "reads SQL file and executes via connection"
      pattern: "schema\\.sql"
    - from: "rag-ingestion/src/bbj_rag/db.py"
      to: "rag-ingestion/src/bbj_rag/config.py"
      via: "uses Settings.database_url for connection"
      pattern: "from bbj_rag\\.config import Settings"
---

<objective>
Create the pgvector schema DDL, database connection module, and schema application helper that wire the Pydantic data models (from Plan 09-01) to PostgreSQL storage with idempotent deduplication.

Purpose: This completes the database layer that all pipeline components write to. Parsers produce Chunk objects (Plan 09-01), and this module stores them with ON CONFLICT deduplication. The DDL is the database contract — every index (HNSW, GIN) and generated column (tsvector) is defined here.

Output: sql/schema.sql DDL file, db.py connection/insert module, schema.py DDL applicator, and unit tests.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-schema-data-models/09-RESEARCH.md
@.planning/phases/09-schema-data-models/09-01-SUMMARY.md
@rag-ingestion/pyproject.toml
@rag-ingestion/src/bbj_rag/models.py
@rag-ingestion/src/bbj_rag/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pgvector schema DDL file</name>
  <files>
    rag-ingestion/sql/schema.sql
  </files>
  <action>
Create the directory `rag-ingestion/sql/` and the file `rag-ingestion/sql/schema.sql` containing:

1. **Extension:** `CREATE EXTENSION IF NOT EXISTS vector;`

2. **Table:** `CREATE TABLE IF NOT EXISTS chunks` with columns:
   - `id BIGSERIAL PRIMARY KEY`
   - `source_url TEXT NOT NULL`
   - `title TEXT NOT NULL`
   - `doc_type TEXT NOT NULL`
   - `content TEXT NOT NULL`
   - `content_hash VARCHAR(64) NOT NULL UNIQUE`
   - `generations TEXT[] NOT NULL DEFAULT '{}'`
   - `embedding vector(1536)` — nullable, populated by embedding pipeline later
   - `search_vector tsvector GENERATED ALWAYS AS (to_tsvector('english', coalesce(title, '') || ' ' || coalesce(content, ''))) STORED`
   - `metadata JSONB NOT NULL DEFAULT '{}'`
   - `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`
   - `updated_at TIMESTAMPTZ NOT NULL DEFAULT now()`

3. **Indexes:**
   - `CREATE INDEX IF NOT EXISTS idx_chunks_embedding_hnsw ON chunks USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);`
   - `CREATE INDEX IF NOT EXISTS idx_chunks_search_vector_gin ON chunks USING GIN (search_vector);`
   - `CREATE INDEX IF NOT EXISTS idx_chunks_generations_gin ON chunks USING GIN (generations);`

**Critical pitfalls from RESEARCH.md:**
- Use 2-arg `to_tsvector('english', ...)` — single-arg form is STABLE, not IMMUTABLE, and fails in GENERATED columns
- Use `||` operator for string concatenation — `concat()` / `concat_ws()` are STABLE, not IMMUTABLE
- Wrap columns with `coalesce(col, '')` since `NULL || 'text'` returns NULL
- The UNIQUE constraint on content_hash automatically creates a btree index; no separate index needed

Add a header comment explaining the schema purpose and the pitfalls avoided.
  </action>
  <verify>
    File `rag-ingestion/sql/schema.sql` exists.
    File contains `CREATE EXTENSION IF NOT EXISTS vector`.
    File contains `CREATE TABLE IF NOT EXISTS chunks` with all columns.
    File contains all three CREATE INDEX statements (HNSW, GIN search_vector, GIN generations).
    File uses `to_tsvector('english',` (two-arg form) and `||` (not concat).
  </verify>
  <done>Standalone DDL file defines the complete chunks table with vector column, tsvector generated column, generation array, UNIQUE content_hash, and all three indexes (HNSW cosine, GIN text search, GIN generations).</done>
</task>

<task type="auto">
  <name>Task 2: Create database connection module and schema helper</name>
  <files>
    rag-ingestion/src/bbj_rag/db.py
    rag-ingestion/src/bbj_rag/schema.py
  </files>
  <action>
1. Create `rag-ingestion/src/bbj_rag/db.py` with:

   **get_connection(database_url: str) -> psycopg.Connection:**
   - Opens a psycopg connection to the given URL
   - Calls `register_vector(conn)` immediately after connect — this is MANDATORY for pgvector type handling (Pitfall 3 from RESEARCH.md)
   - Returns the connection (caller manages lifecycle via context manager or explicit close)
   - Import: `import psycopg` and `from pgvector.psycopg import register_vector`

   **insert_chunk(conn: psycopg.Connection, chunk: Chunk) -> bool:**
   - Executes INSERT with ON CONFLICT (content_hash) DO NOTHING and RETURNING id
   - Uses `psycopg.types.json.Json(chunk.metadata)` for the JSONB column (Pitfall 6 from RESEARCH.md)
   - Parameters: (source_url, title, doc_type, content, content_hash, generations, embedding, Json(metadata))
   - Returns True if inserted (RETURNING yielded a row), False if duplicate skipped
   - The embedding parameter is chunk.embedding (may be None — that's fine, the column is nullable)

   **insert_chunks_batch(conn: psycopg.Connection, chunks: list[Chunk]) -> int:**
   - Uses `cur.executemany()` with the same INSERT ... ON CONFLICT DO NOTHING query
   - Returns `cur.rowcount` (number of rows actually inserted, excluding conflict skips)
   - Each chunk's metadata must be wrapped with `Json()`

   Import `from bbj_rag.models import Chunk` for type annotations.

2. Create `rag-ingestion/src/bbj_rag/schema.py` with:

   **apply_schema(conn: psycopg.Connection) -> None:**
   - Locates `sql/schema.sql` relative to the package (use `Path(__file__).resolve().parent.parent.parent / "sql" / "schema.sql"` — going from src/bbj_rag/schema.py up to rag-ingestion/sql/schema.sql). Alternatively, use `importlib.resources` or a relative path constant. The key is that it works when run from the rag-ingestion/ directory.
   - Reads the SQL file content
   - Executes it via `conn.execute(sql_content)`
   - Commits the transaction: `conn.commit()`
   - Prints a confirmation message or logs success

   **Why a helper?** The DDL lives in SQL (readable by DBAs, testable independently) but needs a Python entry point for the pipeline to call `apply_schema()` during setup.

3. Run lint and type checks:
   - `cd rag-ingestion && uv run ruff check src/bbj_rag/db.py src/bbj_rag/schema.py`
   - `cd rag-ingestion && uv run mypy src/bbj_rag/db.py src/bbj_rag/schema.py`
   Fix any issues. If mypy complains about psycopg or pgvector missing stubs, add overrides to pyproject.toml:
   ```toml
   [[tool.mypy.overrides]]
   module = ["pgvector.*", "psycopg.*"]
   ignore_missing_imports = true
   ```
  </action>
  <verify>
    `cd rag-ingestion && uv run python -c "from bbj_rag.db import get_connection, insert_chunk, insert_chunks_batch; print('OK')"` — imports succeed.
    `cd rag-ingestion && uv run python -c "from bbj_rag.schema import apply_schema; print('OK')"` — import succeeds.
    `cd rag-ingestion && uv run ruff check src/bbj_rag/db.py src/bbj_rag/schema.py` — clean.
    `cd rag-ingestion && uv run mypy src/bbj_rag/` — clean.
  </verify>
  <done>db.py provides get_connection (with register_vector), insert_chunk (with ON CONFLICT dedup and Json adapter), and insert_chunks_batch. schema.py reads and executes the standalone DDL file.</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for database module (no live DB required)</name>
  <files>
    rag-ingestion/tests/test_db.py
  </files>
  <action>
Create `rag-ingestion/tests/test_db.py` with tests that verify the database module's logic WITHOUT requiring a running PostgreSQL instance. These are structural/import tests and SQL-correctness tests:

1. **test_db_module_exports** — verify `get_connection`, `insert_chunk`, `insert_chunks_batch` are importable from `bbj_rag.db`

2. **test_schema_module_exports** — verify `apply_schema` is importable from `bbj_rag.schema`

3. **test_schema_sql_file_exists** — verify that `rag-ingestion/sql/schema.sql` exists and is readable (use Path to locate it relative to the test file or project root)

4. **test_schema_sql_contains_required_elements** — read schema.sql content and assert it contains:
   - `CREATE EXTENSION IF NOT EXISTS vector`
   - `CREATE TABLE IF NOT EXISTS chunks`
   - `content_hash` with `UNIQUE`
   - `vector(1536)`
   - `to_tsvector('english'` (two-arg form)
   - `GENERATED ALWAYS AS`
   - `hnsw` index
   - `GIN` indexes (at least 2)
   - `ON` keyword (part of index definitions)

5. **test_schema_sql_avoids_pitfalls** — read schema.sql and assert:
   - Does NOT contain `concat(` or `concat_ws(` (STABLE functions forbidden in generated columns)
   - Uses `||` for concatenation
   - Uses `coalesce(` for NULL handling

6. **test_chunk_insert_uses_dedup** — inspect the insert_chunk function (via `inspect.getsource`) or read db.py source to verify the INSERT statement contains `ON CONFLICT (content_hash) DO NOTHING`. This is a code-level assertion, not a DB test.

Run `cd rag-ingestion && uv run pytest -v` and verify all tests pass (both 09-01 and 09-02 tests).
Run `cd rag-ingestion && uv run ruff check tests/` — clean.
  </action>
  <verify>
    `cd rag-ingestion && uv run pytest -v` — all tests pass (models, config, and db tests).
    `cd rag-ingestion && uv run ruff check src/ tests/` — clean.
    `cd rag-ingestion && uv run mypy src/bbj_rag/` — clean.
  </verify>
  <done>Tests verify: schema.sql exists with all required DDL elements (vector column, tsvector generated column, HNSW/GIN indexes, UNIQUE content_hash), avoids known pitfalls (concat, single-arg to_tsvector), and db.py insert uses ON CONFLICT for deduplication. Full test suite passes.</done>
</task>

</tasks>

<verification>
1. `rag-ingestion/sql/schema.sql` contains complete DDL with CREATE EXTENSION, CREATE TABLE, and all 3 indexes
2. `cd rag-ingestion && uv run python -c "from bbj_rag.db import get_connection, insert_chunk; from bbj_rag.schema import apply_schema"` — all imports work
3. `cd rag-ingestion && uv run pytest -v` — full test suite passes (models + config + db tests)
4. `cd rag-ingestion && uv run ruff check src/ tests/` — clean
5. `cd rag-ingestion && uv run mypy src/bbj_rag/` — clean
6. schema.sql uses two-arg to_tsvector, || operator, coalesce — no immutability pitfalls
7. db.py uses register_vector(), Json() adapter, ON CONFLICT DO NOTHING
</verification>

<success_criteria>
- Schema DDL file creates chunks table with all columns (vector, tsvector generated, generations array, content_hash UNIQUE) and all indexes (HNSW, GIN x2)
- db.py insert_chunk uses ON CONFLICT (content_hash) DO NOTHING for idempotent re-ingestion
- db.py get_connection calls register_vector() on every connection
- db.py insert functions use Json() adapter for JSONB metadata
- schema.py reads and executes the standalone SQL file
- All tests pass, lint clean, types check
</success_criteria>

<output>
After completion, create `.planning/phases/09-schema-data-models/09-02-SUMMARY.md`
</output>
