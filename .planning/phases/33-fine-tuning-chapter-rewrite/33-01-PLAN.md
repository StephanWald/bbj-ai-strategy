---
phase: 33-fine-tuning-chapter-rewrite
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [docs/03-fine-tuning/index.md]
autonomous: true

must_haves:
  truths:
    - "Reader understands why 14B-Base is recommended over 7B-Base and 32B-Instruct, with tradeoff analysis visible in a comparison table and narrative"
    - "Reader can follow the two-stage training approach (continued pretraining then instruction fine-tuning) and understand why it matters for zero-representation languages"
    - "The relationship between bbjllm repo and recommended approach is documented with side-by-side table, three named blocker issues, and constructive framing"
    - "Evaluation methodology section describes bbjcpl-based compile@1 metric, qualitative review, baseline comparison, test set structure, and includes a sample eval test case"
    - "Training data pipeline connecting training-data/ markdown format to bbjllm ChatML JSONL is described with clear contributor workflow"
  artifacts:
    - path: "docs/03-fine-tuning/index.md"
      provides: "Rewritten Chapter 3 core content (bbjllm foundation, model selection, training data, QLoRA+two-stage, evaluation)"
      contains: "## The bbjllm Foundation"
  key_links:
    - from: "docs/03-fine-tuning/index.md"
      to: "bbjllm gap analysis -> model selection rationale"
      via: "bbjllm section establishes context for why 14B-Base is recommended"
      pattern: "alignment tax"
    - from: "docs/03-fine-tuning/index.md"
      to: "training data -> two-stage training"
      via: "training data section feeds into QLoRA two-stage methodology"
      pattern: "continued pretraining"
    - from: "docs/03-fine-tuning/index.md"
      to: "evaluation methodology -> bbjcpl"
      via: "evaluation section references compile@1 metric using bbjcpl"
      pattern: "compile@1"
---

<objective>
Rewrite the core content sections of Chapter 3 (Fine-Tuning the Model): replace the TL;DR, add a new bbjllm Foundation section, rewrite Base Model Selection with 14B-Base recommendation and alignment tax analysis, modify Training Data with pipeline description, integrate two-stage training into the QLoRA section, and add a new Evaluation Methodology section.

Purpose: This plan addresses FT-01, FT-03, FT-04, FT-05, FT-06, FT-07, and FT-09 -- the intellectually demanding content that forms the chapter's new narrative arc. The bbjllm context informs model selection, which informs training approach, which informs evaluation. Writing them together ensures internal consistency.

Output: `docs/03-fine-tuning/index.md` with core sections rewritten (lines 1-239 substantially changed, new evaluation section inserted after QLoRA). Sections after line 239 (Toolchain, Hosting, Status) are left for Plan 33-02.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/fine-tuning/SUMMARY.md
@.planning/phases/33-fine-tuning-chapter-rewrite/33-RESEARCH.md
@.planning/phases/33-fine-tuning-chapter-rewrite/33-CONTEXT.md
@docs/03-fine-tuning/index.md
@training-data/FORMAT.md
@training-data/gui/hello-window.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace TL;DR, add bbjllm Foundation section, rewrite Base Model Selection</name>
  <files>docs/03-fine-tuning/index.md</files>
  <action>
Read the current chapter (423 lines). Edit from line 1 through the end of the Base Model Selection section (~line 169). Preserve the YAML front matter sidebar_position and title. Make these changes:

**1. Front matter description (line 4):**
Update to mention 14B-Base: "How to create a BBj-aware language model using QLoRA fine-tuning on Qwen2.5-Coder-14B-Base, with self-hosted deployment via Ollama."

**2. TL;DR block (lines 9-11):**
Replace entirely. New TL;DR must mention:
- Qwen2.5-Coder-14B-Base (not 7B)
- Two-stage training approach (continued pretraining + instruction fine-tuning)
- bbjllm as existing implementation that informs the approach
- Evaluation via bbjcpl compile@1 metric
- Ollama for self-hosted inference (keep this)
- Do NOT include "$1,500 GPU" detail (user decision: no cost/hardware in model section)

**3. Intro paragraphs (lines 13-15):**
Minor updates only. The conceptual framing is still accurate. Update "This chapter is the technical blueprint" paragraph to preview the new sections: bbjllm context, model selection rationale, training data pipeline, two-stage approach, evaluation methodology, and deployment.

**4. NEW SECTION: "## The bbjllm Foundation" -- insert AFTER the intro paragraphs, BEFORE Training Data:**
This is an entirely new section (~80-120 lines). Structure:

a) Opening paragraph: Frame bbjllm as a "valuable first attempt" that demonstrated feasibility, produced working training infrastructure, and created 9,922 ChatML training examples using Qwen2.5-Coder-32B-Instruct with QLoRA/PEFT. Tone: constructive, acknowledging accomplishment.

b) "What bbjllm built" list: Working QLoRA pipeline, 9,922 curated ChatML examples, proved fine-tuning is viable for BBj, established Qwen2.5-Coder as the right model family.

c) Side-by-side table (per user decision -- LOCKED):
| Aspect | bbjllm Current | Recommended Approach |
Rows for: Model, Model variant, Training stages, Loss computation, Validation, Learning rate, Library stack, Evaluation, Artifact management
Values from .planning/research/fine-tuning/SUMMARY.md "Gap Analysis" section.

d) Three blocker issues (per user decision -- LOCKED: "Be direct and specific about the 3 blocker issues: name each one, explain why each is a blocker, state the fix"):
  - **No validation set**: 100% data used for training, overfitting detection impossible. Fix: 90/10 train/val split with evaluation_strategy="steps".
  - **Full-sequence loss computation**: Loss on system prompt + user question + assistant response wastes 30-40% of gradient signal on constant tokens. Fix: mask non-assistant tokens or use TRL SFTTrainer's completion-only mode.
  - **Instruct model choice**: 32B-Instruct fine-tuning risks performance degeneration (alignment tax). Fix: switch to Base variant for domain adaptation.

e) Closing paragraph: "The recommended approach described in this chapter builds on bbjllm's foundation while addressing these gaps." Transition to model selection.

**5. Rewrite "## Base Model Selection" section (currently lines 118-169):**

a) New decision callout:
```
:::info[Decision: Qwen2.5-Coder-14B-Base as Primary Recommendation]
**Choice:** Qwen2.5-Coder-14B-Base for fine-tuning, selected based on research into training outcomes and alignment characteristics.
**Rationale:** 14B shows greater improvement from fine-tuning than 7B (per Qwen technical report), remains trainable on a single 24GB GPU, and the Base variant avoids the alignment tax of fine-tuning an already instruction-tuned model.
**Alternatives considered:** 7B-Base (smaller but less fine-tuning headroom), 32B-Instruct (used by bbjllm; higher base quality but alignment tax makes domain adaptation counterproductive).
**Status:** Active research -- bbjllm experiment used 32B-Instruct; research recommends switching to 14B-Base for next training iteration.
:::
```

b) Keep the `### Why Qwen2.5-Coder` subsection heading EXACTLY as-is (preserve `#why-qwen25-coder` anchor for cross-references from chapters 4 and 7). Update the content: change benchmark references to reflect 14B as recommendation while keeping the Qwen family overview.

c) New comparison table (per user decision -- LOCKED: "Three focused rows with tradeoffs, make 14B-Base preference clear"):
Focus on training suitability, NOT cost/hardware (user decision: no cost/hardware implications).

| Model | Parameters | Fine-Tuning Improvement | Base vs Instruct | Alignment Tax Risk | Recommendation |
| Qwen2.5-Coder-7B-Base | 7B | Moderate | Base (clean slate) | None | Starting point for experimentation |
| **Qwen2.5-Coder-14B-Base** | 14B | **High** (per Qwen technical report) | **Base (clean slate)** | **None** | **Primary recommendation** |
| Qwen2.5-Coder-32B-Instruct | 32B | Low (alignment tax) | Instruct (pre-aligned) | High | Not recommended for domain fine-tuning |

d) New subsection "### The Alignment Tax" (per user decision -- LOCKED: "Explain the alignment tax concept, define it, explain why Instruct models resist domain adaptation, cite research"):
- Start with practical consequence: "Fine-tuning an Instruct model on BBj data risks degrading its ability to follow instructions -- the very capability that makes it useful."
- Define alignment tax: the hidden cost of fine-tuning a model that has already been trained to follow instructions. The model's weights encode BOTH domain knowledge AND instruction-following behavior; domain fine-tuning overwrites both simultaneously.
- Cite Shadow-FT research (ICLR 2025): research demonstrates that directly fine-tuning Instruct models can lead to "marginal improvements and even performance degeneration."
- Connect back to BBj: this is why bbjllm's choice of 32B-Instruct is identified as a blocker. The model may learn BBj syntax but degrade its response quality.
- Close: "This is why the recommended approach uses 14B-Base, not 32B-Instruct."

e) Landscape comparison table update (FT-09): Update the broader model comparison table. Add Qwen3 dense models. Note CodeLlama and StarCoder2 as superseded. Remove DeepSeek-V3 row (671B MoE is irrelevant for fine-tuning context). Update Qwen3-Coder note to confirm MoE-only with no dense fine-tunable variants.

f) Keep the "Model selection is not a permanent decision" closing paragraph (still valid).

**CRITICAL CONSTRAINTS:**
- Do NOT include cost/hardware implications in model comparison (user decision)
- Do NOT use adversarial language about bbjllm ("critical flaw", "fundamentally broken") -- use constructive framing ("gaps the recommended approach addresses")
- PRESERVE the `### Why Qwen2.5-Coder` heading exactly to maintain the `#why-qwen25-coder` anchor
- Use Phase 32 status terminology: "operational", "operational for internal exploration", "active research", "planned"
  </action>
  <verify>
1. Grep for "7B" in the modified sections -- should only appear in the comparison table rows for 7B-Base, NOT as the recommended model
2. Grep for "alignment tax" -- must appear in at least 2 contexts (bbjllm blocker + dedicated subsection)
3. Verify `### Why Qwen2.5-Coder` heading is preserved exactly
4. Grep for prohibited terms: "shipped", "production-grade", "deployed" as final state -- none should appear
5. Verify the bbjllm side-by-side table has rows for at minimum: Model, Training stages, Loss computation, Validation
  </verify>
  <done>
- TL;DR references 14B-Base, two-stage training, bbjllm context, bbjcpl evaluation, and Ollama
- bbjllm Foundation section exists with constructive framing, side-by-side table, and three named blocker issues
- Base Model Selection recommends 14B-Base with comparison table, alignment tax explanation, and landscape update
- `#why-qwen25-coder` anchor preserved
- No cost/hardware implications in model comparison section
  </done>
</task>

<task type="auto">
  <name>Task 2: Modify Training Data + QLoRA sections, add Evaluation Methodology</name>
  <files>docs/03-fine-tuning/index.md</files>
  <action>
Continue editing the chapter from the Training Data section through to the end of the QLoRA section, then add the new Evaluation Methodology section. Leave everything after (Toolchain, Hosting, Status) untouched for Plan 33-02.

**1. Modify "## Training Data Structure" section (currently lines 17-116):**

a) Keep the section heading and opening paragraph about data quality (lines 17-19). Still accurate.

b) "### Format and Schema" subsection: The current chapter shows a JSON format that was never actually used. Replace with the ACTUAL format from `training-data/FORMAT.md`:
- Show the training-data/ Markdown with YAML front matter format as the canonical contributor format
- Show a real example from `training-data/gui/hello-window.md` (the actual seed file)
- Keep the generation labeling table (lines 49-58) -- it is still accurate and used in the Markdown format

c) Keep the "### Example Types" subsection (lines 60-67) -- still accurate.

d) Keep the "### BBj Code in Training Data" subsection with the OrderForm code example (lines 69-96) -- still accurate, good illustration. Note: verify the code block is valid BBj by checking that it matches known BBj patterns (bbjcpl validation will happen in 33-02).

e) "### Volume and Quality Targets" subsection (lines 98-116): Update to reflect reality:
- Change "10,000 to 50,000 targets" to actual state: bbjllm has 9,922 ChatML examples, training-data/ repo has 2 seed examples with 7 topic directories ready for expansion
- Update the data collection strategy to reflect what actually happened vs. what is planned
- Update the decision callout Status field: change "Approximately 10,000 training examples curated" to "bbjllm repo contains 9,922 ChatML examples created independently. The training-data/ repository provides 2 seed examples in Markdown format with JSON Schema validation and contributor guides. Conversion pipeline from Markdown to ChatML is planned."

f) NEW subsection "### Training Data Pipeline" (FT-06, ~40-60 lines):
Describe the two-repo relationship clearly so a reader can answer "If I want to add a training example, what format do I use and where do I put it?"

- **training-data/ repository** (this repo): Markdown with YAML front matter format. Human-readable, validated against JSON Schema, organized by topic (gui/, etc.). This is where contributors create new examples. Show the format briefly (title, type, generation in YAML front matter + Code/Explanation sections).
- **bbjllm repository** (separate repo): ChatML JSONL format. 9,922 examples created independently for training. This is the actual input to the training script.
- **Conversion pipeline** (planned): A `convert_to_chatml.py` script to transform training-data/ Markdown examples into ChatML JSONL suitable for training. This pipeline does not yet exist -- the bbjllm examples were created independently, not converted from the training-data/ format.
- **Flow diagram**: Show: Markdown (contributor creates) -> ChatML JSONL (training input) -> Fine-tuned model -> GGUF (deployment). Can be a simple text flow or Mermaid diagram.
- Mention data quality issues at summary level: the bbjllm dataset has known quality issues (duplicates, formatting inconsistencies) that should be addressed before the next training run. Do NOT reproduce the full statistical breakdown -- keep it to one paragraph.

**2. Modify "## The QLoRA Fine-Tuning Approach" section (currently lines 171-239):**

a) Keep section heading (preserves `#the-qlora-fine-tuning-approach` anchor).

b) Update opening paragraph: change "7B parameter model" references to "14B". The QLoRA description is still accurate.

c) Keep "### How LoRA Works" subsection largely intact. Update the hardware comparison table:
- Change "VRAM Required (7B)" column header to "VRAM Required (14B)"
- Update QLoRA row to ~16-20 GB for 14B (still single RTX 4090)
- Other rows scale proportionally

d) "### Recommended Hyperparameters" (lines 191-205): Keep the table. Add a note after the table:
- Note that bbjllm's training script uses 2e-5 learning rate (5-10x lower than QLoRA recommendations), which risks undertraining. The recommended range of 2e-4 to 5e-5 in the table above is correct for QLoRA.
- Add a row or note about completion masking: "Train on assistant responses only (mask system/user tokens with -100 in labels, or use TRL SFTTrainer which handles this automatically)."

e) "### Sequential Fine-Tuning" (lines 208-229): This section already describes two-stage training. Updates:
- Rename to "### Two-Stage Training Approach" (or keep as-is -- Claude's discretion on heading)
- Update the Mermaid diagram: change "Qwen2.5-Coder-7B" to "Qwen2.5-Coder-14B-Base"
- Update Stage 2 description: change "structured JSON examples" to "ChatML examples" to match reality
- Add context from bbjllm: "The bbjllm experiment skipped Stage 1 (continued pretraining) and went directly to instruction fine-tuning on the 32B-Instruct model. The recommended approach includes Stage 1 because BBj has near-zero representation in the base model's pre-training data."
- Explain WHY Stage 1 matters for zero-representation languages: the base model needs to learn BBj syntax patterns (tokenization, control flow, object notation) before it can learn to follow instructions about BBj.

f) Keep "### Avoiding Catastrophic Forgetting" (lines 231-239) -- still accurate.

**3. NEW SECTION: "## Evaluation Methodology" (FT-04, ~100-150 lines):**
Insert AFTER the QLoRA section, BEFORE the Toolchain section. This is entirely new content.

a) Opening paragraph: Explain that without a BBj-specific evaluation framework, improvements from fine-tuning cannot be measured. No public BBj benchmark exists, so a custom evaluation approach is required.

b) "### compile@1: Automated Syntax Validation"
- Define compile@1: the percentage of generated BBj code samples that compile successfully on first attempt using bbjcpl.
- Explain the metric: prompt the model with N test cases, collect generated code, validate each with `bbjcpl -N`, compute pass rate.
- Why bbjcpl is BBj's "secret weapon": the compiler IS ground truth for syntactic correctness. This creates a unique, non-gameable metric that most niche-language fine-tuning efforts lack.

c) "### Qualitative Evaluation"
- Human review criteria (per user decision -- LOCKED: "Cover both compile@1 (automated) and qualitative evaluation"):
  - Code quality: Does the generated code follow BBj conventions? (naming, structure, error handling)
  - Idiomatic patterns: Does the model use generation-appropriate idioms? (e.g., DWC patterns for DWC context, not legacy character patterns)
  - Documentation quality: Are explanations accurate and helpful?

d) "### Baseline Comparison"
- Name specific baselines (per user decision -- LOCKED):
  1. **Qwen2.5-Coder-14B-Base** (unmodified): What can the base model do before fine-tuning? This is the floor.
  2. **Claude API** (current production system): What does the current RAG + Claude approach achieve? This is the bar to clear.
  3. **bbjllm 32B output**: What did the existing fine-tuning experiment produce? This is the comparison point.
- Explain: Run the same test set against all three. If the fine-tuned model does not beat the base model on compile@1, fine-tuning has not helped. If it does not approach Claude API quality, it may not justify deployment.

e) "### Test Set Structure"
- Held-out split: 10% of training data reserved for evaluation (never used in training)
- Category coverage: test cases should span all generation labels (all, character, vpro5, bbj-gui, dwc) and all example types (comprehension, completion, migration, explanation)
- Size: minimum 50-100 test cases for statistically meaningful results

f) "### Sample Evaluation Test Case" (per user decision -- LOCKED: "Include a sample eval test case showing a prompt and what pass/fail looks like with bbjcpl"):
Show a concrete example:

Prompt: "Write a BBj program that creates a window with a button. When the button is clicked, display a message box saying 'Hello'."

PASS example: syntactically valid BBj that compiles with `bbjcpl -N` (show the code, show `bbjcpl -N` returning exit code 0)

FAIL example: code with a common LLM fabrication error (e.g., using `.addEventListener()` instead of `.setCallback()`, or missing `methodend`) -- show `bbjcpl -N` returning compilation error

This makes the methodology tangible. Use realistic BBj code for both examples.

g) "### Reporting Format"
- Brief description of how to report results: table with model name, compile@1 score, qualitative notes, date.

**CRITICAL CONSTRAINTS:**
- Do NOT edit anything after the QLoRA section in the existing file (leave Toolchain, Hosting, Status for Plan 33-02)
- The Evaluation Methodology section must be actionable enough that "someone could build the eval suite from this section" (user decision)
- The sample eval test case must use realistic BBj code, not pseudocode
- Training data format should show the ACTUAL Markdown format from training-data/FORMAT.md, not the theoretical JSON format currently in the chapter
  </action>
  <verify>
1. Verify "## Evaluation Methodology" section exists with subsections for compile@1, qualitative, baseline comparison, test set structure, and sample test case
2. Grep for "compile@1" -- must appear in the evaluation section
3. Grep for "training-data/" -- must appear in the Training Data Pipeline subsection
4. Grep for "ChatML" -- must appear in both training data pipeline and QLoRA context
5. Verify the Mermaid diagram in sequential fine-tuning references "14B-Base" not "7B"
6. Verify the sample eval test case shows both a PASS and FAIL example with bbjcpl
7. Verify no edits were made to the Toolchain section (## Toolchain) or anything after it
  </verify>
  <done>
- Training Data section shows Markdown format from training-data/ as canonical, references bbjllm's 9,922 ChatML examples, describes the conversion pipeline, and mentions data quality issues at summary level
- QLoRA section references 14B instead of 7B, notes bbjllm's learning rate issue, includes completion masking guidance, and integrates two-stage training with bbjllm context
- Evaluation Methodology section is a new major section with compile@1 metric, qualitative criteria, three named baselines, test set structure, a concrete sample test case (pass+fail), and reporting format
- Toolchain, Hosting, and Status sections are untouched (left for 33-02)
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. The chapter should now have these major sections in order: TL;DR, intro, The bbjllm Foundation, Training Data Structure (updated), Base Model Selection (rewritten), The QLoRA Fine-Tuning Approach (modified), Evaluation Methodology (new), Toolchain (unchanged), Hosting (unchanged), Current Status (unchanged)
2. Wait -- the recommended section ordering from research puts bbjllm BEFORE Training Data, and Model Selection BEFORE Training Data. Verify the actual ordering matches the research recommendation: bbjllm Foundation -> Base Model Selection -> Training Data -> QLoRA+Two-Stage -> Evaluation
3. `npm run build` should pass (though some content in later sections may reference outdated versions -- that is expected and will be fixed in 33-02)
4. No prohibited terminology in the new/modified content: "shipped", "production-grade", "deployed" as final state
5. Cross-reference anchors preserved: `#why-qwen25-coder`, `#the-qlora-fine-tuning-approach`
</verification>

<success_criteria>
- Chapter 3 core content rewritten with bbjllm context, 14B-Base recommendation, updated training data, two-stage training, and evaluation methodology
- All 7 requirements covered by this plan are addressed: FT-01 (14B-Base), FT-03 (Base vs Instruct), FT-04 (evaluation), FT-05 (bbjllm relationship), FT-06 (training pipeline), FT-07 (two-stage), FT-09 (comparison table)
- Chapter builds without errors (`npm run build`)
- No prohibited terminology in modified content
</success_criteria>

<output>
After completion, create `.planning/phases/33-fine-tuning-chapter-rewrite/33-01-SUMMARY.md`
</output>
