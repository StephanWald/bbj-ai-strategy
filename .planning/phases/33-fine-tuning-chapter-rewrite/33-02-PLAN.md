---
phase: 33-fine-tuning-chapter-rewrite
plan: 02
type: execute
wave: 2
depends_on: [33-01]
files_modified: [docs/03-fine-tuning/index.md]
autonomous: true

must_haves:
  truths:
    - "Toolchain section reflects current library versions (Unsloth 2026.x, transformers 5.x, peft 0.18.x, Ollama 0.15.x) with version comparison table"
    - "Training workflow section describes artifact management, what to commit back, and iterative improvement process"
    - "Hosting section references 14B model sizes, current Ollama version, and correct MCP tool status"
    - "Status block follows Phase 32 conventions with correct terminology (operational, active research, planned)"
    - "No prohibited terminology survives anywhere in the chapter: no 'shipped', 'production-grade', 'deployed' as final state"
    - "Chapter builds cleanly with npm run build"
  artifacts:
    - path: "docs/03-fine-tuning/index.md"
      provides: "Complete rewritten Chapter 3 with all sections updated"
      contains: "## Training Workflow"
  key_links:
    - from: "docs/03-fine-tuning/index.md (Toolchain)"
      to: "version comparison table"
      via: "bbjllm version vs current version table"
      pattern: "transformers.*5\\.1"
    - from: "docs/03-fine-tuning/index.md (Status)"
      to: "Phase 32 status conventions"
      via: ":::note[Where Things Stand] block"
      pattern: "Active research|Operational|Planned"
---

<objective>
Complete the Chapter 3 rewrite by updating the Toolchain section with current library versions, adding a new Training Workflow section, updating the Hosting section for 14B model, replacing the Current Status block with Phase 32 conventions, and performing a full-file consistency and tone pass.

Purpose: This plan addresses FT-02 (toolchain versions) and FT-08 (training workflow), plus mechanical updates to Hosting and Status that must be consistent with the core content rewritten in Plan 33-01. It also performs the critical full-file pass to catch stale references (7B, old versions, prohibited terminology) that might survive in retained sections.

Output: `docs/03-fine-tuning/index.md` fully complete, all sections updated, build-verified.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/fine-tuning/SUMMARY.md
@.planning/phases/33-fine-tuning-chapter-rewrite/33-RESEARCH.md
@.planning/phases/33-fine-tuning-chapter-rewrite/33-01-SUMMARY.md
@docs/03-fine-tuning/index.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update Toolchain, add Training Workflow, update Hosting section</name>
  <files>docs/03-fine-tuning/index.md</files>
  <action>
Read the current chapter (as modified by 33-01). Edit the Toolchain, Hosting, and Status sections. Add the new Training Workflow section.

**1. Modify "## Toolchain: Unsloth + llama.cpp + Ollama" section:**

a) Update the Mermaid pipeline diagram:
- Change "Qwen2.5-Coder-7B" to "Qwen2.5-Coder-14B-Base" in the diagram
- Change "JSON examples" to "ChatML examples" in the DATA node (matching the training data pipeline described in 33-01)

b) Update Unsloth description:
- Add version reference: Unsloth 2026.1.4
- Update capabilities: mention Dynamic 4-bit Quantization (better accuracy for critical parameters vs. uniform NF4), 500K context support, built-in GGUF export (can eliminate llama.cpp conversion step in some workflows)
- Keep the 2x speed / 70% VRAM claims (still accurate per Unsloth docs)
- Keep alternative framework mentions (LLaMA-Factory, Axolotl)

c) NEW subsection "### Version Comparison" (FT-02):
Add a table showing bbjllm's pinned versions vs. current versions with key changes. This makes the upgrade path concrete.

| Library | bbjllm Version | Current (Feb 2026) | Key Changes |
|---------|---------------|-------------------|-------------|
| transformers | 4.44.0 | 5.1.0 | Major v5 release (first in 5 years), significant API changes |
| peft | 0.12.0 | 0.18.1 | 6 minor versions, Python 3.9 dropped |
| bitsandbytes | 0.43.0 | 0.49.1 | **Critical:** 0.43.0 has QLoRA memory bug wasting 5-10GB VRAM (fixed in 0.43.2) |
| trl | (not used) | 0.27.x | SFTTrainer with built-in completion masking and packing |
| Unsloth | (not used) | 2026.1.4 | Recommended replacement for raw PEFT; 2-3x faster, 70% less VRAM |

Add a note: "The bbjllm training script pins library versions via starttrain.sh. Before the next training run, update at minimum bitsandbytes (critical bug fix) and add trl for completion masking support."

d) Update quantization table for 14B model sizes:
- F16: ~28 GB (was ~14 GB for 7B)
- Q8_0: ~14 GB (was ~7 GB)
- Q4_0: ~8 GB (was ~4 GB)
- **Q4_K_M: ~8.5 GB** (was ~4.5 GB) -- still recommended
- Q2_K: ~5 GB (was ~2.5 GB)

e) Update Modelfile example: change `bbj-coder-7b-q4_k_m.gguf` to `bbj-coder-14b-q4_k_m.gguf`.

**2. NEW SECTION: "## Training Workflow" (FT-08, ~30-50 lines):**
Insert AFTER Toolchain, BEFORE Hosting.

a) "### Artifact Management"
- LoRA adapter weights (100-300MB): commit to repo with Git LFS. Currently adapters exist only on the training server -- if rebuilt, all training results are lost.
- Merged model weights (full precision): store on training server, do not commit (too large)
- GGUF quantized models: store in model registry for distribution
- Training logs and metrics: track with W&B (free tier) or TensorBoard for run comparison

b) "### What to Commit Back"
After each training run, commit:
- Updated adapter weights (Git LFS)
- Model card with: hyperparameters, training metrics (final loss, validation loss), evaluation results (compile@1 score, baseline comparison), date and duration
- Any dataset changes (deduplication, corrections, new examples)

c) "### Iterative Improvement Process"
The training cycle:
1. Evaluate current model against baseline (compile@1 + qualitative)
2. Identify weak areas (which generation labels? which example types?)
3. Create targeted training examples for weak areas
4. Retrain with updated dataset
5. Evaluate again -- compare to previous run
6. If improved, export to GGUF and update Ollama deployment

Brief, practical -- not academic. This section should read like a recipe, not a research paper.

**3. Modify "## Hosting via Ollama" section:**

a) Update the decision callout:
- Change `v0.9.x+` to `v0.15.x` in the Choice field
- Update Status: "Ollama infrastructure validated for internal exploration. Model packaging workflow defined but not yet automated."

b) Update hardware requirements table:
- All model size references change from 7B to 14B
- Minimum tier: 16GB RAM (was 8GB), 14B Q4_K_M ~8.5GB
- Recommended tier: 24GB+ RAM, GPU with 12GB+ VRAM
- Team/Enterprise tiers: scale accordingly

c) Update API compatibility paragraph:
- Change "Ollama v0.9.x" to "Ollama v0.15.x"
- Soften the Anthropic-format claim: "Ollama supports OpenAI-compatible API endpoints. Recent versions have added support for tool calling and structured JSON output."

d) Update MCP Integration subsection:
- The current text says the fine-tuned model "is consumed through the BBj MCP server's generate_bbj_code tool" without clarifying this tool does not yet exist
- Add status clarity: "The `generate_bbj_code` tool described in Chapter 2 is planned but not yet implemented. When built, it will accept a natural language prompt..."
- Change present tense ("is consumed", "forwards it") to future tense where describing planned functionality

e) Update "Model updates are distributed" paragraph to reference 14B model.

**4. Replace "## Current Status" section:**
Delete the existing content (lines 409-421 area) and replace with Phase 32 conventions:

```markdown
## Current Status

:::note[Where Things Stand]
- **Active research:** bbjllm experiment (9,922 ChatML examples fine-tuned on Qwen2.5-Coder-32B-Instruct via QLoRA/PEFT). Research recommends switching to Qwen2.5-Coder-14B-Base with two-stage training approach to address identified gaps (validation, loss masking, model variant).
- **Operational:** Training data repository with 2 seed examples, 7 topic directories, JSON Schema validation, and contributor guides. Toolchain components (Unsloth, llama.cpp, Ollama) are publicly available and actively maintained.
- **Planned:** Evaluation suite using bbjcpl-based compile@1 metric. Training data conversion pipeline (training-data/ Markdown to ChatML JSONL). Training workflow with artifact management and iterative improvement.
:::
```

**5. Update closing paragraph:**
- Ensure cross-references to other chapters still work
- Update any remaining "7B" references to "14B"
- Ensure the paragraph reflects the chapter's new structure

**CRITICAL CONSTRAINTS:**
- All version numbers must match .planning/research/fine-tuning/SUMMARY.md (source of truth for versions)
- Status block must use Phase 32 terminology exactly: "operational", "operational for internal exploration", "active research", "planned" -- no other status labels
- MCP integration must clearly distinguish operational (search_bbj_knowledge, validate_bbj_syntax) from planned (generate_bbj_code)
- No dates in status blocks (v1.3 decision from Phase 32 conventions)
  </action>
  <verify>
1. Grep for "v0.9" -- should NOT appear anywhere in the chapter (replaced with v0.15.x)
2. Grep for "7B" -- should only appear in comparison table rows, bbjllm references (describing what it used), and the Q4_0 row (if historical). Should NOT appear as the recommended model.
3. Verify "## Training Workflow" section exists
4. Verify version comparison table has rows for transformers, peft, bitsandbytes, trl, Unsloth
5. Verify Status block uses Phase 32 format: `:::note[Where Things Stand]` with bold status labels
6. Verify Modelfile references `bbj-coder-14b-q4_k_m.gguf`
  </verify>
  <done>
- Toolchain section updated with Unsloth 2026.1.4 features, version comparison table, and 14B model sizes
- Training Workflow section added with artifact management, commit practices, and iterative improvement
- Hosting section updated to Ollama v0.15.x, 14B hardware requirements, and clarified MCP tool status
- Current Status block replaced with Phase 32 conventions
- Closing paragraph updated with 14B references
  </done>
</task>

<task type="auto">
  <name>Task 2: Full-file consistency pass and build verification</name>
  <files>docs/03-fine-tuning/index.md</files>
  <action>
Perform a systematic full-file review and build verification. This is the quality gate before the chapter is considered complete.

**1. Terminology scan:**
Search the ENTIRE file for prohibited terms and fix any found:
- "shipped" -> "operational" or remove
- "production" -> "operational for internal exploration" or context-appropriate replacement (note: "production" is allowed when referring to customer environments per Phase 32 decision)
- "deployed" as final state -> "operational for internal exploration"
- "production-grade" -> "functional" or remove
- "production corpus" -> "full documentation corpus"
- "in progress" -> "active research" or "planned" as appropriate

**2. Model reference consistency:**
Search for ALL occurrences of "7B" in the file. Each should be in one of these contexts:
- Comparison table row for the 7B-Base option
- Historical reference to what bbjllm used or what was previously recommended
- The QLoRA hardware comparison table (if it mentions 7B as a comparison point)

Any other "7B" reference (e.g., "fine-tune the 7B model") must be updated to "14B".

**3. Version number consistency:**
Verify these version numbers appear correctly throughout:
- Ollama: v0.15.x (not v0.9.x)
- transformers: 5.1.0 (in version table)
- peft: 0.18.1 (in version table)
- bitsandbytes: 0.49.1 (in version table)
- Unsloth: 2026.1.4 (in Toolchain section)

**4. Cross-reference anchor verification:**
Confirm these anchors exist and are correct:
- `#why-qwen25-coder` (from `### Why Qwen2.5-Coder` heading)
- `#the-qlora-fine-tuning-approach` (from `## The QLoRA Fine-Tuning Approach` heading)

**5. Decision callout format verification:**
Every `:::info[Decision: ...]` block must have all four fields:
- **Choice:**
- **Rationale:**
- **Alternatives considered:**
- **Status:** (using Phase 32 terminology)

**6. Mermaid diagram consistency:**
Check all Mermaid diagrams in the file:
- All should reference "14B" not "7B"
- Training data flow should reference ChatML (not JSON) where describing actual training input

**7. Build verification:**
Run `npm run build` from the project root. The build must complete with zero errors. Warnings about other chapters' stale cross-references (e.g., Chapter 2 referencing "7B") are expected and will be fixed in Phase 36 -- they should NOT be fixed in this plan.

**8. BBj code validation (if bbjcpl available):**
If `bbjcpl` is available on the system, run it against any BBj code blocks in the chapter. If not available, note this in the summary and flag for manual verification.

**CRITICAL CONSTRAINTS:**
- Do NOT edit files outside docs/03-fine-tuning/index.md
- Cross-reference issues in OTHER chapters are Phase 36 scope -- do not fix them here
- If build warnings are about OTHER chapters' links to Chapter 3, note them in the summary for Phase 36 but do not fix
  </action>
  <verify>
1. `grep -c "shipped\|production-grade\|production corpus" docs/03-fine-tuning/index.md` returns 0
2. `npm run build` completes without errors (warnings about other chapters are acceptable)
3. grep for `### Why Qwen2.5-Coder` confirms anchor heading exists
4. Every `:::info[Decision:` block has Choice, Rationale, Alternatives considered, Status fields
5. All Mermaid diagrams reference 14B
  </verify>
  <done>
- No prohibited terminology anywhere in the chapter
- All model references are consistent (14B as recommendation, 7B only in comparison/historical context)
- All version numbers match research source of truth
- Cross-reference anchors preserved
- All decision callouts have complete four-field format
- Chapter builds cleanly with `npm run build`
- Full Chapter 3 rewrite complete and verified
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. Chapter 3 is a complete, self-consistent document with all 9 FT requirements addressed
2. `npm run build` passes cleanly (zero errors)
3. No prohibited terminology survives
4. All cross-reference anchors are preserved
5. Status block matches Phase 32 conventions
6. Version numbers match research source of truth
7. All references to the recommended model say "14B-Base" (not "7B")
</verification>

<success_criteria>
- Toolchain section updated with current versions and comparison table (FT-02)
- Training Workflow section added with artifact management and iteration process (FT-08)
- Hosting section reflects 14B model and current Ollama version
- Status block uses Phase 32 conventions
- Full-file consistency pass complete with zero prohibited terms
- `npm run build` passes
- Chapter 3 rewrite fully complete
</success_criteria>

<output>
After completion, create `.planning/phases/33-fine-tuning-chapter-rewrite/33-02-SUMMARY.md`
</output>
