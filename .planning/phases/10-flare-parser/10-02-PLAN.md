---
phase: 10-flare-parser
plan: 02
type: tdd
wave: 2
depends_on: ["10-01"]
files_modified:
  - rag-ingestion/src/bbj_rag/parsers/flare.py
  - rag-ingestion/tests/test_flare_parser.py
autonomous: true

must_haves:
  truths:
    - "Parser reads raw Flare XHTML files and produces Document objects with extracted text, headings, and source paths"
    - "MadCap namespace tags are handled per-type: keyword/concept stripped, xref/toggler keep text, snippets resolved and inlined"
    - "Code blocks (<pre><code class='language-*'>) are preserved as markdown fenced code blocks with language hints"
    - "HTML tables (Methods_Table, Parameter_Table, Flag_Table) are converted to markdown table format"
    - "Code_Table class tables are extracted as code blocks, not markdown tables"
    - "TOC hierarchy paths appear in Document metadata as arrow-separated breadcrumbs"
    - "Orphan topics (78% of all files) get directory-based fallback hierarchy paths"
    - "Snippets (.flsnp files) are resolved from their relative src paths and inlined into content"
    - "Topic-level conditions are extracted and mapped to generations on each Document"
    - "Parser yields Documents via Iterator (not loading all 7,083 files into memory)"
  artifacts:
    - path: "rag-ingestion/src/bbj_rag/parsers/flare.py"
      provides: "FlareParser class implementing DocumentParser protocol"
      exports: ["FlareParser"]
    - path: "rag-ingestion/tests/test_flare_parser.py"
      provides: "Tests for Flare XHTML parser against real project files"
  key_links:
    - from: "rag-ingestion/src/bbj_rag/parsers/flare.py"
      to: "rag-ingestion/src/bbj_rag/parsers/flare_toc.py"
      via: "build_toc_index for hierarchy lookup"
      pattern: "from bbj_rag\\.parsers\\.flare_toc import build_toc_index"
    - from: "rag-ingestion/src/bbj_rag/parsers/flare.py"
      to: "rag-ingestion/src/bbj_rag/parsers/flare_cond.py"
      via: "extract_topic_conditions + map_conditions_to_generations for Document.generations"
      pattern: "from bbj_rag\\.parsers\\.flare_cond import"
    - from: "rag-ingestion/src/bbj_rag/parsers/flare.py"
      to: "rag-ingestion/src/bbj_rag/models.py"
      via: "Document model as output type"
      pattern: "from bbj_rag\\.models import Document"
    - from: "rag-ingestion/src/bbj_rag/parsers/flare.py"
      to: "lxml.etree"
      via: "XML parsing of .htm and .flsnp files"
      pattern: "etree\\.parse|etree\\.XMLParser"
---

<objective>
Implement the Flare XHTML parser -- the core of Phase 10. This parser reads raw MadCap Flare project files from a local Content/ directory and produces Document objects with properly extracted text, code blocks, tables, hierarchy paths, and generation metadata. This is the largest and most complex parser, handling 7,083 topic files with 12 types of MadCap namespace tags, 205 snippets, code block preservation, and table-to-markdown conversion.

Purpose: The Flare documentation is the biggest source in the RAG pipeline (~55 MB of XHTML). Getting this parser right validates the entire pipeline architecture -- if it handles Flare correctly, everything else is simpler.

Output: FlareParser class that yields Document objects from a Flare Content/ directory, with comprehensive tests against real project files.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-flare-parser/10-RESEARCH.md
@.planning/phases/10-flare-parser/10-01-SUMMARY.md
@rag-ingestion/src/bbj_rag/models.py
@rag-ingestion/src/bbj_rag/parsers/__init__.py
@rag-ingestion/src/bbj_rag/parsers/flare_toc.py
@rag-ingestion/src/bbj_rag/parsers/flare_cond.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement FlareParser with MadCap tag handling, snippet resolution, code blocks, and table conversion</name>
  <files>rag-ingestion/src/bbj_rag/parsers/flare.py</files>
  <action>
    Create `flare.py` implementing the `DocumentParser` protocol. The parser must handle the full complexity of MadCap Flare XHTML files.

    **Class: FlareParser**
    ```python
    class FlareParser:
        def __init__(self, content_dir: Path, project_dir: Path) -> None:
            # content_dir: path to Content/ directory (e.g., /Users/beff/bbjdocs/Content)
            # project_dir: path to Project/ directory (e.g., /Users/beff/bbjdocs/Project)
            # Build TOC index and load snippets in __init__
        def parse(self) -> Iterator[Document]: ...
    ```

    **Initialization:**
    - Build TOC index from project_dir/TOCs/ using `build_toc_index()`
    - Pre-load all 205 snippets from content_dir/Resources/Snippets/ into `dict[str, etree._Element]`
      - Key: canonical path relative to Content/ (e.g., "Resources/Snippets/Unspecified_xywh.flsnp")
      - Value: parsed `<body>` element of the snippet
    - Store content_dir for file iteration

    **parse() method:**
    - Walk content_dir recursively for all .htm files
    - EXCLUDE content_dir/Resources/ directory (snippets, styles, images -- not topics)
    - For each .htm file:
      1. Parse with `etree.XMLParser(remove_comments=True)` -- files are valid XML
      2. Extract title: `<head><title>` text, fallback to first `<h1>` text, fallback to filename
      3. Extract topic-level conditions using `extract_topic_conditions(root)`
      4. Map conditions to generations using `map_conditions_to_generations(conditions)`
      5. Look up hierarchy path from TOC index; fall back to `directory_fallback_path()`
      6. Extract body content using the content extraction pipeline (see below)
      7. Build `source_url` from Content-relative path (e.g., `"flare://Content/bbjobjects/..."`)
      8. Yield `Document(source_url=..., title=..., doc_type="flare", content=..., generations=..., metadata={...})`
      9. Put conditions, section_path (hierarchy), and inline_conditions into metadata dict
    - Skip files that produce empty content after extraction
    - Handle XML parse errors gracefully: log warning, skip file, continue

    **Content extraction pipeline (process body element):**
    Walk the `<body>` element recursively. For each element:

    1. **MadCap namespace elements** -- dispatch by local tag name:
       - `keyword`, `concept`, `miniTocProxy`, `indexProxy`, `conceptLink`, `microContent`: **Strip entirely** (skip element and all children)
       - `xref`, `toggler`: **Keep text content** -- collect `itertext()`, strip the tag wrapper
       - `snippetBlock`: **Resolve snippet** -- resolve `src` attribute relative to the topic file's directory, find in pre-loaded snippets dict, recursively extract snippet body content as block-level content
       - `snippetText`: **Resolve snippet** -- same resolution but inline (no paragraph breaks)
       - `popup`: **Keep popupHead text only** -- find `MadCap:popupHead` child, extract its text, ignore `popupBody`
       - `variable`: **Strip** (only 2 in entire corpus, both system variables)

    2. **Code blocks** (`<pre>` containing `<code class="language-*">`):
       - Detect `<pre>` elements whose child is `<code>` with `class` attribute containing "language-"
       - Extract language from class: `"language-bbj"` -> `"bbj"`, `"language-java"` -> `"java"`, etc.
       - Get code text preserving whitespace (use `itertext()` on the `<code>` element)
       - Output as markdown fenced code block: triple-backtick + language + newline + code + newline + triple-backtick
       - Do NOT process children of `<pre>` as normal text

    3. **Tables:**
       - If table has class `Code_Table`: treat as code block extraction (extract the `<pre><code>` inside), not as a table
       - For `Methods_Table`, `Parameter_Table`, `Flag_Table`, and other non-code tables:
         - Extract rows and cells using `.findall(".//tr")`, then `th`/`td` within each row
         - Cell text: recursively extract text from cell (may contain nested elements)
         - Build markdown table: header row, separator row (`---`), data rows, joined with `" | "`
       - Handle tables with no rows gracefully (skip)

    4. **Standard HTML elements** (headings, paragraphs, lists, etc.):
       - `<h1>` through `<h6>`: Output as markdown headings (`# `, `## `, etc.) with extracted text
       - `<p>`: Extract text, output as paragraph (separated by blank line)
       - `<p class="Code">`: Single-line code -- wrap in backticks
       - `<ul>`, `<ol>`: Process `<li>` children with `- ` or `1. ` prefixes
       - `<a href="...">`: Keep link text only (strip href -- these are internal Flare links)
       - Other block elements (`<div>`, `<section>`): Process children recursively
       - Inline elements (`<b>`, `<i>`, `<span>`, `<em>`, `<strong>`): Keep text content

    5. **MadCap attributes on standard elements:**
       - `MadCap:conditions` on any element: Collect for inline condition tracking but do NOT exclude the content (downstream decides)
       - `MadCap:autosort*`, `MadCap:targetName`: Ignore (strip attribute, keep element)

    **Snippet resolution detail:**
    - Snippet `src` attributes are relative paths like `"../../../Resources/Snippets/foo.flsnp"`
    - Resolve against the topic file's parent directory using `pathlib.Path.resolve()`
    - Convert resolved path to Content-relative key for snippet dict lookup
    - If snippet not found: log warning, output empty string (graceful degradation)
    - Snippet XHTML files have the same structure as topics -- parse body, extract content recursively
    - Guard against circular snippet references (a snippet referencing itself)

    **Error handling:**
    - XML parse errors (BOM issues, malformed XML): log warning with filename, skip file
    - File encoding issues: try UTF-8 first (lxml default handles BOM)
    - Missing snippet files: log warning, continue without snippet content
    - Empty content after extraction: skip document (don't yield empty Documents)

    **Performance:**
    - Process one file at a time, yield Document objects (generator pattern)
    - Pre-load snippets once (205 files, small in memory)
    - Pre-build TOC index once
    - Do NOT accumulate all Documents in a list

    **Type annotations:**
    - Full mypy strict compliance
    - Use `from __future__ import annotations`
    - Type the snippet dict, tag handlers, and all helper functions
  </action>
  <verify>
    ```bash
    cd rag-ingestion
    uv run python -c "
    from pathlib import Path
    from bbj_rag.parsers.flare import FlareParser
    p = FlareParser(
        content_dir=Path('/Users/beff/bbjdocs/Content'),
        project_dir=Path('/Users/beff/bbjdocs/Project'),
    )
    docs = list(p.parse())
    print(f'Parsed {len(docs)} documents')
    # Expect roughly 5000-7000 documents (some files may be empty after extraction)
    assert len(docs) > 4000, f'Too few documents: {len(docs)}'
    # Spot check first few
    for d in docs[:3]:
        print(f'  {d.title}: {len(d.content)} chars, generations={d.generations}')
        assert d.content.strip(), 'Empty content'
        assert d.generations, 'No generations'
    print('Quick sanity check passed')
    "
    uv run mypy src/bbj_rag/parsers/flare.py
    uv run ruff check src/bbj_rag/parsers/flare.py
    ```
  </verify>
  <done>
    - FlareParser class created implementing DocumentParser protocol
    - Parses 5,000+ documents from real Flare Content/ directory
    - MadCap namespace tags handled per research recommendations (12 tag types)
    - Code blocks preserved as markdown fenced blocks with language hints
    - Tables converted to markdown format (Code_Table handled separately)
    - Snippets resolved and inlined from .flsnp files
    - TOC hierarchy and directory fallback paths in metadata
    - Conditions extracted and mapped to generations
    - mypy strict and ruff pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Write comprehensive tests for FlareParser against real Flare project files</name>
  <files>rag-ingestion/tests/test_flare_parser.py</files>
  <action>
    Write tests that validate the FlareParser against the actual Flare project at `/Users/beff/bbjdocs/`.

    Use `pytest.mark.skipif(not Path("/Users/beff/bbjdocs").exists(), reason="Flare source not available")` on all real-file tests.

    **Integration tests (real files):**
    - `test_parse_yields_documents`: Parse real Content/ dir, verify yields >4,000 Document objects
    - `test_document_has_required_fields`: Check first N documents have non-empty source_url, title, content, generations
    - `test_document_content_no_madcap_tags`: Verify no document content contains `"MadCap:"` string (namespace tags should be stripped/handled)
    - `test_document_has_section_path`: Verify documents have `section_path` in metadata (either from TOC or directory fallback)
    - `test_toc_topic_has_arrow_breadcrumb`: Find a known TOC topic, verify its section_path uses " > " separators
    - `test_orphan_topic_has_directory_path`: Find a topic NOT in TOC, verify section_path derived from directory
    - `test_code_blocks_preserved`: Find a topic known to have code blocks (e.g., in bbjobjects/), verify content contains markdown fenced code blocks (triple backticks)
    - `test_code_block_has_language_hint`: Verify code blocks include language (```bbj, ```java, etc.)
    - `test_tables_converted_to_markdown`: Find a topic with Methods_Table or Parameter_Table, verify content contains markdown table separators (` | ` and `---`)
    - `test_snippet_resolution`: Find a topic known to use snippetBlock, verify content includes resolved snippet text (not empty gaps)
    - `test_conditions_mapped_to_generations`: Verify documents with Primary.BASISHelp condition have "bbj" in generations
    - `test_deprecated_topic_flagged`: Find a deprecated topic, verify "deprecated" appears in generations or metadata
    - `test_empty_content_skipped`: Verify no yielded Document has empty content (validator prevents this, but verify parser doesn't try)
    - `test_source_url_format`: Verify source_url follows expected format (e.g., starts with "flare://")

    **Unit tests (synthetic XML -- no file dependency):**
    - `test_extract_title_from_head`: Synthetic XHTML with `<head><title>Test</title></head>` -> title = "Test"
    - `test_extract_title_fallback_h1`: Synthetic XHTML with no `<title>` but `<h1>Hello</h1>` -> title = "Hello"
    - `test_keyword_tag_stripped`: Body with `<MadCap:keyword term="test"/>` produces no content for that tag
    - `test_xref_keeps_text`: Body with `<MadCap:xref href="foo.htm">Link Text</MadCap:xref>` produces "Link Text"
    - `test_code_table_extracted_as_code`: Table with class="Code_Table" containing pre/code -> fenced code block
    - `test_markdown_table_output`: Simple 2x2 table -> proper markdown with header, separator, data row
    - `test_heading_levels`: h1 through h4 -> markdown # through ####

    **Run and iterate:**
    ```bash
    cd rag-ingestion && uv run pytest tests/test_flare_parser.py -v
    ```
    If tests fail, fix flare.py (TDD green phase). Then run full suite including previous tests:
    ```bash
    cd rag-ingestion && uv run pytest -v
    ```
  </action>
  <verify>
    ```bash
    cd rag-ingestion
    uv run pytest tests/test_flare_parser.py -v
    uv run pytest -v  # full suite including Plan 01 tests
    uv run mypy src/bbj_rag/parsers/ tests/
    uv run ruff check src/ tests/
    ```
    All tests pass, no mypy errors, no ruff violations.
  </verify>
  <done>
    - All Flare parser tests pass (both real-file integration and synthetic unit tests)
    - Content contains no raw MadCap namespace tags
    - Code blocks preserved as markdown fenced blocks
    - Tables converted to markdown
    - TOC hierarchy paths present in metadata
    - Conditions correctly mapped to generations
    - Full test suite passes (including Plan 01 tests)
    - mypy strict and ruff pass on all files
  </done>
</task>

</tasks>

<verification>
Run full verification from `rag-ingestion/`:
```bash
uv run pytest -v
uv run mypy src/bbj_rag/parsers/
uv run ruff check src/ tests/
```

Additionally, run a quick sanity check on document count:
```bash
uv run python -c "
from pathlib import Path
from bbj_rag.parsers.flare import FlareParser
p = FlareParser(Path('/Users/beff/bbjdocs/Content'), Path('/Users/beff/bbjdocs/Project'))
count = sum(1 for _ in p.parse())
print(f'Total documents: {count}')
assert count > 4000
"
```
</verification>

<success_criteria>
1. FlareParser.parse() yields 5,000+ Document objects from real Flare project
2. No Document content contains raw "MadCap:" namespace tag strings
3. Code blocks (3,029 in corpus) preserved as markdown fenced blocks with language hints
4. API reference tables converted to readable markdown tables
5. Snippet content resolved and inlined (not missing gaps)
6. Every Document has a section_path in metadata (TOC or directory fallback)
7. Conditions mapped to generations (Primary.BASISHelp -> "bbj", etc.)
8. All tests pass, mypy strict passes, ruff passes
</success_criteria>

<output>
After completion, create `.planning/phases/10-flare-parser/10-02-SUMMARY.md`
</output>
