---
phase: 10-flare-parser
plan: 03
type: tdd
wave: 2
depends_on: ["10-01"]
files_modified:
  - rag-ingestion/src/bbj_rag/parsers/web_crawl.py
  - rag-ingestion/tests/test_web_crawl.py
autonomous: true

must_haves:
  truths:
    - "Web crawl parser extracts content from documentation.basis.cloud topic pages"
    - "Flare navigation chrome (sidebar, breadcrumbs, search bar, header/footer) is stripped -- only page content remains"
    - "Hierarchy paths derived from URL structure using arrow separator (e.g., 'bbjobjects > Window > bbjwindow')"
    - "Code blocks in crawled pages preserved as markdown fenced code blocks"
    - "Tables in crawled pages converted to markdown table format"
    - "All crawled documents default to generations=['bbj'] since condition metadata is unavailable from web"
    - "Crawl respects rate limits (1-2 requests per second) and robots.txt"
    - "Parser yields Documents via Iterator, processing pages one at a time"
  artifacts:
    - path: "rag-ingestion/src/bbj_rag/parsers/web_crawl.py"
      provides: "WebCrawlParser class implementing DocumentParser protocol"
      exports: ["WebCrawlParser"]
    - path: "rag-ingestion/tests/test_web_crawl.py"
      provides: "Tests for web crawl parser"
  key_links:
    - from: "rag-ingestion/src/bbj_rag/parsers/web_crawl.py"
      to: "rag-ingestion/src/bbj_rag/parsers/__init__.py"
      via: "DocumentParser protocol implementation"
      pattern: "from bbj_rag\\.parsers import DocumentParser"
    - from: "rag-ingestion/src/bbj_rag/parsers/web_crawl.py"
      to: "rag-ingestion/src/bbj_rag/models.py"
      via: "Document model as output type"
      pattern: "from bbj_rag\\.models import Document"
    - from: "rag-ingestion/src/bbj_rag/parsers/web_crawl.py"
      to: "httpx"
      via: "HTTP client for fetching pages"
      pattern: "import httpx"
    - from: "rag-ingestion/src/bbj_rag/parsers/web_crawl.py"
      to: "bs4"
      via: "BeautifulSoup for HTML parsing"
      pattern: "from bs4 import BeautifulSoup"
---

<objective>
Implement the web crawl fallback parser for documentation.basis.cloud. This parser provides an alternative ingestion path when engineers don't have access to the local Flare project files. It crawls the published documentation site, strips the Flare navigation chrome, and produces Document objects compatible with the same pipeline.

Purpose: Not all engineers will have MadCap Flare project access. The web crawl parser ensures the RAG pipeline can be populated from the publicly available documentation site, fulfilling Phase 10 Success Criterion #4.

Output: WebCrawlParser class that crawls documentation.basis.cloud and yields Document objects, with tests validating content extraction and chrome stripping.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-flare-parser/10-RESEARCH.md
@.planning/phases/10-flare-parser/10-01-SUMMARY.md
@rag-ingestion/src/bbj_rag/models.py
@rag-ingestion/src/bbj_rag/parsers/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement WebCrawlParser with httpx + BeautifulSoup for documentation.basis.cloud</name>
  <files>rag-ingestion/src/bbj_rag/parsers/web_crawl.py</files>
  <action>
    Create `web_crawl.py` implementing the `DocumentParser` protocol for crawling documentation.basis.cloud.

    **Class: WebCrawlParser**
    ```python
    class WebCrawlParser:
        def __init__(
            self,
            base_url: str = "https://documentation.basis.cloud/BASISHelp/WebHelp/",
            rate_limit: float = 0.5,  # seconds between requests
        ) -> None: ...
        def parse(self) -> Iterator[Document]: ...
    ```

    **Initialization:**
    - Store base_url (default: `https://documentation.basis.cloud/BASISHelp/WebHelp/`)
    - Store rate_limit for polite crawling
    - Create httpx.Client with reasonable timeout (30s), user-agent header identifying the crawler

    **parse() method -- crawl strategy:**
    1. **Discover pages:** Start from the base_url index page. Use BeautifulSoup to find all internal `<a href="...">` links pointing to .htm files within the same domain/path prefix. Build a set of unique URLs to visit.
    2. **Optionally check robots.txt** at `https://documentation.basis.cloud/robots.txt` -- respect Disallow rules if present. If robots.txt is unavailable, proceed with rate limiting.
    3. **Crawl loop:** For each discovered URL:
       a. Fetch page with `httpx.Client.get(url)` -- handle 404, 500, timeouts gracefully (log warning, skip)
       b. Parse HTML with `BeautifulSoup(response.text, "lxml")` (HTML parser, NOT XML -- these are rendered pages)
       c. Strip Flare navigation chrome (see below)
       d. Extract page content (see below)
       e. Derive hierarchy from URL path
       f. Yield Document if content is non-empty
       g. Sleep `rate_limit` seconds between requests
    4. **Follow internal links:** As pages are crawled, discover new internal links not yet in the visited set. Add them to the queue. This ensures full coverage even if the index page doesn't link everything directly.

    **Chrome stripping (remove Flare build artifacts):**
    The published site wraps each topic in navigation UI. Strip:
    - Sidebar/navigation panel: typically in a `<nav>`, `<div class="sidenav">`, or similar container
    - Top breadcrumb bar
    - Search bar / search results panel
    - Header and footer elements
    - "Previous / Next" navigation links
    - Flare-generated JavaScript and `<script>` tags
    - Approach: Look for the main content container. Common MadCap Flare output selectors:
      - `<div class="body-container">` or `<div class="topic-content">` or the `<body>` main content area
      - If a specific container is found, use it. Otherwise fall back to `<body>` and strip known chrome elements.
    - **Important:** Inspect the actual site structure during implementation. The research indicates path after `/BASISHelp/WebHelp/` mirrors Content/ directory. Look for the content div by examining a few sample pages.

    **Content extraction from crawled HTML:**
    Use BeautifulSoup (not lxml.etree -- these are rendered HTML, not XHTML):
    - **Headings** (`h1`-`h6`): Convert to markdown headings
    - **Paragraphs** (`p`): Extract text
    - **Code blocks** (`pre > code`): Preserve as fenced markdown code blocks with language hint from class
    - **Tables**: Convert to markdown table format (same logic as Flare parser but using BS4 API)
    - **Lists** (`ul`, `ol`): Convert to markdown list format
    - **Links** (`a`): Keep text only (strip href)
    - **Images**: Skip (not useful for RAG text)
    - Strip all remaining HTML tags, keeping only text content

    **Hierarchy from URL:**
    - `https://documentation.basis.cloud/BASISHelp/WebHelp/bbjobjects/Window/bbjwindow/bbjwindow_addbutton.htm`
    - Strip base URL prefix, strip `.htm` extension, split on `/`
    - Result path components: `["bbjobjects", "Window", "bbjwindow", "bbjwindow_addbutton"]`
    - Section path: everything except last component, joined with " > "
    - e.g., `"bbjobjects > Window > bbjwindow"`

    **Title extraction:**
    - From `<title>` tag (strip any " - BASISHelp" or similar suffix)
    - Fallback to first `<h1>` text
    - Fallback to last URL path component (cleaned up)

    **Document construction:**
    ```python
    Document(
        source_url=url,  # full URL
        title=extracted_title,
        doc_type="web_crawl",
        content=extracted_content,
        generations=["bbj"],  # default -- no condition metadata from web
        metadata={
            "section_path": hierarchy_from_url,
            "source": "web_crawl",
        },
    )
    ```

    **Error handling:**
    - HTTP errors (4xx, 5xx): log warning, skip page, continue crawling
    - Connection errors / timeouts: retry once with backoff, then skip
    - Encoding issues: let httpx handle (it detects encoding from headers/content)
    - Empty content after extraction: skip (don't yield empty Documents)
    - Infinite crawl loops: track visited URLs in a set, never revisit

    **Type annotations:**
    - Full mypy strict compliance
    - Use `from __future__ import annotations`
    - Type all methods, helpers, and data structures
  </action>
  <verify>
    ```bash
    cd rag-ingestion
    uv run python -c "from bbj_rag.parsers.web_crawl import WebCrawlParser; print('import OK')"
    uv run mypy src/bbj_rag/parsers/web_crawl.py
    uv run ruff check src/bbj_rag/parsers/web_crawl.py
    ```
    Note: Full crawl test deferred to Task 2 (requires network access and takes time).
  </verify>
  <done>
    - WebCrawlParser class created implementing DocumentParser protocol
    - Crawl logic uses httpx + BeautifulSoup
    - Flare navigation chrome stripping implemented
    - Content extraction with code block and table preservation
    - URL-based hierarchy derivation
    - Rate limiting and error handling
    - mypy strict and ruff pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Write tests for WebCrawlParser with mocked HTTP and optional live crawl test</name>
  <files>rag-ingestion/tests/test_web_crawl.py</files>
  <action>
    Write tests for WebCrawlParser. Most tests use mocked HTTP responses to avoid network dependency. One optional test does a limited live crawl.

    **Unit tests (mocked HTTP -- no network needed):**
    - `test_url_to_hierarchy_path`: Test URL path extraction:
      - `"https://documentation.basis.cloud/BASISHelp/WebHelp/bbjobjects/Window/bbjwindow/bbjwindow_addbutton.htm"` -> `"bbjobjects > Window > bbjwindow"`
    - `test_title_extraction_from_html`: Provide sample HTML with `<title>BBjWindow::addButton - BASISHelp</title>`, verify title extracted as "BBjWindow::addButton" (suffix stripped)
    - `test_chrome_stripping`: Provide sample HTML with nav sidebar, search bar, and content div. Verify only content div text ends up in output.
    - `test_code_block_preserved`: Sample HTML with `<pre><code class="language-bbj">REM test</code></pre>`, verify markdown fenced block in output
    - `test_table_to_markdown`: Sample HTML table, verify markdown table output
    - `test_empty_content_skipped`: Page with only navigation (no content) should not yield a Document
    - `test_default_generations`: All crawled documents should have `generations=["bbj"]`
    - `test_doc_type_is_web_crawl`: Verify doc_type is "web_crawl"

    To mock HTTP, create a helper that provides fake HTML responses. Use `unittest.mock.patch` to replace `httpx.Client.get` with the mock. Alternatively, test the content extraction functions directly (extract them as standalone functions that take HTML strings).

    **Live crawl test (optional, slow):**
    Mark with `@pytest.mark.skipif` and `@pytest.mark.slow`:
    ```python
    @pytest.mark.slow
    @pytest.mark.skipif(
        os.environ.get("RUN_SLOW_TESTS") != "1",
        reason="Slow test -- set RUN_SLOW_TESTS=1 to run"
    )
    def test_live_crawl_limited():
        """Crawl a small number of pages from the live site."""
        parser = WebCrawlParser(rate_limit=1.0)
        docs = []
        for doc in parser.parse():
            docs.append(doc)
            if len(docs) >= 5:
                break
        assert len(docs) >= 1
        for doc in docs:
            assert doc.content.strip()
            assert doc.generations == ["bbj"]
            assert doc.doc_type == "web_crawl"
    ```

    **Run tests:**
    ```bash
    cd rag-ingestion && uv run pytest tests/test_web_crawl.py -v
    ```
    If tests fail, fix web_crawl.py implementation. Then run full suite:
    ```bash
    cd rag-ingestion && uv run pytest -v --ignore=tests/test_web_crawl.py -k "not slow" && uv run pytest tests/test_web_crawl.py -v -k "not slow"
    ```
  </action>
  <verify>
    ```bash
    cd rag-ingestion
    uv run pytest tests/test_web_crawl.py -v -k "not slow"
    uv run pytest -v -k "not slow"  # full suite
    uv run mypy src/bbj_rag/parsers/ tests/
    uv run ruff check src/ tests/
    ```
    All non-slow tests pass, mypy strict passes, ruff passes.
  </verify>
  <done>
    - All unit tests pass for WebCrawlParser (mocked HTTP)
    - Chrome stripping verified against sample HTML
    - Code blocks and tables preserved in output
    - URL-to-hierarchy conversion tested
    - Default generations and doc_type correct
    - Full test suite passes (Plans 01, 02, 03)
    - mypy strict and ruff pass on all files
  </done>
</task>

</tasks>

<verification>
Run full verification from `rag-ingestion/`:
```bash
uv run pytest -v -k "not slow"
uv run mypy src/bbj_rag/parsers/
uv run ruff check src/ tests/
```
All tests pass. WebCrawlParser can be instantiated and its content extraction logic works correctly on sample HTML.
</verification>

<success_criteria>
1. WebCrawlParser implements DocumentParser protocol
2. Content extraction strips Flare navigation chrome correctly
3. Code blocks preserved as markdown fenced blocks in crawled content
4. Tables converted to markdown in crawled content
5. URL-based hierarchy derivation produces arrow-separated paths
6. All crawled documents default to generations=["bbj"]
7. Rate limiting implemented (configurable delay between requests)
8. All tests pass (mocked HTTP), mypy strict passes, ruff passes
</success_criteria>

<output>
After completion, create `.planning/phases/10-flare-parser/10-03-SUMMARY.md`
</output>
