---
phase: 21-data-configuration-ingestion
plan: 02
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - rag-ingestion/src/bbj_rag/ingest_all.py
  - rag-ingestion/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "A single command (bbj-ingest-all) triggers ingestion of all enabled sources in sequence"
    - "The script validates all source paths, database connectivity, and embedding model before starting any ingestion"
    - "Each source runs through the existing run_pipeline() with correct parser, embedder, and a fresh connection opened/closed per source via try/finally"
    - "A --clean flag wipes existing chunks for a source before re-ingesting"
    - "A --resume flag skips sources completed in a previous interrupted run"
    - "A final summary table shows per-source stats (files, chunks, duration, status)"
    - "Failed sources do not stop remaining sources from running"
    - "After full ingestion, all 3 MDX directories produce chunks with distinct source_url prefixes (mdx-dwc://, mdx-beginner://, mdx-db-modernization://)"
  artifacts:
    - path: "rag-ingestion/src/bbj_rag/ingest_all.py"
      provides: "Standalone orchestration CLI for all-source ingestion"
      exports: ["ingest_all"]
    - path: "rag-ingestion/pyproject.toml"
      provides: "bbj-ingest-all script entry point"
      contains: "bbj-ingest-all"
  key_links:
    - from: "rag-ingestion/src/bbj_rag/ingest_all.py"
      to: "rag-ingestion/src/bbj_rag/source_config.py"
      via: "load_sources_config() + validate_sources() for config loading and fail-fast"
      pattern: "load_sources_config"
    - from: "rag-ingestion/src/bbj_rag/ingest_all.py"
      to: "rag-ingestion/src/bbj_rag/pipeline.py"
      via: "run_pipeline() called per source with parser + embedder + conn"
      pattern: "run_pipeline"
    - from: "rag-ingestion/src/bbj_rag/ingest_all.py"
      to: "rag-ingestion/src/bbj_rag/parsers/mdx.py"
      via: "MdxParser instantiated with source_prefix=source.name"
      pattern: "MdxParser.*source_prefix"
    - from: "rag-ingestion/src/bbj_rag/ingest_all.py"
      to: "rag-ingestion/src/bbj_rag/db.py"
      via: "Fresh connection per source with try/finally close: conn = get_connection_from_settings(settings) ... finally: conn.close()"
      pattern: "finally.*conn\\.close"
---

<objective>
Build the standalone ingestion orchestration CLI that reads `sources.toml`, validates all paths and dependencies, runs the existing `run_pipeline()` for each enabled source, tracks progress for resume capability, and prints a summary table.

Purpose: This is the single-command entry point that satisfies the phase's core requirement -- one command to ingest all 6 sources into pgvector. It bridges the config infrastructure from Plan 21-01 with the existing pipeline machinery.

Output: `ingest_all.py` CLI script registered as `bbj-ingest-all` in pyproject.toml.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-data-configuration-ingestion/21-RESEARCH.md
@.planning/phases/21-data-configuration-ingestion/21-01-SUMMARY.md

@rag-ingestion/src/bbj_rag/source_config.py
@rag-ingestion/src/bbj_rag/pipeline.py
@rag-ingestion/src/bbj_rag/cli.py
@rag-ingestion/src/bbj_rag/config.py
@rag-ingestion/src/bbj_rag/db.py
@rag-ingestion/src/bbj_rag/embedder.py
@rag-ingestion/src/bbj_rag/parsers/__init__.py
@rag-ingestion/src/bbj_rag/parsers/mdx.py
@rag-ingestion/src/bbj_rag/parsers/flare.py
@rag-ingestion/src/bbj_rag/parsers/bbj_source.py
@rag-ingestion/src/bbj_rag/parsers/wordpress.py
@rag-ingestion/src/bbj_rag/parsers/web_crawl.py
@rag-ingestion/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ingest_all.py orchestration CLI</name>
  <files>
    rag-ingestion/src/bbj_rag/ingest_all.py
  </files>
  <action>
    Create `rag-ingestion/src/bbj_rag/ingest_all.py` as a standalone Click CLI with the following structure:

    **CLI Options:**
    ```python
    @click.command()
    @click.option("--config", default="sources.toml", type=click.Path(exists=True), help="Path to sources.toml config file")
    @click.option("--clean", is_flag=True, help="Wipe existing chunks before re-ingesting each source")
    @click.option("--resume", is_flag=True, help="Skip sources completed in a previous interrupted run")
    @click.option("--parallel", is_flag=True, help="Run file-based sources in parallel (experimental)")
    @click.option("--verbose", "-v", is_flag=True, help="Per-file progress output")
    @click.option("--source", "source_names", multiple=True, help="Run only named sources (repeatable, e.g., --source flare --source pdf)")
    @click.option("--data-dir", "data_dir_override", default=None, type=click.Path(exists=True), help="Override data_dir from config")
    def ingest_all(config, clean, resume, parallel, verbose, source_names, data_dir_override):
    ```

    **Orchestration Flow:**

    1. **Configure logging:** `logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")`

    2. **Load config:** Call `load_sources_config(Path(config))`. Call `resolve_data_dir(cfg)` (with `data_dir_override` taking precedence if provided via CLI -- set `DATA_DIR` env var before calling resolve if override provided).

    3. **Filter sources:**
       - Start with `cfg.sources` filtered to `enabled=True`
       - If `source_names` provided, further filter to only those names. Error if any requested name doesn't exist in config.

    4. **Fail-fast validation:**
       - Call `validate_sources(enabled_sources, data_dir)`. If errors, print all and `sys.exit(1)`.
       - Load `Settings()` and try `get_connection_from_settings(settings)`. If fails, print DB connection error and exit.
       - Try `create_embedder(settings)` and run a single test embedding: `embedder.embed_batch(["test"])`. If fails, print Ollama error with `ollama pull {settings.embedding_model}` hint and exit.
       - Print: `"Validation OK: {N} sources, database connected, embedder ready"`

    5. **Resume state:** If `--resume`, load `.ingestion-state.json` via `_load_resume_state()`. Skip sources in `completed_sources`. If not `--resume`, clear any existing state file.

    6. **Run ingestion loop:**
       For each source (sequential by default):
       ```
       [i/N] source_name ...
       ```

       a. If `--clean`, call `clean_source_chunks(conn, prefix)` where prefix comes from `get_source_url_prefix(source)`. Print deleted count.

       b. Create parser via `_create_parser_for_source(source, data_dir, settings)` (see parser factory below).

       c. Call `run_pipeline(parser=parser, embedder=embedder, conn=conn, batch_size=settings.embedding_batch_size, resume=False, max_tokens=settings.chunk_size, overlap_tokens=settings.chunk_overlap)`.

       d. Record stats and duration. Print per-source summary line.

       e. On success, save source name to resume state `completed_sources`.

       f. On exception, record failure, print error, continue to next source.

       g. Connection is closed in the `finally` block (see "Important implementation details" below). No keepalive needed since each source gets a fresh connection.

    7. **Print summary table:** After all sources complete, print a formatted table:
       ```
       ======================================================================
         Source                     Files   Chunks   Duration   Status
       ----------------------------------------------------------------------
         flare                      1247     4521      45.2s       OK
         pdf                          89      312       8.1s       OK
         ...
       ======================================================================
         Total: 1500 docs, 5200 chunks, 120.5s
         Failures: web-crawl
       ```

    8. **Exit code:** `sys.exit(1)` if any source failed, `sys.exit(0)` if all succeeded.

    **Parser Factory** (`_create_parser_for_source`):
    Create a private function that maps source config to parser instances. There are 7 parser types serving 9 source entries (the "mdx" parser is used 3 times with different source_prefix values, and WordPress has 2 distinct parser types):

    - `"flare"` -> `FlareParser(content_dir=data_dir / paths[0] / "Content", project_dir=data_dir / paths[0])`
    - `"pdf"` -> `PdfParser(pdf_path=data_dir / paths[0])`
    - `"mdx"` -> `MdxParser(docs_dir=data_dir / paths[0], source_prefix=source.name)`  **Critical:** Pass `source.name` as `source_prefix` for unique URLs.
    - `"bbj-source"` -> `BbjSourceParser(source_dirs=[data_dir / p for p in source.paths])`
    - `"wordpress-advantage"` -> `AdvantageParser(index_url=settings.advantage_index_url)`
    - `"wordpress-kb"` -> `KnowledgeBaseParser(index_url=settings.kb_index_url)`
    - `"web-crawl"` -> `WebCrawlParser()`

    Use lazy imports (import inside the function branches) to avoid loading unused parser dependencies.

    **Clean Function** (`_clean_source_chunks`):
    ```python
    def _clean_source_chunks(conn, prefix: str) -> int:
        with conn.cursor() as cur:
            cur.execute("DELETE FROM chunks WHERE source_url LIKE %s", (f"{prefix}%",))
            count = cur.rowcount
        conn.commit()
        return count
    ```

    **Resume State Functions:**
    - `_load_resume_state(state_file: Path) -> dict` -- loads JSON, returns `{"completed_sources": [], "failed_sources": {}}` if file doesn't exist.
    - `_save_resume_state(state_file: Path, state: dict) -> None` -- writes JSON with indent=2.
    - State file path: `Path(".ingestion-state.json")`

    **Summary Table Function** (`_print_summary_table`):
    Formatted output as shown above. Use `click.echo()` for all output.

    **Parallel Mode (--parallel):**
    When `--parallel` is passed, split sources into two groups:
    - File-based: parsers in `{"flare", "pdf", "mdx", "bbj-source"}` -- run with `ThreadPoolExecutor`
    - URL-based: parsers in `{"wordpress-advantage", "wordpress-kb", "web-crawl"}` -- run sequentially after file-based complete
    For the initial implementation, focus on correctness of sequential mode. Parallel is a stretch goal -- if implementation feels complex, just print a warning that parallel is not yet implemented and fall back to sequential.

    **Important implementation details:**
    - Open a FRESH database connection per source (not one shared connection) to avoid timeout issues during long runs. Use a try/finally pattern to guarantee closure:
      ```python
      for source in sources:
          conn = get_connection_from_settings(settings)
          try:
              # ... parser creation, run_pipeline, etc.
          except Exception as exc:
              # ... record failure, continue
          finally:
              conn.close()
      ```
      Each source gets its own connection opened before pipeline runs and closed in the `finally` block regardless of success or failure. This prevents connection leaks on exception and avoids idle timeout issues during long runs.
    - The embedder can be shared across sources (it's stateless).
    - All output goes to stdout via `click.echo()` -- no log files.

    Set `__all__ = ["ingest_all"]`.
  </action>
  <verify>
    ```bash
    cd rag-ingestion
    # Verify the module imports cleanly
    python -c "from bbj_rag.ingest_all import ingest_all; print('ingest_all.py imports OK')"
    # Verify --help works
    python -m bbj_rag.ingest_all --help 2>&1 | head -20
    ```
  </verify>
  <done>
    ingest_all.py exists with Click CLI, parser factory for all 7 parser types serving 9 source entries (MDX used 3 times with different prefixes), fail-fast validation (paths + DB + embedder), clean/resume/parallel/verbose/source flags, per-source pipeline execution with try/finally connection lifecycle per source, continue-on-failure, resume state tracking via JSON, and formatted summary table output.
  </done>
</task>

<task type="auto">
  <name>Task 2: Register bbj-ingest-all script entry point and verify CLI</name>
  <files>
    rag-ingestion/pyproject.toml
  </files>
  <action>
    **Update `rag-ingestion/pyproject.toml`:**

    Add a new entry to `[project.scripts]`:
    ```toml
    [project.scripts]
    bbj-rag = "bbj_rag.cli:cli"
    bbj-ingest-all = "bbj_rag.ingest_all:ingest_all"
    ```

    This registers `bbj-ingest-all` as a standalone CLI command separate from the existing `bbj-rag` CLI.

    **After editing pyproject.toml**, reinstall the package in development mode:
    ```bash
    cd rag-ingestion && uv pip install -e .
    ```

    **Verify the CLI is accessible:**
    ```bash
    bbj-ingest-all --help
    ```

    The help output should show all options: --config, --clean, --resume, --parallel, --verbose, --source, --data-dir.

    **Also add the `if __name__ == "__main__":` block** to `ingest_all.py` so it can be run directly:
    ```python
    if __name__ == "__main__":
        ingest_all()
    ```
    (This should be added in Task 1 but verify it exists here.)
  </action>
  <verify>
    ```bash
    cd rag-ingestion
    # Reinstall package
    uv pip install -e . 2>&1 | tail -5
    # Verify CLI entry point
    bbj-ingest-all --help 2>&1
    # Verify existing CLI still works
    bbj-rag --help 2>&1
    # Verify fail-fast validation catches missing DB (should fail gracefully)
    bbj-ingest-all --config sources.toml 2>&1 | head -10
    ```
  </verify>
  <done>
    pyproject.toml has bbj-ingest-all script entry, the command is accessible after uv pip install -e ., --help shows all expected options, and the existing bbj-rag CLI is unaffected.
  </done>
</task>

</tasks>

<verification>
Full verification sequence:

1. **Import check:** `python -c "from bbj_rag.ingest_all import ingest_all; print('OK')"`

2. **CLI help:** `bbj-ingest-all --help` shows all options.

3. **Fail-fast validation with real config:** `bbj-ingest-all --config sources.toml` should either:
   - Fail at DB connection (if Docker not running) with a clean error message
   - Fail at embedder (if Ollama not running) with a clean error message
   - Or proceed to ingest if infrastructure is up

4. **Single source test:** `bbj-ingest-all --config sources.toml --source pdf` should ingest only the PDF source (good for quick testing).

5. **MDX directory ingestion verification (DATA-02):** After running ingestion for all 3 MDX sources, verify database chunks exist with correct source_url prefixes:
   ```bash
   cd rag-ingestion
   python -c "
   from bbj_rag.config import Settings
   from bbj_rag.db import get_connection_from_settings
   settings = Settings()
   conn = get_connection_from_settings(settings)
   cur = conn.cursor()
   for prefix in ['mdx-dwc://', 'mdx-beginner://', 'mdx-db-modernization://']:
       cur.execute('SELECT COUNT(*) FROM chunks WHERE source_url LIKE %s', (f'{prefix}%',))
       count = cur.fetchone()[0]
       status = 'OK' if count > 0 else 'MISSING'
       print(f'  {prefix}: {count} chunks [{status}]')
   cur.close()
   conn.close()
   "
   ```
   All 3 MDX prefixes must have >0 chunks after a full ingestion run.

6. **Existing tests pass:** `cd rag-ingestion && python -m pytest tests/ -v 2>&1 | tail -10`

7. **Ruff lint:** `cd rag-ingestion && python -m ruff check src/bbj_rag/ingest_all.py src/bbj_rag/source_config.py`
</verification>

<success_criteria>
1. `bbj-ingest-all` CLI is installed and shows help with all expected options
2. Fail-fast validation catches missing DB, missing Ollama, and missing source paths before any ingestion starts
3. Parser factory correctly instantiates all 7 parser types from 9 source config entries (MDX used 3 times with different prefixes)
4. MdxParser receives `source.name` as `source_prefix` for unique source_url generation
5. --clean deletes existing chunks by source_url prefix before re-ingesting
6. --resume skips completed sources from .ingestion-state.json
7. Failed sources don't block remaining sources; all failures reported at end
8. Summary table prints per-source stats (files, chunks, duration, status)
9. Exit code is 1 if any source failed, 0 if all succeeded
10. Existing bbj-rag CLI and all tests remain unaffected
11. After full ingestion, database contains chunks with source_urls matching mdx-dwc://, mdx-beginner://, and mdx-db-modernization:// prefixes
</success_criteria>

<output>
After completion, create `.planning/phases/21-data-configuration-ingestion/21-02-SUMMARY.md`
</output>
