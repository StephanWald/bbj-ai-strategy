---
phase: 24-end-to-end-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - rag-ingestion/scripts/validate_e2e.py
  - rag-ingestion/VALIDATION.md
autonomous: true

must_haves:
  truths:
    - "REST API query about a known BBj topic returns relevant documentation chunks from the real corpus"
    - "MCP tool invocation returns relevant BBj documentation via search_bbj_knowledge"
    - "Results across the full query set come from multiple different source parsers (not just one)"
    - "All 6 logical sources have chunks in the database (verified by source-targeted queries returning results)"
    - "VALIDATION.md report exists with per-query pass/fail, scores, and content snippets"
  artifacts:
    - path: "rag-ingestion/scripts/validate_e2e.py"
      provides: "End-to-end validation script with prerequisite checks, REST queries, MCP queries, and report generation"
      min_lines: 200
    - path: "rag-ingestion/VALIDATION.md"
      provides: "Human-readable validation report with corpus stats, per-query results, and cross-source summary"
      contains: "End-to-End Validation Report"
  key_links:
    - from: "rag-ingestion/scripts/validate_e2e.py"
      to: "http://localhost:10800/search"
      via: "httpx.AsyncClient POST"
      pattern: "client\\.post.*\\/search"
    - from: "rag-ingestion/scripts/validate_e2e.py"
      to: "search_bbj_knowledge"
      via: "ClientSession.call_tool()"
      pattern: "session\\.call_tool.*search_bbj_knowledge"
    - from: "rag-ingestion/scripts/validate_e2e.py"
      to: "rag-ingestion/VALIDATION.md"
      via: "file write"
      pattern: "VALIDATION\\.md.*write"
---

<objective>
Write and execute an end-to-end validation script that proves the full RAG pipeline works: user queries enter through both the REST API and the MCP server, hit real ingested BBj documentation across all 6 sources, and return relevant results. Produce a human-readable VALIDATION.md report.

Purpose: This is the final phase of v1.4. The system must be proven to work before the milestone can be closed. The report serves as visible proof that Docker + pgvector + Ollama + REST API + MCP server deliver real BBj documentation retrieval.

Output: `rag-ingestion/scripts/validate_e2e.py` (validation script) and `rag-ingestion/VALIDATION.md` (generated report).
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-end-to-end-validation/24-RESEARCH.md
@.planning/phases/24-end-to-end-validation/24-CONTEXT.md
@rag-ingestion/src/bbj_rag/api/schemas.py
@rag-ingestion/src/bbj_rag/mcp_server.py
@rag-ingestion/src/bbj_rag/source_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end validation script</name>
  <files>rag-ingestion/scripts/validate_e2e.py</files>
  <action>
Create `rag-ingestion/scripts/validate_e2e.py` -- a standalone async Python script that validates the complete RAG pipeline and generates a Markdown report.

**Structure the script with these sections:**

1. **Constants and configuration:**
   - `API_BASE = "http://localhost:10800"` (match existing convention)
   - `RAG_DIR` = path to rag-ingestion directory (resolved from script location)
   - Define the 6 logical source groups with expected `source_url` prefixes:
     - Flare: `flare://`
     - PDF: `pdf://`
     - MDX: `mdx-` (covers mdx-dwc://, mdx-beginner://, mdx-db-modernization://)
     - BBj Source: `file://`
     - WordPress: `https://basis.cloud/`
     - Web Crawl: `https://documentation.basis.cloud/`

2. **Query definitions (two categories):**

   Source-targeted queries (6 queries, one per logical source, with expected_prefix for validation):
   - Flare: "BBjWindow addButton method" -> flare://
   - PDF: "customer information program BBj GUI example" -> pdf://
   - MDX: "DWC web component tutorial getting started" -> mdx-
   - BBj Source: "BBj sample source code PROCESS_EVENTS" -> file://
   - WordPress: "Advantage magazine BBj article" -> https://basis.cloud/
   - Web Crawl: "documentation.basis.cloud BBj reference" -> https://documentation.basis.cloud/

   Topic-based queries (7 queries, keywords list for heuristic validation):
   - "How do I create a BBjGrid?" -> keywords: ["grid", "BBjGrid"]
   - "BBj string manipulation functions" -> keywords: ["string", "str"]
   - "DWC web component styling CSS" -> keywords: ["css", "style", "dwc"]
   - "BBj database connection SQL" -> keywords: ["sql", "database", "connection"]
   - "Event handling callbacks in BBj" -> keywords: ["event", "callback", "notify"]
   - "BBj file I/O operations" -> keywords: ["file", "open", "read", "write"]
   - "Migration from Visual PRO/5 to BBj" -> keywords: ["migration", "pro/5", "vpro5", "visual pro"]

   MCP validation subset (3 queries selected from above):
   - "BBjWindow addButton method" (source-targeted: Flare)
   - "How do I create a BBjGrid?" (topic-based)
   - "Event handling callbacks in BBj" (topic-based)

   Optional generation-filtered query (1 extra):
   - "DWC web component styling CSS" with generation="dwc" (topic-based + generation filter)

3. **Prerequisites check function** (`check_prerequisites`):
   - GET `{API_BASE}/health` -- fail with clear message if not 200 or status != "healthy"
   - GET `{API_BASE}/stats` -- capture total_chunks and by_source for the report
   - If total_chunks == 0, fail: "No chunks in database -- run ingestion first"
   - Return stats dict for report use

4. **REST API query runner** (`run_rest_query`):
   - POST to `{API_BASE}/search` with `{"query": query, "limit": 5}` (plus optional `generation` field)
   - Timeout of 30 seconds per query
   - Return parsed JSON response
   - Handle httpx errors gracefully (return error dict, don't crash)

5. **REST result evaluator** (`evaluate_rest_result`):
   - For source-targeted queries: PASS if results > 0 AND top-1 result source_url starts with expected prefix
   - For topic-based queries: PASS if results > 0 AND top-1 result title+content contains at least one keyword (case-insensitive)
   - Record: query, passed (bool), reason (str), result_count, top result's source_url/title/score, content snippet (~150 chars via `textwrap.shorten`)

6. **MCP query runner** (`run_mcp_queries`):
   - Use pattern from RESEARCH.md: spawn ONE `stdio_client` + `ClientSession` for all MCP queries
   - `StdioServerParameters(command="uv", args=["--directory", str(RAG_DIR), "run", "bbj-mcp"], env={...})`
   - Env dict: merge `os.environ` with explicit `BBJ_RAG_API_URL=API_BASE` (research recommends explicit env)
   - Call `session.initialize()`, then `session.list_tools()` to verify `search_bbj_knowledge` is available
   - For each MCP query: `session.call_tool("search_bbj_knowledge", arguments={"query": query, "limit": 5})`
   - PASS if `result.isError` is False AND response text doesn't contain "No results found"
   - Record: query, passed, response_preview (first 200 chars), is_error

7. **Cross-source collector:**
   - After all REST queries complete, collect unique source_url prefixes from ALL results (not just top-1) across ALL queries
   - Map each result's source_url to one of the 6 logical source groups
   - Build a set of which logical sources appeared in results

8. **Report generator** (`generate_report`):
   - Output structure matching CONTEXT.md decisions:
     ```
     # End-to-End Validation Report
     **Generated:** YYYY-MM-DD HH:MM UTC
     **Status:** PASS / FAIL (N/M queries passed)

     ## Corpus Stats
     | Metric | Value |
     (total_chunks, by_source breakdown from /stats, by_generation from /stats)

     ## Source-Targeted Queries
     ### Query 1: [query text]
     - **Result:** PASS / FAIL
     - **Reason:** [why]
     - **Source:** [source_url of top-1]
     - **Title:** [title of top-1]
     - **Score:** [rrf_score]
     - **Snippet:** [~150 chars]

     ## Topic-Based Queries
     (same per-query format, no expected_prefix)

     ## MCP Validation
     ### Query M1: [query text]
     - **Result:** PASS / FAIL
     - **Response preview:** [first 200 chars]

     ## Cross-Source Summary
     | Source Group | Expected Prefix | Found in Results |
     (6 rows, Yes/No for each)

     ## Known Issues
     (any queries with FAIL or questionable results)
     ```

9. **Main function** (async, use `anyio.run` or `asyncio.run`):
   - Print banner: "BBj RAG End-to-End Validation"
   - Run prerequisites check
   - Run all REST queries (source-targeted, then topic-based, then generation-filtered)
   - Evaluate each REST result
   - Run MCP queries
   - Collect cross-source data
   - Generate report string
   - Write to `{RAG_DIR}/VALIDATION.md`
   - Print summary to stdout: total pass/fail counts and output file path

**Important implementation details:**
- Use `from __future__ import annotations` at the top
- Use `#!/usr/bin/env python3` shebang
- Import from `mcp.client.stdio` (stdio_client, StdioServerParameters) and `mcp.client.session` (ClientSession) -- these are already installed via `mcp[cli]>=1.25,<2`
- Use `textwrap.shorten(content, 150, placeholder="...")` for snippets
- Use `datetime.now(timezone.utc)` for timestamps
- Do NOT connect directly to the database -- all queries go through REST API or MCP
- Do NOT use pytest assertions -- this is a report generator, not a test suite
- Use `asyncio.run(main())` in `if __name__ == "__main__":` block
- The script should be runnable via `uv run python scripts/validate_e2e.py` from the rag-ingestion directory
  </action>
  <verify>
    - File exists at `rag-ingestion/scripts/validate_e2e.py`
    - `python -c "import ast; ast.parse(open('rag-ingestion/scripts/validate_e2e.py').read())"` succeeds (valid Python syntax)
    - Script imports resolve: `uv run python -c "from mcp.client.stdio import stdio_client, StdioServerParameters; from mcp.client.session import ClientSession; import httpx; print('OK')"` from the rag-ingestion directory
  </verify>
  <done>
    validate_e2e.py exists with prerequisite checks, 6 source-targeted queries, 7 topic-based queries, 1 generation-filtered query, 3 MCP queries, cross-source collection, and Markdown report generation. Valid Python that imports successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute validation and generate report</name>
  <files>rag-ingestion/VALIDATION.md</files>
  <action>
Run the validation script against the live system and verify the output.

**Pre-flight checks (before running the script):**
1. Verify Docker containers are running: `docker compose ps` from rag-ingestion/ -- both bbj-rag-app and bbj-rag-db should be Up
2. Verify Ollama is running: `curl -s http://localhost:11434/api/tags | head -c 200` should return JSON with models
3. If containers are not running: `docker compose up -d` and wait for health

**Execute the validation:**
```bash
cd rag-ingestion && uv run python scripts/validate_e2e.py
```

**Verify the output:**
1. `VALIDATION.md` exists in `rag-ingestion/` directory
2. Report contains "End-to-End Validation Report" header
3. Corpus Stats section shows total_chunks > 0 (expect ~50,000 based on 23.1 summary)
4. Source-Targeted Queries section has 6 query results
5. Topic-Based Queries section has 7+ query results
6. MCP Validation section has 3 query results
7. Cross-Source Summary shows all 6 source groups found

**Handle failures:**
- If prerequisites fail: ensure Docker is running with `docker compose up -d`, wait 30s for health, retry
- If REST queries fail: check `curl http://localhost:10800/health` manually
- If MCP queries fail: check `uv run bbj-mcp` can start (run and Ctrl-C); check that `BBJ_RAG_API_URL` reaches the API
- If some queries return FAIL but the script completes: that's expected for edge cases. Document in Known Issues section. Quality tuning is out of scope.
- If source-targeted queries for a specific source all fail: that source may have zero chunks. Check `/stats` and reference 23.1-04 summary (50,392 total chunks expected).

**If the script needs fixes:**
Fix issues in validate_e2e.py and re-run until VALIDATION.md is generated successfully. Common fixes:
- MCP env inheritance: make sure `os.environ` is spread into StdioServerParameters env
- Timeout on first query (Ollama cold start): increase timeout or add warm-up call
- Import paths: verify `from mcp.client.stdio import ...` matches installed SDK
  </action>
  <verify>
    - `rag-ingestion/VALIDATION.md` exists and is non-empty
    - `grep -c "^### Query" rag-ingestion/VALIDATION.md` returns at least 14 (6 source + 7 topic + 1 generation + some MCP headers, accounting for format variations)
    - `grep "Total chunks" rag-ingestion/VALIDATION.md` shows a non-zero value
    - `grep "Cross-Source Summary" rag-ingestion/VALIDATION.md` finds the section
    - Status line in report shows the pass/fail ratio
  </verify>
  <done>
    VALIDATION.md exists with per-query results for REST API (source-targeted + topic-based + generation-filtered) and MCP queries. Corpus stats show ~50,000 chunks. Cross-source summary confirms results from multiple parser types. Any FAIL queries are documented as known issues.
  </done>
</task>

</tasks>

<verification>
1. `rag-ingestion/scripts/validate_e2e.py` exists and is syntactically valid Python
2. `rag-ingestion/VALIDATION.md` exists with a complete validation report
3. REST API queries return relevant results (majority pass)
4. MCP tool invocation returns formatted documentation text (majority pass)
5. Cross-source summary shows results from at least 4 of 6 source groups (ideally all 6)
6. Report includes RRF scores alongside results for future tuning baseline
</verification>

<success_criteria>
- VALIDATION.md demonstrates that the complete RAG pipeline works end-to-end
- Source-targeted queries verify all 6 logical sources have searchable content
- Topic-based queries demonstrate hybrid search returns topically relevant BBj documentation
- MCP validation proves search_bbj_knowledge tool works programmatically
- The report is human-readable proof that v1.4 milestone goals are met
</success_criteria>

<output>
After completion, create `.planning/phases/24-end-to-end-validation/24-01-SUMMARY.md`
</output>
