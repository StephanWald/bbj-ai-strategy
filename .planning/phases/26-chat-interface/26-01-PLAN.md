---
phase: 26-chat-interface
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - rag-ingestion/pyproject.toml
  - rag-ingestion/src/bbj_rag/config.py
  - rag-ingestion/src/bbj_rag/chat/__init__.py
  - rag-ingestion/src/bbj_rag/chat/prompt.py
  - rag-ingestion/src/bbj_rag/chat/stream.py
  - rag-ingestion/src/bbj_rag/api/chat.py
  - rag-ingestion/src/bbj_rag/app.py
autonomous: true
user_setup:
  - service: anthropic
    why: "Claude API for answer generation"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "console.anthropic.com -> API Keys -> Create Key"

must_haves:
  truths:
    - "POST /chat/stream with a messages array returns an SSE stream with event types: sources, delta, done, error"
    - "The system prompt includes RAG search results as numbered source blocks with display_url and source_type"
    - "Chat model is configurable via BBJ_RAG_CHAT_MODEL env var, defaulting to claude-sonnet-4-5-20250514"
    - "SSE delta events contain JSON-encoded text payloads (newlines safely escaped)"
    - "A sources SSE event is emitted before streaming begins, containing title, url, and source_type for each RAG result"
    - "A low_confidence flag is sent when fewer than 2 results or top score below 0.3"
  artifacts:
    - path: "rag-ingestion/src/bbj_rag/chat/__init__.py"
      provides: "Chat module package"
    - path: "rag-ingestion/src/bbj_rag/chat/prompt.py"
      provides: "System prompt construction with RAG context and citation instructions"
      exports: ["build_rag_system_prompt"]
    - path: "rag-ingestion/src/bbj_rag/chat/stream.py"
      provides: "Anthropic SDK streaming generator yielding SSE event dicts"
      exports: ["stream_chat_response"]
    - path: "rag-ingestion/src/bbj_rag/api/chat.py"
      provides: "Chat API routes: GET /chat (page), POST /chat/stream (SSE)"
      exports: ["router"]
  key_links:
    - from: "rag-ingestion/src/bbj_rag/api/chat.py"
      to: "rag-ingestion/src/bbj_rag/chat/stream.py"
      via: "stream_chat_response() called from chat_stream endpoint"
      pattern: "stream_chat_response"
    - from: "rag-ingestion/src/bbj_rag/chat/stream.py"
      to: "rag-ingestion/src/bbj_rag/chat/prompt.py"
      via: "build_rag_system_prompt() called before Claude API call"
      pattern: "build_rag_system_prompt"
    - from: "rag-ingestion/src/bbj_rag/app.py"
      to: "rag-ingestion/src/bbj_rag/api/chat.py"
      via: "app.include_router(chat_router)"
      pattern: "include_router.*chat"
---

<objective>
Create the backend chat infrastructure: Python dependencies, chat module with prompt construction and Claude API streaming, and the FastAPI chat endpoint that orchestrates RAG search + Claude response streaming via SSE.

Purpose: This is the backbone of the chat feature. The SSE streaming endpoint is the contract that the frontend will consume. Getting JSON-encoded SSE events, RAG-grounded system prompts, and source metadata emission right here means the frontend plan can focus purely on rendering.

Output: Working POST /chat/stream endpoint that accepts a messages array, runs RAG search, streams Claude's response as SSE events with JSON-encoded payloads, and emits source citation metadata.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/26-chat-interface/26-RESEARCH.md
@.planning/phases/26-chat-interface/26-CONTEXT.md
@rag-ingestion/src/bbj_rag/app.py
@rag-ingestion/src/bbj_rag/api/routes.py
@rag-ingestion/src/bbj_rag/api/deps.py
@rag-ingestion/src/bbj_rag/api/schemas.py
@rag-ingestion/src/bbj_rag/search.py
@rag-ingestion/src/bbj_rag/config.py
@rag-ingestion/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies and chat config settings</name>
  <files>
    rag-ingestion/pyproject.toml
    rag-ingestion/src/bbj_rag/config.py
  </files>
  <action>
    1. Add `anthropic` and `sse-starlette` to pyproject.toml dependencies:
       - `"anthropic>=0.77,<1"` — Claude API SDK with async streaming support
       - `"sse-starlette>=3.2,<4"` — W3C-compliant SSE responses for FastAPI
       - `"jinja2>=3.1,<4"` — Template rendering (may already be pulled by fastapi but declare explicitly)

    2. Run `uv lock` and `uv sync` to update the lock file and install.

    3. Add chat-specific settings to Settings class in config.py:
       - `chat_model: str = Field(default="claude-sonnet-4-5-20250514")` with env var `BBJ_RAG_CHAT_MODEL`
       - `chat_max_tokens: int = Field(default=2048)` with env var `BBJ_RAG_CHAT_MAX_TOKENS`
       - `chat_max_history: int = Field(default=20)` — sliding window limit for conversation messages (10 turns = 20 messages)
       - `chat_confidence_min_results: int = Field(default=2)` — below this count triggers low_confidence flag
       - `chat_confidence_min_score: float = Field(default=0.3)` — below this top score triggers low_confidence flag

    These follow the existing BBJ_RAG_ env prefix convention (so `BBJ_RAG_CHAT_MODEL`, etc.).
  </action>
  <verify>
    - `cd rag-ingestion && uv run python -c "from bbj_rag.config import Settings; s = Settings(); print(s.chat_model, s.chat_max_tokens)"` prints `claude-sonnet-4-5-20250514 2048`
    - `uv run python -c "import anthropic; import sse_starlette; import jinja2; print('OK')"` prints OK
  </verify>
  <done>anthropic, sse-starlette, jinja2 are importable; Settings has chat_model, chat_max_tokens, chat_max_history, chat_confidence_min_results, chat_confidence_min_score fields with correct defaults.</done>
</task>

<task type="auto">
  <name>Task 2: Create chat module and wire SSE streaming endpoint</name>
  <files>
    rag-ingestion/src/bbj_rag/chat/__init__.py
    rag-ingestion/src/bbj_rag/chat/prompt.py
    rag-ingestion/src/bbj_rag/chat/stream.py
    rag-ingestion/src/bbj_rag/api/chat.py
    rag-ingestion/src/bbj_rag/app.py
  </files>
  <action>
    **chat/__init__.py:** Empty module init (or re-export `build_rag_system_prompt` and `stream_chat_response`).

    **chat/prompt.py — `build_rag_system_prompt(results, low_confidence)`:**
    - Accepts `list[SearchResult]` (from search.py) and `low_confidence: bool`.
    - Builds a system prompt string with these sections:
      1. Role: "You are a BBj programming assistant embedded in the official BBj documentation site."
      2. Instructions: Answer using ONLY the reference material. Cite sources inline using [Source N](url) markdown link notation. For each claim, cite the 1-3 most relevant sources. When citing, mention the source type label (e.g., "according to the Flare Docs [Source 1](url)..." or "the PDF Manual notes [Source 2](url)...").
      3. Confidence caveat: If `low_confidence` is True, add instruction: "Note: Limited reference material was found for this query. Indicate this to the user and be transparent about what you can and cannot confirm."
      4. Formatting: Use Markdown. Use ```bbj for BBj code blocks. Keep answers focused and practical.
      5. Reference Material: Numbered source blocks, each containing:
         - `[Source {i}: {title}]`
         - `URL: {display_url}`
         - `Type: {source_type}`
         - `Context: {context_header}` (if non-empty)
         - Content text
         - Separated by `---`
    - Returns the assembled string.

    **chat/stream.py — `stream_chat_response(messages, search_results, settings, low_confidence)`:**
    - Accepts:
      - `messages: list[dict]` — conversation history (role/content dicts)
      - `search_results: list[SearchResult]` — RAG results for context
      - `settings: Settings` — for model name, max_tokens
      - `low_confidence: bool` — flag for confidence hint
    - Creates an `AsyncAnthropic()` client (reads ANTHROPIC_API_KEY from env automatically).
    - Calls `build_rag_system_prompt(search_results, low_confidence)` to get system prompt.
    - Truncates messages to `settings.chat_max_history` most recent entries (sliding window).
    - Is an async generator that yields dicts in sse-starlette format:
      1. First yield: `{"event": "sources", "data": json.dumps(sources_list)}` where sources_list contains `{"title", "url", "source_type", "low_confidence"}` for each result.
      2. Then streams: `async with client.messages.stream(model=..., max_tokens=..., system=system_prompt, messages=truncated_messages) as stream:` and for each `text` in `stream.text_stream`, yields `{"event": "delta", "data": json.dumps({"text": text})}`.
      3. After stream completes: `final = await stream.get_final_message()`, yield `{"event": "done", "data": json.dumps({"input_tokens": ..., "output_tokens": ...})}`.
      4. On exception: yield `{"event": "error", "data": json.dumps({"message": str(exc)})}`.
    - Wrap the streaming section in try/except/finally. In finally, ensure the stream is properly closed.

    **api/chat.py — Chat API router:**
    - Create `router = APIRouter(prefix="/chat", tags=["chat"])`.
    - Pydantic models:
      - `ChatMessage(BaseModel)`: `role: str`, `content: str`
      - `ChatRequest(BaseModel)`: `messages: list[ChatMessage]`
    - `GET /chat` — Returns HTML page (placeholder for now, just return a simple "Chat page coming in Plan 02" text response; Plan 02 will add the real template rendering). Use `HTMLResponse` with a minimal placeholder.
    - `POST /chat/stream` — The main endpoint:
      1. Extract latest user message: `body.messages[-1].content`.
      2. Get dependencies via FastAPI DI (same pattern as routes.py): `get_conn`, `get_ollama_client`, `get_settings`.
      3. Embed the query using Ollama (same pattern as the /search endpoint in routes.py).
      4. Run `async_hybrid_search(conn, embedding, query_text, limit=10)` then `rerank_for_diversity(results, limit=5)` — fetch top 5 diverse results for RAG context.
      5. Determine `low_confidence`: `len(results) < settings.chat_confidence_min_results or (len(results) > 0 and results[0].score < settings.chat_confidence_min_score)`.
      6. Convert messages to dicts: `[{"role": m.role, "content": m.content} for m in body.messages]`.
      7. Return `EventSourceResponse(stream_chat_response(messages_dicts, results, settings, low_confidence))`.

    **app.py — Wire the chat router:**
    - Import: `from bbj_rag.api.chat import router as chat_router`
    - Add: `app.include_router(chat_router)` after the existing `app.include_router(api_router)`.
  </action>
  <verify>
    - `cd rag-ingestion && uv run python -c "from bbj_rag.chat.prompt import build_rag_system_prompt; print('prompt OK')"` succeeds
    - `cd rag-ingestion && uv run python -c "from bbj_rag.chat.stream import stream_chat_response; print('stream OK')"` succeeds
    - `cd rag-ingestion && uv run python -c "from bbj_rag.api.chat import router; print('chat router OK, routes:', [r.path for r in router.routes])"` shows `/chat` and `/chat/stream` paths
    - `cd rag-ingestion && uv run python -c "from bbj_rag.app import app; routes = [r.path for r in app.routes]; assert '/chat' in routes or '/chat/' in routes; assert '/chat/stream' in routes; print('app wiring OK')"` succeeds
    - Existing tests still pass: `cd rag-ingestion && uv run pytest tests/ -x -q` (no regressions)
  </verify>
  <done>
    Chat module exists with prompt.py (build_rag_system_prompt) and stream.py (stream_chat_response). api/chat.py has GET /chat (placeholder) and POST /chat/stream (SSE streaming with JSON-encoded events). app.py includes the chat router. All existing tests pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run python -c "from bbj_rag.config import Settings; s = Settings(); print(s.chat_model)"` outputs `claude-sonnet-4-5-20250514`
2. `uv run python -c "import anthropic, sse_starlette, jinja2; print('deps OK')"` outputs `deps OK`
3. `uv run python -c "from bbj_rag.chat.prompt import build_rag_system_prompt; from bbj_rag.chat.stream import stream_chat_response; print('chat module OK')"` outputs `chat module OK`
4. `uv run python -c "from bbj_rag.app import app; print([r.path for r in app.routes])"` includes `/chat` and `/chat/stream`
5. `uv run pytest tests/ -x -q` passes with no regressions
</verification>

<success_criteria>
- anthropic and sse-starlette are installed and importable
- Settings has chat_model defaulting to claude-sonnet-4-5-20250514
- chat/prompt.py builds a system prompt with numbered source blocks and inline citation instructions
- chat/stream.py streams Claude API responses as JSON-encoded SSE events (sources, delta, done, error)
- api/chat.py exposes GET /chat and POST /chat/stream
- app.py includes the chat router
- No existing tests broken
</success_criteria>

<output>
After completion, create `.planning/phases/26-chat-interface/26-01-SUMMARY.md`
</output>
