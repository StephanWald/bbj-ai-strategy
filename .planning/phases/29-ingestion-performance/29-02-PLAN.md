---
phase: 29-ingestion-performance
plan: 02
type: execute
wave: 2
depends_on: ["29-01"]
files_modified:
  - rag-ingestion/src/bbj_rag/parallel.py
  - rag-ingestion/tests/test_parallel.py
autonomous: true

must_haves:
  truths:
    - "Multiple workers process chunks concurrently via asyncio"
    - "Failed batches are retried up to 3 times before giving up"
    - "Failed items are logged to a JSON file for later retry"
    - "Successful chunks remain in DB even if other batches fail"
  artifacts:
    - path: "rag-ingestion/src/bbj_rag/parallel.py"
      provides: "Parallel ingestion worker pool"
      contains: "class ParallelIngestor"
    - path: "rag-ingestion/tests/test_parallel.py"
      provides: "Unit tests for parallel worker"
      contains: "test_parallel"
  key_links:
    - from: "rag-ingestion/src/bbj_rag/parallel.py"
      to: "AsyncOllamaEmbedder"
      via: "import from embedder module"
      pattern: "from bbj_rag.embedder import.*AsyncOllamaEmbedder"
    - from: "rag-ingestion/src/bbj_rag/parallel.py"
      to: "asyncio worker pool"
      via: "asyncio.gather or TaskGroup"
      pattern: "asyncio\\.(gather|TaskGroup)"
---

<objective>
Create a parallel ingestion worker module that processes embedding batches concurrently.

Purpose: Enable 1.5-2x speedup for corpus rebuilds by running multiple embedding workers in parallel, with robust retry logic and failure tracking for partial failure recovery.

Output: ParallelIngestor class with asyncio worker pool, retry logic, and failure logging.
</objective>

<execution_context>
@/Users/beff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/beff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-ingestion-performance/29-CONTEXT.md
@.planning/phases/29-ingestion-performance/29-01-SUMMARY.md
@rag-ingestion/src/bbj_rag/embedder.py
@rag-ingestion/src/bbj_rag/pipeline.py
@rag-ingestion/src/bbj_rag/config.py
@rag-ingestion/src/bbj_rag/db.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ParallelIngestor worker pool</name>
  <files>rag-ingestion/src/bbj_rag/parallel.py</files>
  <action>
Create a new module `parallel.py` with a ParallelIngestor class that:

1. **Work distribution:**
   - Accept a list of Chunk objects to process
   - Distribute chunks into batches (configurable batch_size, default 64)
   - Use asyncio.Queue to distribute batches to workers
   - Workers pull from queue until exhausted

2. **Worker implementation:**
   - Each worker has its own AsyncOllamaEmbedder instance (for connection pool isolation)
   - Worker embeds batch, then stores to DB using a connection from pool
   - Workers run concurrently via asyncio.gather()

3. **Retry logic:**
   - If embed_batch fails, retry up to `retries` times (default 3 from config)
   - Exponential backoff between retries (1s, 2s, 4s)
   - After max retries, add batch to failed_batches list and continue

4. **Stats tracking:**
   - Track: total_batches, completed_batches, failed_batches, total_chunks_stored
   - Return stats dict when complete

Class interface:
```python
class ParallelIngestor:
    def __init__(
        self,
        settings: Settings,
        num_workers: int = 4,
        batch_size: int = 64,
        verbose: bool = False,
    ) -> None: ...

    async def ingest_chunks(
        self,
        chunks: list[Chunk],
        db_url: str,
    ) -> IngestResult: ...
```

IngestResult dataclass with fields:
- chunks_embedded: int
- chunks_stored: int
- batches_completed: int
- batches_failed: int
- failed_chunks: list[Chunk]  # For failure logging
- duration: float

Use psycopg_pool.AsyncConnectionPool for database connections in async context.
  </action>
  <verify>
Run: cd rag-ingestion && uv run python -c "
from bbj_rag.parallel import ParallelIngestor, IngestResult
from bbj_rag.config import Settings
print('ParallelIngestor imported successfully')
print(f'IngestResult fields: {IngestResult.__dataclass_fields__.keys()}')
"
Should import without errors and show IngestResult fields.
  </verify>
  <done>ParallelIngestor class exists with asyncio worker pool, batch distribution via queue, and configurable worker count.</done>
</task>

<task type="auto">
  <name>Task 2: Implement failure logging and retry support</name>
  <files>rag-ingestion/src/bbj_rag/parallel.py</files>
  <action>
Add failure handling to ParallelIngestor:

1. **Failure logging:**
   Add method to save failed chunks to JSON file:
   ```python
   def save_failure_log(
       self,
       failed_chunks: list[Chunk],
       filepath: Path,
   ) -> None:
   ```
   - Save chunk content_hash, source_url, and error message
   - Append to existing file if present (don't overwrite prior failures)
   - Use JSON lines format for easy streaming reads

2. **Retry from failure log:**
   Add class method to load failed chunks:
   ```python
   @classmethod
   def load_failure_log(cls, filepath: Path) -> list[dict]:
   ```
   - Returns list of {content_hash, source_url, error} dicts
   - Caller can filter chunks to re-process based on content_hash

3. **Verbose progress output:**
   - If verbose=True, print per-worker progress: "[Worker 1] Batch 5/20 complete (312 chunks)"
   - Print retry attempts: "[Worker 2] Batch 8 failed, retrying (1/3)..."
   - Use standard print() or logging based on verbose flag

4. **Completion report:**
   Add method to print summary:
   ```python
   def print_completion_report(self, result: IngestResult) -> None:
   ```
   - Total chunks processed
   - Success rate (batches_completed / total_batches)
   - Failures (if any) with hint about --retry-failed

Export: ParallelIngestor, IngestResult, and any helper functions needed by CLI.
  </action>
  <verify>
Run: cd rag-ingestion && uv run python -c "
from bbj_rag.parallel import ParallelIngestor, IngestResult
from pathlib import Path
import tempfile

# Test failure log save/load
with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:
    path = Path(f.name)

# Create mock failed data
from bbj_rag.models import Chunk
chunk = Chunk(
    content='test',
    source_url='test://url',
    title='Test',
    content_hash='abc123',
)
ParallelIngestor.save_failure_log([chunk], path, 'test error')
loaded = ParallelIngestor.load_failure_log(path)
print(f'Loaded {len(loaded)} failed items')
print(f'First item: {loaded[0]}')
path.unlink()
print('OK')
"
Should save and load failure log successfully.
  </verify>
  <done>Failure logging saves to JSON, load_failure_log reads it back, verbose mode prints per-worker progress, and completion report summarizes results.</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for parallel module</name>
  <files>rag-ingestion/tests/test_parallel.py</files>
  <action>
Create unit tests for the parallel module:

1. **Test batch distribution:**
   - Given 150 chunks with batch_size=64, verify 3 batches created
   - Verify all chunks accounted for

2. **Test retry logic (mocked):**
   - Mock AsyncOllamaEmbedder to fail first 2 times, succeed on 3rd
   - Verify batch eventually completes
   - Verify retry count tracked

3. **Test failure logging:**
   - Test save_failure_log writes correct JSON
   - Test load_failure_log reads it back
   - Test append behavior (multiple failures)

4. **Test IngestResult dataclass:**
   - Verify all fields present
   - Verify duration calculation

Use pytest fixtures and mocking (unittest.mock.AsyncMock for async methods).
Do NOT test actual Ollama/database - those are integration tests.
  </action>
  <verify>
Run: cd rag-ingestion && uv run pytest tests/test_parallel.py -v
All tests should pass.
  </verify>
  <done>Unit tests exist for batch distribution, retry logic, failure logging, and IngestResult. Tests pass without requiring Ollama or database.</done>
</task>

</tasks>

<verification>
1. ParallelIngestor can be instantiated with settings and worker count
2. ingest_chunks method signature matches specification
3. Failure log saves and loads correctly
4. Verbose mode produces progress output
5. All new tests pass: cd rag-ingestion && uv run pytest tests/test_parallel.py -v
6. Existing tests still pass: cd rag-ingestion && uv run pytest -v --ignore=tests/test_search_validation.py
</verification>

<success_criteria>
- ParallelIngestor class with asyncio.gather-based worker pool
- Batch distribution via asyncio.Queue
- Retry logic with exponential backoff (max 3 attempts)
- Failure logging to JSON file
- Verbose progress output option
- Completion report with success rate
- Unit tests for all core functionality
</success_criteria>

<output>
After completion, create `.planning/phases/29-ingestion-performance/29-02-SUMMARY.md`
</output>
