# Milestone v1.2: RAG Ingestion Pipeline

**Status:** âœ… SHIPPED 2026-02-01
**Phases:** 8-14
**Total Plans:** 15

## Overview

This milestone bridges the gap between Chapter 6's strategic RAG design and actual implementation by delivering a Python-based ingestion pipeline that processes five BBj documentation sources into a generation-aware pgvector database. The work progresses from project scaffolding and schema contracts, through the primary Flare parser, BBj-specific intelligence (the make-or-break generation tagger), embedding pipeline, additional source parsers, and finally documentation and quality validation. Phases 8-14 continue from v1.1's Phase 7.

## Phases

### Phase 8: Project Scaffold & README

**Goal**: Engineers can clone the repo and find a working Python project structure alongside the docs site, with a clear repo README explaining the whole project
**Depends on**: Phase 7 (v1.1 complete)
**Plans**: 1 plan

Plans:
- [x] 08-01-PLAN.md -- Scaffold rag-ingestion/ Python project with uv and write repo README

**Details:**
- Python 3.12+ sub-project (bbj-rag) with uv, hatchling src-layout, pytest/ruff/mypy toolchain
- Pre-commit hooks at repo root scoped to rag-ingestion/ via files filter
- Makefile task runner for dev workflow (sync, test, lint, format, typecheck, check)
- Repo README replaced Docusaurus boilerplate with project overview

### Phase 9: Schema & Data Models

**Goal**: The database schema and data contracts are defined so all downstream parsers, taggers, and embedders work against stable interfaces
**Depends on**: Phase 8
**Plans**: 2 plans

Plans:
- [x] 09-01-PLAN.md -- Pydantic data models (Document/Chunk), config system (pydantic-settings + TOML), content-hash dedup, unit tests
- [x] 09-02-PLAN.md -- pgvector schema DDL, database connection module (psycopg3 + register_vector), schema helper, DB tests

**Details:**
- Document/Chunk Pydantic models with SHA-256 content-hash deduplication
- TOML + env var configuration with BBJ_RAG_ prefix
- pgvector schema DDL with HNSW cosine index, GIN indexes for tsvector and generations
- ON CONFLICT (content_hash) DO NOTHING for idempotent re-ingestion

### Phase 10: Flare Parser

**Goal**: The MadCap Flare documentation corpus (the largest and most complex source) is parseable into structured Document objects, validating the entire pipeline architecture
**Depends on**: Phase 9
**Plans**: 3 plans

Plans:
- [x] 10-01-PLAN.md -- Parser foundation: dependencies, DocumentParser protocol, TOC index builder, condition tag extractor
- [x] 10-02-PLAN.md -- Flare XHTML parser: MadCap tag handling, snippet resolution, code blocks, tables, hierarchy paths
- [x] 10-03-PLAN.md -- Web crawl fallback parser: httpx + BeautifulSoup for documentation.basis.cloud

**Details:**
- DocumentParser Protocol as contract for all parsers
- TOC index from 4 .fltoc files (1,595 entries) with priority ordering
- FlareParser yielding 7,079 Documents from raw XHTML with code blocks, tables, snippet inlining
- WebCrawlParser as fallback for when Flare project files unavailable
- Condition tag extraction with 6 generation-relevant mappings

### Phase 11: BBj Intelligence

**Goal**: Every parsed document is automatically classified by BBj generation and document type, and chunks carry contextual headers derived from the source hierarchy
**Depends on**: Phase 10
**Plans**: 2 plans

Plans:
- [x] 11-01-PLAN.md -- Generation tagger with multi-signal classification (StrEnum, path/condition/content signals, report), model updates (context_header, deprecated), DDL changes
- [x] 11-02-PLAN.md -- Document type classifier (7 types, extensible rule registry) and contextual chunk header builder

**Details:**
- Generation StrEnum with 5 canonical labels: all, character, vpro5, bbj_gui, dwc
- Signal-based scoring: path (0.6), condition (0.3-0.5), content (0.4) with 0.3 threshold
- DocType classifier with 7 types via extensible frozen-dataclass rule registry
- Contextual header builder combining TOC breadcrumbs with page titles

### Phase 12: Embedding Pipeline

**Goal**: The full pipeline runs end-to-end for the Flare source -- parse, tag, chunk, embed, store -- producing searchable vectors in pgvector with working hybrid retrieval
**Depends on**: Phase 11
**Plans**: 2 plans

Plans:
- [x] 12-01-PLAN.md -- Chunker, embedder, bulk storage, config/schema updates, pipeline orchestrator, and Click CLI
- [x] 12-02-PLAN.md -- Search functions (dense, BM25, hybrid RRF), YAML validation cases, parametrized test suite

**Details:**
- Heading-aware markdown chunker (400-token target, 50-token overlap)
- Ollama embedder (Qwen3-Embedding-0.6B, 1024 dims) with OpenAI fallback
- Binary COPY bulk insert via staging table with ON CONFLICT dedup
- Click CLI: ingest, parse, report, validate commands
- Hybrid search: dense, BM25, RRF with k=50 constant

### Phase 13: Additional Parsers

**Goal**: The remaining four source types plug into the proven pipeline, completing source coverage
**Depends on**: Phase 12
**Plans**: 3 plans

Plans:
- [x] 13-01-PLAN.md -- PDF parser (pymupdf4llm) and BBj source code parser with unit tests
- [x] 13-02-PLAN.md -- WordPress parsers (Advantage + Knowledge Base) via httpx + BeautifulSoup web crawl
- [x] 13-03-PLAN.md -- Docusaurus MDX parser, pipeline integration, CLI expansion for all sources

**Details:**
- PdfParser with pymupdf4llm heading-boundary splitting and per-section generation tagging
- BbjSourceParser with DWC/GUI pattern classification and BBj keyword validation
- AdvantageParser and KnowledgeBaseParser with WordPress chrome stripping and sitemap fallback
- MdxParser for DWC-Course with JSX stripping and uniform dwc generation tagging
- Pipeline intelligence bypass for pre-populated Documents from non-Flare parsers
- CLI expanded to all 6 source types

### Phase 14: Documentation & Quality

**Goal**: The pipeline is documented for engineers to set up and run, the Getting Started page connects strategy docs to actual code, and ingestion quality is measurable
**Depends on**: Phase 13
**Plans**: 2 plans

Plans:
- [x] 14-01-PLAN.md -- Getting Started sub-page under Chapter 6 with pipeline diagram, source coverage, data model snippets, and GitHub code links
- [x] 14-02-PLAN.md -- Quality report CLI command, post-ingest auto-report, anomaly warnings, and comprehensive rag-ingestion README

**Details:**
- Getting Started page with Mermaid pipeline diagram, source coverage table, inline data model snippets
- DB-query-based quality report (source/generation/doc_type distributions)
- 5 automated anomaly warnings (empty sources, low counts, untagged %, unknown types, dominant source)
- Comprehensive rag-ingestion README with setup, config, and CLI reference

---

## Milestone Summary

**Key Decisions:**

- Python ingestion sub-project as mono-repo directory (keeps scripts co-located with strategy docs)
- Both Flare export and crawl ingestion paths (engineers may or may not have Flare project access)
- hatchling build backend for stable src-layout support
- vector(1024) matching Qwen3-Embedding-0.6B default output dimensions
- Ollama as default embedding provider with OpenAI as configurable fallback
- Binary COPY via staging table + INSERT ON CONFLICT for idempotent bulk inserts
- Signal-based generation tagging with weighted aggregation (path=0.6, condition=0.3-0.5, content=0.4)
- Pipeline intelligence bypass for non-Flare parsers that pre-populate doc_type/generations

**Issues Resolved:**

- Windows backslash paths in Flare snippet src attributes (normalization fix)
- 58 unresolvable snippet references identified as genuine Flare authoring issues (graceful degradation)
- pydantic-settings 2.12 TOML loading requires settings_customise_sources override
- PostgreSQL IMMUTABLE constraint requires two-arg to_tsvector with || operator

**Issues Deferred:**

- None

**Technical Debt Incurred:**

- None identified. All phases completed cleanly with no deferred items.

---

_For current project status, see .planning/MILESTONES.md_
